# Copyright (c) AIRBUS and its affiliates.
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from __future__ import annotations

import os
import sys
from typing import Any, Callable, Dict, List, Optional, Tuple

from skdecide import Domain, Solver, hub
from skdecide.builders.domain import (
    Actions,
    DeterministicInitialized,
    Environment,
    FullyObservable,
    Markovian,
    Rewards,
    Sequential,
    SingleAgent,
)
from skdecide.builders.solver import (
    DeterministicPolicies,
    FromAnyState,
    ParallelSolver,
    Utilities,
)

record_sys_path = sys.path
skdecide_cpp_extension_lib_path = os.path.abspath(hub.__path__[0])
if skdecide_cpp_extension_lib_path not in sys.path:
    sys.path.append(skdecide_cpp_extension_lib_path)

try:

    from __skdecide_hub_cpp import _RIWSolver_ as riw_solver

    class D(
        Domain,
        SingleAgent,
        Sequential,
        Environment,
        Actions,
        DeterministicInitialized,
        Markovian,
        FullyObservable,
        Rewards,
    ):  # TODO: check why DeterministicInitialized & PositiveCosts/Rewards?
        pass

    class RIW(ParallelSolver, Solver, DeterministicPolicies, Utilities, FromAnyState):
        """This is the skdecide implementation of "Planning with Pixels in
        (Almost) Real Time" by Wilmer Bandres, Blai Bonet, Hector Geffner (AAAI 2018)
        """

        T_domain = D

        def __init__(
            self,
            domain_factory: Callable[[], T_domain],
            state_features: Callable[[T_domain, D.T_state], Any],
            use_state_feature_hash: bool = False,
            use_simulation_domain: bool = False,
            time_budget: int = 3600000,
            rollout_budget: int = 100000,
            max_depth: int = 1000,
            exploration: float = 0.25,
            residual_moving_average_window: int = 100,
            epsilon: float = 0.001,
            discount: float = 1.0,
            online_node_garbage: bool = False,
            continuous_planning: bool = True,
            parallel: bool = False,
            shared_memory_proxy=None,
            callback: Callable[[RIW, Optional[int]], bool] = lambda slv, i=None: False,
            verbose: bool = False,
        ) -> None:
            """Construct a RIW solver instance

            # Parameters
            domain_factory (Callable[[], T_domain]): The lambda function to create a domain instance.
            state_features (Callable[[T_domain, D.T_state], Any]): State feature vector
                used to compute the novelty measure
            use_state_feature_hash (bool, optional): Boolean indicating whether states
                must be hashed by using their features (True) or by using their native
                hash function (False). Defaults to False.
            use_simulation_domain (bool, optional): Boolean indicating whether the state
                transitions should be generated by using the `Simulation.sample`
                method of the domain (True) or the `Environment.step` method of
                the domain (False) depending on the domain's dynamics capabilities.
                Defaults to False.
            time_budget (int, optional): Maximum solving time in milliseconds. Defaults to 3600000.
            rollout_budget (int, optional): Maximum number of rollouts. Defaults to 100000.
            max_depth (int, optional): Maximum depth of each RIW trial (rollout). Defaults to 1000.
            exploration (float, optional): Probability of choosing a non-solved child of
                a given node (more precisely, a first-time explored child is chosen with a
                probability 'exploration', and a already-explored but non-solved child is
                chosen with a probability of '1 - exploration' divided by its novelty
                measure; probabilities among children are then normalized). Defaults to 0.25.
            residual_moving_average_window (int, optional): Number of latest computed residual values
                to memorize in order to compute the average Bellman error (residual) at the root state
                of the search (deactivated when use_labels is True). Defaults to 100.
            epsilon (float, optional): Maximum Bellman error (residual) allowed to decide that a state
                is solved, or to decide when no labels are used that the value function of the root state
                of the search has converged (in the latter case: the root state's Bellman error is averaged
                over the residual_moving_average_window, deactivated when use_labels is True). Defaults to 0.001.
            discount (float, optional): Value function's discount factor. Defaults to 1.0.
            online_node_garbage (bool, optional): Boolean indicating whether the search graph which is
                no more reachable from the root solving state should be deleted (True) or not (False). Defaults to False.
            continuous_planning (bool, optional): Boolean whether the solver should optimize again the policy
                from the current solving state (True) or not (False) even if the policy is already defined
                in this state. Defaults to True.
            parallel (bool, optional): Parallelize RIW rollouts on different processes using duplicated domains (True)
                or not (False). Defaults to False.
            shared_memory_proxy (_type_, optional): The optional shared memory proxy. Defaults to None.
            callback (Callable[[RIW, Optional[int]], optional): Function called at the end of each RIW rollout,
                taking as arguments the solver and the thread/process ID (i.e. parallel domain ID, which is equal to None
                in case of sequential execution, i.e. when 'parallel' is set to False in this constructor) from
                which the callback is called, and returning True if the solver must be stopped. The callback lambda
                function cannot take the (potentially parallelized) domain as argument because we could not otherwise
                serialize (i.e. pickle) the solver to pass it to the corresponding parallel domain process in case of parallel
                execution. Nevertheless, the `ParallelSolver.get_domain` method callable on the solver instance
                can be used to retrieve either the user domain in sequential execution, or the parallel domains proxy
                `ParallelDomain` in parallel execution from which domain methods can be called by using the
                callback's process ID argument. Defaults to (lambda slv, i=None: False).
            verbose (bool, optional): Boolean indicating whether verbose messages should be logged (True)
                or not (False). Defaults to False.
            """
            ParallelSolver.__init__(
                self,
                parallel=parallel,
                shared_memory_proxy=shared_memory_proxy,
            )
            Solver.__init__(self, domain_factory=domain_factory)
            self._solver = None
            self._domain = None
            self._state_features = state_features
            self._use_state_feature_hash = use_state_feature_hash
            self._use_simulation_domain = use_simulation_domain
            self._time_budget = time_budget
            self._rollout_budget = rollout_budget
            self._max_depth = max_depth
            self._exploration = exploration
            self._residual_moving_average_window = residual_moving_average_window
            self._epsilon = epsilon
            self._discount = discount
            self._online_node_garbage = online_node_garbage
            self._continuous_planning = continuous_planning
            self._callback = callback
            self._verbose = verbose
            self._lambdas = [self._state_features]
            self._ipc_notify = True

        def close(self):
            """Joins the parallel domains' processes.

            !!! warning
                Not calling this method (or not using the 'with' context statement)
                results in the solver forever waiting for the domain processes to exit.

            """
            if self._parallel:
                self._solver.close()
            ParallelSolver.close(self)

        def _init_solve(self) -> None:
            self._solver = riw_solver(
                solver=self,
                domain=self.get_domain(),
                state_features=(
                    (lambda d, s, i=None: self._state_features(d, s))
                    if not self._parallel
                    else (lambda d, s, i=None: d.call(i, 0, s))
                ),
                use_state_feature_hash=self._use_state_feature_hash,
                use_simulation_domain=self._use_simulation_domain,
                time_budget=self._time_budget,
                rollout_budget=self._rollout_budget,
                max_depth=self._max_depth,
                exploration=self._exploration,
                residual_moving_average_window=self._residual_moving_average_window,
                epsilon=self._epsilon,
                discount=self._discount,
                online_node_garbage=self._online_node_garbage,
                parallel=self._parallel,
                callback=self._callback,
                verbose=self._verbose,
            )
            self._solver.clear()

        def _reset(self) -> None:
            """Clears the search graph."""
            self._solver.clear()

        def _solve_from(self, memory: D.T_memory[D.T_state]) -> None:
            """Run the RIW algorithm from a given root solving state

            # Parameters
            memory (D.T_memory[D.T_state]): State from which to run the RIW algorithm
                (root of the search graph)
            """
            self._solver.solve(memory)

        def _is_solution_defined_for(
            self, observation: D.T_agent[D.T_observation]
        ) -> bool:
            """Indicates whether the solution policy is defined for a given state

            # Parameters
            observation (D.T_agent[D.T_observation]): State for which an entry is searched
                in the policy graph

            # Returns
            bool: True if the state has been explored and an action is defined in this state,
                False otherwise
            """
            return self._solver.is_solution_defined_for(observation)

        def _get_next_action(
            self, observation: D.T_agent[D.T_observation]
        ) -> D.T_agent[D.T_concurrency[D.T_event]]:
            """Get the best computed action in terms of best Q-value in a given state. The search
                subgraph which is no more reachable after executing the returned action is
                also deleted if node garbage was set to true in the RIW instance's constructor.
                The solver is run from `observation` if `continuous_planning` was set to True
                in the RIW instance's constructor or if no solution is defined (i.e. has been
                previously computed) in `observation`.

            !!! warning
                Returns a random action if no action is defined in the given state,
                which is why it is advised to call `RIW.is_solution_defined_for` before

            # Parameters
            observation (D.T_agent[D.T_observation]): State for which the best action is requested

            # Returns
            D.T_agent[D.T_concurrency[D.T_event]]: Best computed action
            """
            if self._continuous_planning or not self._is_solution_defined_for(
                observation
            ):
                self._solve_from(observation)
            action = self._solver.get_next_action(observation)
            if action is None:
                print(
                    "\x1b[3;33;40m"
                    + "No best action found in observation "
                    + str(observation)
                    + ", applying random action"
                    + "\x1b[0m"
                )
                return self.call_domain_method("get_action_space").sample()
            else:
                return action

        def _get_utility(self, observation: D.T_agent[D.T_observation]) -> D.T_value:
            """Get the best Q-value in a given state

            !!! warning
                Returns None if no action is defined in the given state, which is why
                it is advised to call `RIW.is_solution_defined_for` before

            # Parameters
            observation (D.T_agent[D.T_observation]): State from which the best Q-value is requested

            # Returns
            D.T_value: Maximum Q-value of the given state over the applicable actions in this state
            """
            return self._solver.get_utility(observation)

        def get_nb_explored_states(self) -> int:
            """Get the number of states present in the search graph (which can be
                lower than the number of actually explored states if node garbage was
                set to True in the RIW instance's constructor)

            # Returns
            int: Number of states present in the search graph
            """
            return self._solver.get_nb_explored_states()

        def get_nb_pruned_states(self) -> int:
            """Get the number of states present in the search graph that have been
                pruned by the novelty test (which can be lower than the number of actually
                explored states if node garbage was set to True in the RIWSolver
                instance's constructor)

            # Returns
            int: Number of states present in the search graph that have been pruned by
                the novelty test
            """
            return self._solver.get_nb_pruned_states()

        def get_exploration_statistics(self) -> int:
            """Get the exploration statistics as number of states present in the
                search graph and number of such states that have been pruned by the novelty
                test (both statistics can be lower than the number of actually
                explored states if node garbage was set to True in the RIWSolver
                instance's constructor)

            # Returns
            int: Pair of number of states present in the search graph and of number of
                such states that have been pruned by the novelty test
            """
            return self._solver.get_nb_pruned_states()

        def get_nb_rollouts(self) -> int:
            """Get the number of rollouts since the beginning of the search from
                the root solving state

            # Returns
            int: Number of RIW rollouts
            """
            return self._solver.get_nb_rollouts()

        def get_residual_moving_average(self) -> float:
            """Get the average Bellman error (residual) at the root state of the search,
                or an infinite value if the number of computed residuals is lower than
                the epsilon moving average window set in the RIW instance's constructor

            # Returns
            float: Bellman error at the root state of the search averaged over
                the epsilon moving average window
            """
            return self._solver.get_residual_moving_average()

        def get_solving_time(self) -> int:
            """Get the solving time in milliseconds since the beginning of the
                search from the root solving state

            # Returns
            int: Solving time in milliseconds
            """
            return self._solver.get_solving_time()

        def get_policy(
            self,
        ) -> Dict[
            D.T_agent[D.T_observation],
            Tuple[D.T_agent[D.T_concurrency[D.T_event]], D.T_value],
        ]:
            """Get the (partial) solution policy defined for the states for which
                the Q-value has been updated at least once (which is optimal if the
                algorithm has converged and labels are used)

            !!! warning
                Only defined over the states reachable from the last root solving state
                when node garbage was set to True in the RIW instance's constructor

            # Returns
            Dict[ D.T_agent[D.T_observation], Tuple[D.T_agent[D.T_concurrency[D.T_event]], D.T_value], ]:
                Mapping from states to pairs of action and best Q-value
            """
            return self._solver.get_policy()

        def get_action_prefix(self) -> List[D.T_agent[D.T_observation]]:
            """Get the list of actions returned by the solver so far after each
                call to the RIW.get_next_action method (mostly internal use in order
                to rebuild the sequence of visited states until reaching the current
                solving state, when 'use_simulation_domain' was set to False in the
                RIW instance's constructor for which we can only progress the
                transition function with steps that hide the current state of the environment)

            # Returns
            List[D.T_agent[D.T_observation]]: List of actions executed by the solver
                so far after each call to the `RIW.get_next_action` method
            """
            return self._solver.get_action_prefix()

except ImportError:
    sys.path = record_sys_path
    print(
        'Scikit-decide C++ hub library not found. Please check it is installed in "skdecide/hub".'
    )
    raise
