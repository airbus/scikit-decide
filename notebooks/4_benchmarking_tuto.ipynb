{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scikit-decide benchmark\n",
        "\n",
        "This notebook demonstrates how to run & compare scikit-decide solvers compatible with a given domain. This benchmark is supported by [Ray Tune](https://docs.ray.io/en/latest/tune/index.html), a scalable Python library for experiment execution and hyperparameter tuning (incl. running experiments in parallel and logging results to Tensorboard). Benchmarking is important since the most efficient solvers might greatly vary depending on the domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "First we install the full scikit-decide package, the Tune library and we load the Tensorboard extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WARNING: if you run this notebook in Google Colab, don't forget to click on \"RESTART RUNTIME\" if prompted after executing this cell\n",
        "!pip install scikit-decide[all] # tested with version 0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ray Tune for benchmarking purpose (usually already included in full scikit-decide install)\n",
        "!pip install ray[tune]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create or load a domain\n",
        "\n",
        "As an example, we will choose the Maze domain available in scikit-decide:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from skdecide import utils\n",
        "\n",
        "MyDomain = utils.load_registered_domain('Maze')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select solvers to benchmark\n",
        "\n",
        "We start by automatically detecting compatible solvers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compatible_solvers = utils.match_solvers(MyDomain())\n",
        "print(len(compatible_solvers), 'compatible solvers:', compatible_solvers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optionally filter out some of these solvers: here we iteratively removed the ones running for too long in the cells below (thus blocking CPUs for other trials)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark_solvers = [solver for solver in compatible_solvers if solver.__name__ not in ['AOstar', 'ILAOstar', 'MCTS', 'POMCP', 'UCT']]\n",
        "print(len(benchmark_solvers), 'solvers to benchmark:', benchmark_solvers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define and run benchmark\n",
        "\n",
        "First, customize the objective function to optimize (this will serve to rank solver solutions). Here we choose *mean episode reward* to compare solvers, but we could also consider *reached goal ratio* or a mix of both...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# note: most of this function's content could actually be done in 1 line with scikit-decide rollout utility (but we will need to upgrade it slightly for that)\n",
        "def mean_episode_reward(solution, num_episodes=10, max_episode_steps=1000):\n",
        "    domain = MyDomain()\n",
        "    reward_sum = 0.\n",
        "    for _ in range(num_episodes):\n",
        "        solution.reset()\n",
        "        observation = domain.reset()\n",
        "        episode_reward = 0.\n",
        "        step = 1\n",
        "        while max_episode_steps is None or step <= max_episode_steps:\n",
        "            action = solution.sample_action(observation)\n",
        "            outcome = domain.step(action)\n",
        "            observation = outcome.observation\n",
        "            episode_reward += outcome.value.reward\n",
        "            if outcome.termination:\n",
        "                break\n",
        "            step += 1\n",
        "        reward_sum += episode_reward\n",
        "    return reward_sum / num_episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the training function for each benchmark trial (this is fairly generic and should not change much from one benchmark to another):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inspect import signature\n",
        "from ray import tune\n",
        "\n",
        "def training_function(config):\n",
        "    # Get trial hyperparameters\n",
        "    Solver = config['solver']\n",
        "    solver_args = config.get('solver_args', {}).get(Solver.__name__, {})\n",
        "    if 'domain_factory' in signature(Solver.__init__).parameters: # note: this shouldn't be necessary (but currently required by some solvers until we solve the issue)\n",
        "        solver_args['domain_factory'] = MyDomain\n",
        "    # Solve\n",
        "    with Solver(**solver_args) as s:\n",
        "        solution = MyDomain.solve_with(s)\n",
        "        score = mean_episode_reward(solution)\n",
        "    # Feed the score back back to Tune\n",
        "    tune.report(mean_episode_reward=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it is time to run the benchmark: by default, one free CPU will be allocated for each solver trial... but you can customize allocated CPUs/GPUs using the *resources_per_trial* argument. Some solvers might fail for various reasons (e.g. missing required arguments, as logged in induvidual error.txt files under ~/ray_results), but this will not stop running the other ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO # this is a RL algorithm\n",
        "\n",
        "analysis = tune.run(\n",
        "    training_function,\n",
        "    config={\n",
        "        'solver': tune.grid_search(benchmark_solvers),\n",
        "        'solver_args': { # Optional\n",
        "            # Example of how to customize specific solver arguments (if needed):\n",
        "            'StableBaseline': {\n",
        "                'algo_class': PPO,\n",
        "                'baselines_policy': 'MlpPolicy',\n",
        "                'learn_config': {'total_timesteps': 1000}\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    raise_on_failed_trial=False,\n",
        "    #time_budget_s = 60\n",
        ")\n",
        "\n",
        "# Print (one of the) best solver, i.e. with maximum mean_episode_reward\n",
        "best_config = analysis.get_best_config(metric='mean_episode_reward', mode='max')\n",
        "print('==> Best solver:', best_config['solver'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze results\n",
        "\n",
        "Let us get a dataframe for analyzing trial results and exporting them to CSV:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = analysis.results_df\n",
        "df = df[df.index.notnull()] # remove failed runs (avoids rows filled with NaN)\n",
        "df.to_csv('benchmark_results.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use Tensorboard to visualize results of all solvers, e.g. *mean_episode_reward* plot for performance and *time_this_iter_s* plot for computation time (incl. evaluation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir ~/ray_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This concludes this benchmarking notebook, but we just scratched the surface of Ray Tune possibilties. Feel free to further experiment e.g. by finetuning the hyperparameters of a specific solver to improve its results (the progress can sometimes be very significant)!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
