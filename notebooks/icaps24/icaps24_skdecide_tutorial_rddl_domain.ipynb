{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Implementing a scikit-decide domain for RDDL problems\n",
    "\n",
    "<!--credits-->\n",
    "Alexandre Arnold, Guillaume Povéda, Florent Teichteil-Königsbuch\n",
    "\n",
    "Credits to [IMACS](https://imacs.polytechnique.fr/) and especially to Nolwen Huet.\n",
    "<!--/credits-->\n",
    "\n",
    "In this notebook we demonstrate how to create a custom scikit-decide domain which can then be solved by scikit-decide solvers that are compatible with the custom created domain.\n",
    "\n",
    "*NB: Since this tutorial, the rddl domain has been introduced into the scikit-decide hub, see for instance [this notebook](https://github.com/airbus/scikit-decide/blob/master/notebooks/16_rddl_tuto.ipynb) about the pre-implemented version.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"characteristic\" rosace of scikit-decide domains show that a solver can handle all the domains whose characteristics are *more specific* than the ones of the domain for which the solver has been designed:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/airbus/scikit-decide/refs/heads/master/docs/.vuepress/public/characteristics.png\" alt=\"SkDecide characteristics\" width=\"1000\"/>\n",
    "\n",
    "For instance, as depicted in this image, if the solver can handle partially observable states, then it can solve also domains whose states are fully observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the python kernel to use for this notebook:\n",
    "- If running locally, be sure to use an environment with `scikit-decide[all]`\n",
    "- If running on colab, the next cell does it for you.\n",
    "- If running on binder, the environment should be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import glob\n",
    "    import json\n",
    "    import sys\n",
    "\n",
    "    using_nightly_version = True\n",
    "\n",
    "    if using_nightly_version:\n",
    "        # look for nightly build download url\n",
    "        release_curl_res = !curl -L   -H \"Accept: application/vnd.github+json\" -H \"X-GitHub-Api-Version: 2022-11-28\" https://api.github.com/repos/airbus/scikit-decide/releases/tags/nightly\n",
    "        release_dict = json.loads(release_curl_res.s)\n",
    "        release_download_url = sorted(\n",
    "            release_dict[\"assets\"], key=lambda d: d[\"updated_at\"]\n",
    "        )[-1][\"browser_download_url\"]\n",
    "        print(release_download_url)\n",
    "\n",
    "        # download and unzip\n",
    "        !wget --output-document=release.zip {release_download_url}\n",
    "        !unzip -o release.zip\n",
    "\n",
    "        # get proper wheel name according to python version used\n",
    "        wheel_pythonversion_tag = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "        wheel_path = glob.glob(\n",
    "            f\"dist/scikit_decide*{wheel_pythonversion_tag}*manylinux*.whl\"\n",
    "        )[0]\n",
    "\n",
    "        skdecide_pip_spec = f\"{wheel_path}[all]\"\n",
    "    else:\n",
    "        skdecide_pip_spec = \"scikit-decide[all]\"\n",
    "\n",
    "    # install scikit-decide with all extras\n",
    "    !pip install {skdecide_pip_spec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the packages that will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime as dt\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pyRDDLGym\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "from IPython.display import clear_output\n",
    "from numpy.typing import ArrayLike\n",
    "from pyRDDLGym.core.visualizer.chart import ChartVisualizer\n",
    "from pyRDDLGym.core.visualizer.movie import MovieGenerator\n",
    "from pyRDDLGym.core.visualizer.viz import BaseViz\n",
    "from pyRDDLGym_rl.core.env import SimplifiedActionRDDLEnv\n",
    "from ray.rllib.algorithms.ppo import PPO as RLLIB_PPO\n",
    "from rddlrepository.archive.competitions.IPPC2023.MountainCar.MountainCarViz import (\n",
    "    MountainCarVisualizer,\n",
    ")\n",
    "from rddlrepository.core.manager import RDDLRepoManager\n",
    "from stable_baselines3 import PPO as SB3_PPO\n",
    "\n",
    "from skdecide.builders.domain import FullyObservable, Renderable, UnrestrictedActions\n",
    "from skdecide.core import Space, TransitionOutcome, Value\n",
    "from skdecide.domains import RLDomain\n",
    "from skdecide.hub.solver.ray_rllib import RayRLlib\n",
    "from skdecide.hub.solver.stable_baselines import StableBaseline\n",
    "from skdecide.hub.space.gym import GymSpace\n",
    "from skdecide.utils import rollout\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-decide's [documentation website](https://airbus.github.io) offers a simple interactive interface to select the characteristics of the domain that you want to define.\n",
    "It also allows you to generate a template for the domain class to implement, containing the minimal set of methods that you have to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "You must go to the [code generator](https://airbus.github.io/scikit-decide/codegen/) page and follow the instructions in the following picture.\n",
    "\n",
    "<img src=\"./images/skdecide_domain_generator.png\" alt=\"SkDecide domain generator\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pyRDDLGym` package provides a Reinforcement Learning Gym environment whose characteristics correspond to scikit-decide's `RLDomain`, `UnrestrictedActions`, `FullyObservable` and `Renderable` domain characteristics.\n",
    "`RLDomain` is itself a short-hand meta-characteristic which brings the following additional characteristics: `SingleAgent`, `Sequential`, `Environment`, `Initializable`, `Markovian`, and `Rewards`.\n",
    "\n",
    "The code generator gives us the following domain class template which we have to fill-in:\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class MyDomain(D):\n",
    "    \n",
    "    def _state_step(self, action: D.T_event) -> TransitionOutcome[D.T_state, Value[D.T_value], D.T_predicate, D.T_info]:\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _state_reset(self) -> D.T_state:\n",
    "        pass\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "\n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual work begins now. It consists in implementing the contents of the methods from the generated domain class above.\n",
    "\n",
    "For our RDDL domain which \"just\" encapsulates the Gym environment from `pyRDDLGym`, we note:\n",
    "- the states are dictionaries mapping grounded fluents to their values ;\n",
    "- the actions are vectors of action values ;\n",
    "- the logics of generating the initial observation with `RDDLDomain._reset()` and the next observation with `RDDLDomain._step()` is entirely delegated to the underlying RDDL Gym environment ;\n",
    "- the rendering of the environment with `RDDLDomain._render_from()` also comes from the underluing Gym environment and the resulting image is sent to the notebook's rendering logics ;\n",
    "- the `RDDLDomain` internally handles the generation of a movie by making use of pyRDDLGym's movie generator ; the movie is generated from intermediate png files in the `Domain._step()` method when the episode reaches a terminal observation or is trunctated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Note: </b> The methods to implement are prefixed with '_' to indicate that they should be considered as protected, i.e. they should not be used by a user of the domain class like a solver. A quick glimpse at the <a href=\"https://github.com/airbus/scikit-decide/blob/master/skdecide/builders/domain/dynamics.py\"> dynamics builder class </a> shows that the class user would actually call the <a href=\"https://github.com/airbus/scikit-decide/blob/9888102d39624c2acc569994725234f984507f7b/skdecide/builders/domain/dynamics.py#L41\"> step() </a> method from which the protected <a href=\"https://github.com/airbus/scikit-decide/blob/9888102d39624c2acc569994725234f984507f7b/skdecide/builders/domain/dynamics.py#L70\"> _step() </a> method is eventually called. The builder classes most generally manage in background the automated casting of domain features to the solver's expected ones (e.g. single agent state to a multi-agent dictionary state if the domain is single-agent and the solver is multi-agent), appropriate re-implementation of methods when walking down the characteristic class hierarchies, and the LRU cache for those methods that can cache their results for later reuse. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Important: </b> Spaces (e.g. observation space, action space) must be casted to scikit-decide's spaces which are equipped with capabilities required by the various domain builder classes. All Gym spaces have their counterpart scikit-decide spaces, including `gym.Space` which corresponds to `skdecide.hub.space.gym.GymSpace`. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    T_state = Dict[str, Any]  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = ArrayLike  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class RDDLDomain(D):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rddl_domain: str,\n",
    "        rddl_instance: str,\n",
    "        visualizer: BaseViz = ChartVisualizer,\n",
    "        movie_name: str = None,\n",
    "        max_frames=1000,\n",
    "    ):\n",
    "        self.rddl_gym_env = pyRDDLGym.make(\n",
    "            rddl_domain,\n",
    "            rddl_instance,\n",
    "            base_class=SimplifiedActionRDDLEnv,\n",
    "            enforce_action_constraints=True,\n",
    "        )\n",
    "        self.movie_name = movie_name\n",
    "        self._nb_step = 0\n",
    "        if movie_name is not None:\n",
    "            self.movie_path = os.path.join(\"rddl_movies\", movie_name)\n",
    "            if not os.path.exists(self.movie_path):\n",
    "                os.makedirs(self.movie_path)\n",
    "            tmp_pngs = os.path.join(self.movie_path, \"tmp_pngs\")\n",
    "            if os.path.exists(tmp_pngs):\n",
    "                shutil.rmtree(tmp_pngs)\n",
    "            os.makedirs(tmp_pngs)\n",
    "            self.movie_gen = MovieGenerator(tmp_pngs, movie_name, max_frames=max_frames)\n",
    "            self.rddl_gym_env.set_visualizer(visualizer, self.movie_gen)\n",
    "        else:\n",
    "            self.movie_gen = None\n",
    "            self.rddl_gym_env.set_visualizer(visualizer)\n",
    "\n",
    "    def _state_step(\n",
    "        self, action: D.T_event\n",
    "    ) -> TransitionOutcome[D.T_state, Value[D.T_value], D.T_predicate, D.T_info]:\n",
    "        next_state, reward, terminated, truncated, _ = self.rddl_gym_env.step(action)\n",
    "        termination = terminated or truncated\n",
    "        if self.movie_gen is not None and (\n",
    "            termination or self._nb_step >= self.movie_gen.max_frames - 1\n",
    "        ):\n",
    "            self.movie_gen.save_animation(self.movie_name)\n",
    "            tmp_pngs = os.path.join(self.movie_path, \"tmp_pngs\")\n",
    "            shutil.move(\n",
    "                os.path.join(tmp_pngs, self.movie_name + \".gif\"),\n",
    "                os.path.join(\n",
    "                    self.movie_path,\n",
    "                    self.movie_name\n",
    "                    + \"_\"\n",
    "                    + str(dt.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "                    + \".gif\",\n",
    "                ),\n",
    "            )\n",
    "        self._nb_step += 1\n",
    "        # TransitionOutcome and Value are scikit-decide types\n",
    "        return TransitionOutcome(\n",
    "            state=next_state, value=Value(reward=reward), termination=termination\n",
    "        )\n",
    "\n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        # Cast to skdecide's GymSpace\n",
    "        return GymSpace(self.rddl_gym_env.action_space)\n",
    "\n",
    "    def _state_reset(self) -> D.T_state:\n",
    "        self._nb_step = 0\n",
    "        # SkDecide only needs the state, not the info\n",
    "        return self.rddl_gym_env.reset()[0]\n",
    "\n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        # Cast to skdecide's GymSpace\n",
    "        return GymSpace(self.rddl_gym_env.observation_space)\n",
    "\n",
    "    def _render_from(self, memory: D.T_state = None, **kwargs: Any) -> Any:\n",
    "        # We do not want the image to be displayed in a pygame window, but rather in this notebook\n",
    "        rddl_gym_img = self.rddl_gym_env.render(to_display=False)\n",
    "        clear_output(wait=True)\n",
    "        display(rddl_gym_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our scikit-decide RDDL-bridge domain, we must first search for a RDDL domain and instance.\n",
    "The pyrddlgym-project provides the [rddlrepository](https://github.com/pyrddlgym-project/rddlrepository) library of RDDL benchmarks from past IPPC competitions and third-party contributors. We list below the available problems with our pip installation of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = RDDLRepoManager(rebuild=True)\n",
    "print(sorted(manager.list_problems()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a scikit-decide `RDDLDomain` instance embedding the `MountainCar_ippc2023` benchmark. We render it using scikit-decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "problem_info = manager.get_problem(\"MountainCar_ippc2023\")\n",
    "\n",
    "\n",
    "domain_factory = lambda alg_name=None: RDDLDomain(\n",
    "    rddl_domain=problem_info.get_domain(),\n",
    "    rddl_instance=problem_info.get_instance(1),\n",
    "    visualizer=MountainCarVisualizer,\n",
    "    movie_name=\"MountainCar_ippc2023-\" + alg_name if alg_name is not None else None,\n",
    ")\n",
    "domain = domain_factory()\n",
    "domain.reset()\n",
    "domain.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the domain with scikit-decide (potentially bridged) solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part: solving the domain with scikit-decide solvers, some of them - especially the reinforcement learning ones - being bridged to state-of-the-art existing libraries (e.g. RLlib, SB3). You will see that once the domain is defined, solving it takes very few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving MountainCar_ippc2023 with RLlib's PPO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a scikit-decide's `RayRLlib` solver, then it calls the `solver.solve()` method, and it finally rollout the optimized policy by using scikit-decide's `rollout` utility function. The latter function will render the solution and the domain will generate a movie in the `rddl_movies` folder when reaching the termination condition of the rollout episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_factory = lambda: RayRLlib(\n",
    "    domain_factory=domain_factory, algo_class=RLLIB_PPO, train_iterations=10\n",
    ")\n",
    "\n",
    "with solver_factory() as solver:\n",
    "    solver.solve()\n",
    "    rollout(domain_factory(alg_name=\"RLLIB-PPO\"), solver, max_steps=300, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of executing the RLlib's PPO policy trained for 100 iterations on the mountain car benchmark:\n",
    "\n",
    "![RLLIB PPO example solution](./images/MountainCar_ippc2023-RLLIB-PPO_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving MountainCar_ippc2023 with StableBaselines-3's PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the domain is defined, very few lines of code are sufficient to test another solver whose capabilities are compatible with the domain. In the cell below, we now test Stablebaselines-3's PPO algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Special notice for binder + sb3:</b>\n",
    "it seems that <a href=https://stable-baselines3.readthedocs.io/en/master/>stable-baselines3</a> algorithms are <em>extremely slow</em> on <a href=https://mybinder.org/>binder</a>. We could not find a proper explanation about it. We strongly advise you to either launch the notebook locally or on colab, or to skip the cells that are using sb3 algorithms (here PPO).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"skdecide.utils\").setLevel(logging.INFO)\n",
    "\n",
    "solver_factory = lambda: StableBaseline(\n",
    "    domain_factory=domain_factory,\n",
    "    algo_class=SB3_PPO,\n",
    "    baselines_policy=\"MultiInputPolicy\",\n",
    "    learn_config={\"total_timesteps\": 10000},\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "with solver_factory() as solver:\n",
    "    solver.solve()\n",
    "    rollout(domain_factory(alg_name=\"SB3-PPO\"), solver, max_steps=1000, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving MountainCar_ippc2023 with CGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-decide provides an implementation of [Cartesian Genetic Programming](https://dl.acm.org/doi/10.1145/3205455.3205578) (CGP), a form of Genetic Programming which optimizes a function (e.g. control policy) by learning its best representation as a directed acyclic graph of mathematical operators. One of the great capabilities of scikit-decide is to provide simple high-level means to compare algorithms from different communities (RL, GP, search, planning, etc.) on the same domains with few lines of code.\n",
    "\n",
    "<img src=\"../rddl_images/cgp-sketch.png\" alt=\"Cartesian Genetic Programming\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our current implementation of CGP in scikit-decide does not handle complex observation spaces such as the dictionary spaces returned by the RDDL simulator, we first specialise our `RDDLDomain` in a `RDDLDomainSimplifiedSpaces` domain where all actions and observations are numpy arrays. To do this, we make use of the powerful `flatten` and `flatten_space` methods of `gymnasium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDDLDomainSimplifiedSpaces(RDDLDomain):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rddl_domain: str,\n",
    "        rddl_instance: str,\n",
    "        visualizer: BaseViz = ChartVisualizer,\n",
    "        movie_name: str = None,\n",
    "        max_frames=1000,\n",
    "    ):\n",
    "        super().__init__(rddl_domain, rddl_instance, visualizer, movie_name, max_frames)\n",
    "\n",
    "    def _state_step(\n",
    "        self, action: D.T_event\n",
    "    ) -> TransitionOutcome[D.T_state, Value[D.T_value], D.T_predicate, D.T_info]:\n",
    "        outcome = super()._state_step(action)\n",
    "        return TransitionOutcome(\n",
    "            state=flatten(self.rddl_gym_env.observation_space, outcome.state),\n",
    "            value=outcome.value,\n",
    "            termination=outcome.termination,\n",
    "        )\n",
    "\n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        return GymSpace(flatten_space(self.rddl_gym_env.action_space))\n",
    "\n",
    "    def _state_reset(self) -> D.T_state:\n",
    "        # SkDecide only needs the state, not the info\n",
    "        return flatten(self.rddl_gym_env.observation_space, super()._state_reset())\n",
    "\n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        return GymSpace(flatten_space(self.rddl_gym_env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we call the CGP solver on this simplified domain and we render the obtained solution after a few iterations (including the generation of the video in the `rddl_movies` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skdecide.hub.solver.cgp import CGP\n",
    "\n",
    "domain_factory = lambda alg_name=None: RDDLDomainSimplifiedSpaces(\n",
    "    rddl_domain=problem_info.get_domain(),\n",
    "    rddl_instance=problem_info.get_instance(1),\n",
    "    visualizer=MountainCarVisualizer,\n",
    "    movie_name=\"MountainCar_ippc2023-\" + alg_name if alg_name is not None else None,\n",
    "    max_frames=200,\n",
    ")\n",
    "\n",
    "domain = domain_factory()\n",
    "\n",
    "if os.path.exists(\"TEMP_CGP\"):\n",
    "    shutil.rmtree(\"TEMP_CGP\")\n",
    "\n",
    "# assert CGP.check_domain(domain)\n",
    "solver_factory = lambda: CGP(\n",
    "    domain_factory=domain_factory, folder_name=\"TEMP_CGP\", n_it=25, verbose=False\n",
    ")\n",
    "with solver_factory() as solver:\n",
    "    solver.solve()\n",
    "    rollout(domain_factory(\"CGP\"), solver, max_steps=200, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of executing the CGP policy on the mountain car benchmark:\n",
    "\n",
    "![CGP example solution](./images/MountainCar_ippc2023-CGP_example.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
