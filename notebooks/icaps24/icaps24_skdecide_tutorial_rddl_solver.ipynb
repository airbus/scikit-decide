{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ICAPS24 SkDecide Tutorial: Implementing a scikit-decide solver embedding the JaxPlan and GurobiPlan planners and solving RDDL-based scikit-decide domains\n",
    "\n",
    "Alexandre Arnold, Guillaume Povéda, Florent Teichteil-Königsbuch\n",
    "\n",
    "Credits to [IMACS](https://imacs.polytechnique.fr/) and especially to Nolwen Huet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This tutorial will demonstrate how to create a custom scikit-decide solver which can solve scikit-domains of whose characteristics are compatible with this solver.\n",
    "\n",
    "*NB: Since this tutorial, the pyrddl solvers jax and gurobi have been introduced into the scikit-decide hub, see for instance [this notebook](https://github.com/airbus/scikit-decide/blob/master/notebooks/16_rddl_tuto.ipynb) about the pre-implemented versions.*\n",
    "\n",
    "The \"characteristic\" rosace of scikit-decide domains show that a solver can handle all the domains whose characteristics are *more specific* than the ones of the domain for which the solver has been designed:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/airbus/scikit-decide/refs/heads/master/docs/.vuepress/public/characteristics.png\" alt=\"SkDecide characteristics\" width=\"1000\"/>\n",
    "\n",
    "For instance, as depicted in this image, if the solver can handle partially observable states, then it can solve also domains whose states are fully observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the python kernel to use for this notebook:\n",
    "- If running locally, be sure to use an environment with `scikit-decide[all]` and `pyRDDLGym-jax[dashboard]` (option needed for visualization)\n",
    "- If running on colab, the next cell does it for you.\n",
    "- If running on binder, the environment should be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import glob\n",
    "    import json\n",
    "    import sys\n",
    "\n",
    "    using_nightly_version = True\n",
    "\n",
    "    if using_nightly_version:\n",
    "        # look for nightly build download url\n",
    "        release_curl_res = !curl -L   -H \"Accept: application/vnd.github+json\" -H \"X-GitHub-Api-Version: 2022-11-28\" https://api.github.com/repos/airbus/scikit-decide/releases/tags/nightly\n",
    "        release_dict = json.loads(release_curl_res.s)\n",
    "        release_download_url = sorted(\n",
    "            release_dict[\"assets\"], key=lambda d: d[\"updated_at\"]\n",
    "        )[-1][\"browser_download_url\"]\n",
    "        print(release_download_url)\n",
    "\n",
    "        # download and unzip\n",
    "        !wget --output-document=release.zip {release_download_url}\n",
    "        !unzip -o release.zip\n",
    "\n",
    "        # get proper wheel name according to python version used\n",
    "        wheel_pythonversion_tag = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "        wheel_path = glob.glob(\n",
    "            f\"dist/scikit_decide*{wheel_pythonversion_tag}*manylinux*.whl\"\n",
    "        )[0]\n",
    "\n",
    "        skdecide_pip_spec = f\"{wheel_path}[all]\"\n",
    "    else:\n",
    "        skdecide_pip_spec = \"scikit-decide[all]\"\n",
    "\n",
    "    # install scikit-decide with all extras\n",
    "    !pip install {skdecide_pip_spec} pyRDDLGym-jax[dashboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the packages that will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime as dt\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import pyRDDLGym\n",
    "from IPython.display import clear_output\n",
    "from numpy.typing import ArrayLike\n",
    "from pyRDDLGym.core.env import RDDLEnv\n",
    "from pyRDDLGym.core.policy import RandomAgent\n",
    "from pyRDDLGym.core.simulator import RDDLSimulator\n",
    "from pyRDDLGym.core.visualizer.chart import ChartVisualizer\n",
    "from pyRDDLGym.core.visualizer.movie import MovieGenerator\n",
    "from pyRDDLGym.core.visualizer.viz import BaseViz\n",
    "from pyRDDLGym_gurobi.core.planner import GurobiOnlineController, GurobiStraightLinePlan\n",
    "from pyRDDLGym_jax.core.planner import (\n",
    "    JaxBackpropPlanner,\n",
    "    JaxOfflineController,\n",
    "    load_config,\n",
    ")\n",
    "from pyRDDLGym_jax.core.simulator import JaxRDDLSimulator\n",
    "from rddlrepository.archive.standalone.Elevators.ElevatorViz import ElevatorVisualizer\n",
    "from rddlrepository.archive.standalone.Quadcopter.QuadcopterViz import (\n",
    "    QuadcopterVisualizer,\n",
    ")\n",
    "from rddlrepository.core.manager import RDDLRepoManager\n",
    "\n",
    "from skdecide.builders.domain import FullyObservable, Renderable, UnrestrictedActions\n",
    "from skdecide.builders.solver import FromInitialState, Policies\n",
    "from skdecide.core import Space, TransitionOutcome, Value\n",
    "from skdecide.domains import RLDomain\n",
    "from skdecide.hub.space.gym import GymSpace\n",
    "from skdecide.solvers import Solver\n",
    "from skdecide.utils import rollout\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-decide's [documentation website](https://airbus.github.io) offers a simple interactive interface to select the characteristics of the solver that you want to define, along with the characteristics of the domains it is intended to solve.\n",
    "It also allows you to generate a template for the solver class to implement, containing the minimal set of methods that you have to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You must go to the [code generator](https://airbus.github.io/scikit-decide/codegen/) page and follow the instructions in the following picture. Do not forget to click on the toggle button below the \"Domain specification\" frame in order to select \"Create Solver\".\n",
    "\n",
    "<img src=\"images/skdecide_solver_generator.png\" alt=\"SkDecide domain generator\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pyRDDLGym-jax` and `pyRDDLGym-gurobi` solvers are compatible with the `pyRDDLGym` environment which we embedded in a scikit-decide `RDDLDomain` in the [tutorial notebook](https://github.com/fteicht/icaps24-skdecide-tutorial/blob/main/notebooks/icaps24_skdecide_tutorial_rddl_domain.ipynb) on creating custom scikit-decide domains. Those solvers actually exploit the logics model of the RDDL domain file description, `pyRDDLGym.model` of type `RDDLLiftedModel`. It means that our custom RDDL solver can embed the `pyRDDLGym-jax` and `pyRDDLGym-gurobi` planners and work on our custom `RDDLDomain` class whose characteristics rre `RLDomain`, `UnrestrictedActions`, `FullyObservable` and `Renderable`.\n",
    "\n",
    "Our custom solver itself will produce probabilistic Markovian policies, meaning that it should inherit from `skdecide.Solver` and `skdecide.Policies`, contrary to the example screenshot above which generates the template class for a deterministic policy solver. Our custom solver will also solve a RDDL problem from the random state sampled by the `RDDLDomain.reset()` method, which corresponds to the solver characteristic `skdecide.FromInitialState`.\n",
    "\n",
    "The code generator gives us the following solver class template which we have to fill-in:\n",
    "\n",
    "```python\n",
    "\n",
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "from skdecide.builders.solver import *\n",
    "\n",
    "\n",
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MySolver(Solver, Policies, FromInitialState):\n",
    "    T_domain = D\n",
    "    \n",
    "    def _solve(self, from_memory: Optional[D.T_state] = None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def _sample_action(self, observation: D.T_observation) -> D.T_event:\n",
    "        pass\n",
    "    \n",
    "    def _is_policy_defined_for(self, observation: D.T_observation) -> bool:\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define in the following cell the sact same `RDDLDomain` as the one we defined in the tutorial notebook on creating custom scikit-decide domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    T_state = Dict[str, Any]  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = ArrayLike  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class RDDLDomain(D):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rddl_domain: str,\n",
    "        rddl_instance: str,\n",
    "        backend: RDDLSimulator = RDDLSimulator,\n",
    "        base_class: RDDLEnv = RDDLEnv,\n",
    "        visualizer: BaseViz = ChartVisualizer,\n",
    "        movie_name: str = None,\n",
    "        max_frames=100,\n",
    "        vectorized=True,\n",
    "    ):\n",
    "        self.rddl_gym_env = pyRDDLGym.make(\n",
    "            rddl_domain,\n",
    "            rddl_instance,\n",
    "            backend=backend,\n",
    "            base_class=base_class,\n",
    "            enforce_action_constraints=True,\n",
    "            vectorized=vectorized,\n",
    "        )\n",
    "        self.movie_name = movie_name\n",
    "        self._nb_step = 0\n",
    "        if movie_name is not None:\n",
    "            self.movie_path = os.path.join(\"rddl_movies\", movie_name)\n",
    "            if not os.path.exists(self.movie_path):\n",
    "                os.makedirs(self.movie_path)\n",
    "            tmp_pngs = os.path.join(self.movie_path, \"tmp_pngs\")\n",
    "            if os.path.exists(tmp_pngs):\n",
    "                shutil.rmtree(tmp_pngs)\n",
    "            os.makedirs(tmp_pngs)\n",
    "            self.movie_gen = MovieGenerator(tmp_pngs, movie_name, max_frames=max_frames)\n",
    "            self.rddl_gym_env.set_visualizer(visualizer, self.movie_gen)\n",
    "        else:\n",
    "            self.movie_gen = None\n",
    "            self.rddl_gym_env.set_visualizer(visualizer)\n",
    "\n",
    "    def _state_step(\n",
    "        self, action: D.T_event\n",
    "    ) -> TransitionOutcome[D.T_state, Value[D.T_value], D.T_predicate, D.T_info]:\n",
    "        next_state, reward, terminated, truncated, _ = self.rddl_gym_env.step(action)\n",
    "        termination = terminated or truncated\n",
    "        if self.movie_gen is not None and (\n",
    "            termination or self._nb_step >= self.movie_gen.max_frames - 1\n",
    "        ):\n",
    "            self.movie_gen.save_animation(self.movie_name)\n",
    "            tmp_pngs = os.path.join(self.movie_path, \"tmp_pngs\")\n",
    "            shutil.move(\n",
    "                os.path.join(tmp_pngs, self.movie_name + \".gif\"),\n",
    "                os.path.join(\n",
    "                    self.movie_path,\n",
    "                    self.movie_name\n",
    "                    + \"_\"\n",
    "                    + str(dt.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "                    + \".gif\",\n",
    "                ),\n",
    "            )\n",
    "        self._nb_step += 1\n",
    "        return TransitionOutcome(\n",
    "            state=next_state, value=Value(reward=reward), termination=termination\n",
    "        )\n",
    "\n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        return GymSpace(self.rddl_gym_env.action_space)\n",
    "\n",
    "    def _state_reset(self) -> D.T_state:\n",
    "        self._nb_step = 0\n",
    "        # SkDecide only needs the state, not the info\n",
    "        return self.rddl_gym_env.reset()[0]\n",
    "\n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        return GymSpace(self.rddl_gym_env.observation_space)\n",
    "\n",
    "    def _render_from(self, memory: D.T_state = None, **kwargs: Any) -> Any:\n",
    "        # We do not want the image to be displayed in a pygame window, but rather in this notebook\n",
    "        rddl_gym_img = self.rddl_gym_env.render(to_display=False)\n",
    "        clear_output(wait=True)\n",
    "        display(rddl_gym_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual work begins now. It consists in implementing the contents of the methods from the generated solver class above. The implementation will depend on the actual underlying planner, which is why we will create specific implementations for each underlying RDDL solver. The methods to implement are:\n",
    "- `_solve()`: solves the RDDL problem by calling a series of `rddl_domain.reset()` and `rddl_domain.step()` methods; other custom solver classes would use more or different domain methods depending on the characteristics of the domains it can handle (e.g. `domain.get_next_state_distribution()`, `domain.get_observation_distribution()`, `domain.is_goal()`, etc.)\n",
    "- `_sample_action()`: samples the next action to be executed by the agent for a given state according to the solver's optimized policy distribution; for a history-dependent solver, the method would take as input a history of observations rather than just the current one;\n",
    "- `is_policy_defined_for()`: indicates whether an action has been computed and is available in the given state; the RDDL planners we are using in this notebook do not explose this information in their Python API, which is why we will always return True from this method in our custom RDDL solver class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Note: </b> The methods to implement are prefixed with '_' to indicate that they should be considered as protected, i.e. they should not be used by a user of the domain class like a solver. As for the domain builder classes, the solver builder classes most generally manage in background the automated casting of domain features to the solver's expected ones (e.g. single agent state to a multi-agent dictionary state if the domain is single-agent and the solver is multi-agent), appropriate re-implementation of methods when walking down the characteristic class hierarchies, and the LRU cache for those methods that can cache their results for later reuse. The user of the solver class, like the `skdecide.utils.rollout()` methods should always use the non-prefixed version of the solver methods. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the simple RDDL random agent which we embed in the `RDDLRandomSolver` below.\n",
    "The solver instance's constructor takes as input the domain factory lambda function, as usual in the solvers provided by scikit-decide.\n",
    "\n",
    "We show how to make additional compatibility checks on the domain beyond the domain characteristics checks, by implementing the `_check_domain_additional()` method. In this simple case, we check that the domain class provide `rddl_gym_env` attribute which is defined in the `RDDLDomain` class defined earlier in this notebook. This method will be called internally, along with other internal domain cahracteristic checking methods, by scikit-decide when the user calls `RDDLRandomSolver.check_domain(domain_instance)`.\n",
    "\n",
    "The `_solve()` method of the rangom agent only creates a `pyRDDLGym.core.policy.RandomAgent` controller, while the `_sample_action()` method calls the `sample_action()` method of the aforementioned controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(RDDLDomain):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RDDLRandomSolver(Solver, Policies, FromInitialState):\n",
    "    T_domain = D\n",
    "\n",
    "    def __init__(self, domain_factory: Callable[[], RDDLDomain]):\n",
    "        Solver.__init__(self, domain_factory=domain_factory)\n",
    "        self._domain = domain_factory()\n",
    "\n",
    "    @classmethod\n",
    "    def _check_domain_additional(cls, domain: D) -> bool:\n",
    "        return hasattr(domain, \"rddl_gym_env\")\n",
    "\n",
    "    def _solve(self, from_memory: Optional[D.T_state] = None) -> None:\n",
    "        self.controller = RandomAgent(\n",
    "            action_space=self._domain.rddl_gym_env.action_space,\n",
    "            num_actions=self._domain.rddl_gym_env.max_allowed_actions,\n",
    "        )\n",
    "\n",
    "    def _sample_action(self, observation: D.T_observation) -> D.T_event:\n",
    "        return self.controller.sample_action(observation)\n",
    "\n",
    "    def _is_policy_defined_for(self, observation: D.T_observation) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the Quadcopter RDDL problem from the `rddlrepository`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = RDDLRepoManager(rebuild=True)\n",
    "problem_info = manager.get_problem(\"Quadcopter\")\n",
    "problem_visualizer = QuadcopterVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the domain factory to be used by the random agent, and render it from the initial state returned by the `domain.reset()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"matplotlib.font_manager\").disabled = True\n",
    "logging.getLogger(\"skdecide.utils\").setLevel(logging.INFO)\n",
    "\n",
    "domain_factory_random_agent = lambda max_frames=None: RDDLDomain(\n",
    "    rddl_domain=problem_info.get_domain(),\n",
    "    rddl_instance=problem_info.get_instance(1),\n",
    "    visualizer=problem_visualizer,\n",
    "    movie_name=\"Quadcopter-RandomAgent\" if max_frames is not None else None,\n",
    "    max_frames=max_frames if max_frames is not None else 100,\n",
    ")\n",
    "domain = domain_factory_random_agent()\n",
    "domain.reset()\n",
    "domain.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to execute the random policy governed by the `RDDLRandomSolver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert RDDLRandomSolver.check_domain(domain_factory_random_agent())\n",
    "\n",
    "with RDDLRandomSolver(domain_factory=domain_factory_random_agent) as solver:\n",
    "    solver.solve()\n",
    "    rollout(\n",
    "        domain_factory_random_agent(max_frames=100),\n",
    "        solver,\n",
    "        max_steps=100,\n",
    "        render=True,\n",
    "        max_framerate=5,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the resulting solution, which obviously does not reach the goal:\n",
    "\n",
    "![RandomAgent example solution](images/Quadcopter-RandomAgent_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try the offline version of [JaxPlan](https://openreview.net/forum?id=7IKtmUpLEH) planner which compiles the RDDL model to a Jax computation graph allowing for planning by backpropagation. Don't miss the presentation of the paper this year at ICAPS!\n",
    "\n",
    "Now, the scikit-decide `_solve()` method creates the `JaxBackpropPlanner` and the `JaxOfflineController`. The latter is used in `_sample_action()` to sample the next action to execute. The solver instance's constructor also takes the configuration file of the `Jax` planner as explained [here](https://github.com/pyrddlgym-project/pyRDDLGym-jax/tree/main?tab=readme-ov-file#writing-a-configuration-file-for-a-custom-domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(RDDLDomain):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RDDLJaxSolver(Solver, Policies, FromInitialState):\n",
    "    T_domain = D\n",
    "\n",
    "    def __init__(\n",
    "        self, domain_factory: Callable[[], RDDLDomain], config: Optional[str] = None\n",
    "    ):\n",
    "        Solver.__init__(self, domain_factory=domain_factory)\n",
    "        self._domain = domain_factory()\n",
    "        if config is not None:\n",
    "            self.planner_args, _, self.train_args = load_config(config)\n",
    "\n",
    "    @classmethod\n",
    "    def _check_domain_additional(cls, domain: D) -> bool:\n",
    "        return hasattr(domain, \"rddl_gym_env\")\n",
    "\n",
    "    def _solve(self, from_memory: Optional[D.T_state] = None) -> None:\n",
    "        planner = JaxBackpropPlanner(\n",
    "            rddl=self._domain.rddl_gym_env.model,\n",
    "            **(self.planner_args if self.planner_args is not None else {}),\n",
    "        )\n",
    "        self.controller = JaxOfflineController(\n",
    "            planner, **(self.train_args if self.train_args is not None else {})\n",
    "        )\n",
    "\n",
    "    def _sample_action(self, observation: D.T_observation) -> D.T_event:\n",
    "        return self.controller.sample_action(observation)\n",
    "\n",
    "    def _is_policy_defined_for(self, observation: D.T_observation) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we optimize the problem with `JaxPlan` and execute the resulting policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Quadcopter_slp.cfg\"):\n",
    "    !wget https://raw.githubusercontent.com/pyrddlgym-project/pyRDDLGym-jax/main/pyRDDLGym_jax/examples/configs/Quadcopter_slp.cfg\n",
    "\n",
    "domain_factory_jax_agent = lambda max_frames=None: RDDLDomain(\n",
    "    rddl_domain=problem_info.get_domain(),\n",
    "    rddl_instance=problem_info.get_instance(1),\n",
    "    visualizer=problem_visualizer,\n",
    "    backend=JaxRDDLSimulator,\n",
    "    movie_name=\"Quadcopter-JaxAgent\" if max_frames is not None else None,\n",
    "    max_frames=max_frames if max_frames is not None else 500,\n",
    ")\n",
    "\n",
    "assert RDDLJaxSolver.check_domain(domain_factory_jax_agent())\n",
    "\n",
    "with RDDLJaxSolver(\n",
    "    domain_factory=domain_factory_jax_agent, config=\"Quadcopter_slp.cfg\"\n",
    ") as solver:\n",
    "    solver.solve()\n",
    "    rollout(\n",
    "        domain_factory_jax_agent(max_frames=500),\n",
    "        solver,\n",
    "        max_steps=500,\n",
    "        render=True,\n",
    "        max_framerate=5,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following example execution of the `Jax` policy, which now clearly converges towards the goal (quadcopters flying towards the red triangle):\n",
    "\n",
    "![JaxAgent example solution](images/Quadcopter-JaxAgent_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gurobi Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally try the online version of [GurobiPlan](https://openreview.net/forum?id=7IKtmUpLEH) planner which compiles the RDDL model to a Gurobi MILP model. Don't miss the presentation of the paper this year at ICAPS!\n",
    "\n",
    "Now, the scikit-decide `_solve()` method creates the `GurobiStraightLinePlan` and the `GurobiOnlineController`. The latter is used in `_sample_action()` to sample the next action to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(RDDLDomain):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RDDLGurobiSolver(Solver, Policies, FromInitialState):\n",
    "    T_domain = D\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        domain_factory: Callable[[], RDDLDomain],\n",
    "        config: Optional[str] = None,\n",
    "        rollout_horizon=5,\n",
    "    ):\n",
    "        Solver.__init__(self, domain_factory=domain_factory)\n",
    "        self._domain = domain_factory()\n",
    "        self._rollout_horizon = rollout_horizon\n",
    "        if config is not None:\n",
    "            self.planner_args, _, self.train_args = load_config(config)\n",
    "\n",
    "    @classmethod\n",
    "    def _check_domain_additional(cls, domain: D) -> bool:\n",
    "        return hasattr(domain, \"rddl_gym_env\")\n",
    "\n",
    "    def _solve(self, from_memory: Optional[D.T_state] = None) -> None:\n",
    "        plan = GurobiStraightLinePlan()\n",
    "        self.controller = GurobiOnlineController(\n",
    "            rddl=self._domain.rddl_gym_env.model,\n",
    "            plan=plan,\n",
    "            rollout_horizon=self._rollout_horizon,\n",
    "            model_params={\"NonConvex\": 2, \"OutputFlag\": 0},\n",
    "        )\n",
    "\n",
    "    def _sample_action(self, observation: D.T_observation) -> D.T_event:\n",
    "        return self.controller.sample_action(observation)\n",
    "\n",
    "    def _is_policy_defined_for(self, observation: D.T_observation) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try this solver on one of the well-known `Elevators` benchmarks.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note: </b>\n",
    "To solve reasonable size problems, the solver needs a real license for Gurobi, as the free license available when installing gurobipy from PyPi is not sufficient to solve this domain. Here we limit the `rollout_horizon` to be able to run it with the free license, because optimization variables are created for each timestep.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "problem_info = manager.get_problem(\"Elevators\")\n",
    "problem_visualizer = ElevatorVisualizer\n",
    "\n",
    "domain_factory_gurobi_agent = lambda max_frames=None: RDDLDomain(\n",
    "    rddl_domain=problem_info.get_domain(),\n",
    "    rddl_instance=problem_info.get_instance(0),\n",
    "    visualizer=problem_visualizer,\n",
    "    movie_name=\"Elevators-GurobiAgent\" if max_frames is not None else None,\n",
    "    max_frames=max_frames if max_frames is not None else 500,\n",
    "    vectorized=False,\n",
    ")\n",
    "\n",
    "assert RDDLGurobiSolver.check_domain(domain_factory_gurobi_agent())\n",
    "\n",
    "with RDDLGurobiSolver(\n",
    "    domain_factory=domain_factory_gurobi_agent,\n",
    "    rollout_horizon=2,  # you can increase it with a real license (e.g. 10)\n",
    ") as solver:\n",
    "    solver.solve()\n",
    "    rollout(\n",
    "        domain_factory_gurobi_agent(max_frames=50),\n",
    "        solver,\n",
    "        max_steps=50,\n",
    "        render=True,\n",
    "        max_framerate=5,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of executing the online `GurobiPlan` strategy on this benchmark:\n",
    "\n",
    "![GurobiAgent example solution](images/Elevators-GurobiAgent_example.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
