(window.webpackJsonp=window.webpackJsonp||[]).push([[162],{672:function(e,a,t){"use strict";t.r(a);var n=t(38),r=Object(n.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"hub-solver-stable-baselines-gnn-ppo-mask-ppo-mask"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hub-solver-stable-baselines-gnn-ppo-mask-ppo-mask"}},[e._v("#")]),e._v(" hub.solver.stable_baselines.gnn.ppo_mask.ppo_mask")]),e._v(" "),t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"custom-block-title"},[e._v("Domain specification")]),e._v(" "),t("skdecide-summary")],1),e._v(" "),t("h2",{attrs:{id:"maskablegraphppo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#maskablegraphppo"}},[e._v("#")]),e._v(" MaskableGraphPPO")]),e._v(" "),t("p",[e._v("Proximal Policy Optimization algorithm (PPO) with graph observations and action masking.")]),e._v(" "),t("p",[e._v("It is meant to be applied to a gymnasium environment")]),e._v(" "),t("ul",[t("li",[e._v("whose observation space is\n"),t("ul",[t("li",[e._v("either a "),t("code",[e._v("gymnasium.spaces.Graph")]),e._v(' => you should use policy="GraphInputPolicy",')]),e._v(" "),t("li",[e._v("or a "),t("code",[e._v("gymnasium.spaces.Dict")]),e._v(" with some subspaces being "),t("code",[e._v("gymnasium.spaces.Graph")]),e._v('\n=> you should use policy="MultiInputPolicy"')])])]),e._v(" "),t("li",[e._v("whose action space is enumerable")]),e._v(" "),t("li",[e._v("which is exposing a method "),t("code",[e._v("action_masks()")]),e._v(" returning a numpy array of same length as\nthe number of actions, filled with 0's and 1's corresponding to applicability of the action\n(0: not applicable, 1: applicable)")])]),e._v(" "),t("p",[e._v("The policies will use a GNN to extract features from the observation before being plug to a MLP for prediction.")]),e._v(" "),t("h3",{attrs:{id:"constructor"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#constructor"}},[e._v("#")]),e._v(" Constructor "),t("Badge",{attrs:{text:"MaskableGraphPPO",type:"tip"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"MaskableGraphPPO",sig:{params:[{name:"policy",annotation:"typing.Union[str, type[stable_baselines3.common.policies.ActorCriticPolicy]]"},{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"rollout_buffer_class",default:"None",annotation:"typing.Optional[type[stable_baselines3.common.buffers.RolloutBuffer]]"},{name:"**kwargs"}]}}}),e._v(" "),t("p",[e._v("Initialize self.  See help(type(self)) for accurate signature.")]),e._v(" "),t("h3",{attrs:{id:"collect-rollouts"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#collect-rollouts"}},[e._v("#")]),e._v(" collect_rollouts "),t("Badge",{attrs:{text:"OnPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"collect_rollouts",sig:{params:[{name:"self"},{name:"env",annotation:"<class 'stable_baselines3.common.vec_env.base_vec_env.VecEnv'>"},{name:"callback",annotation:"<class 'stable_baselines3.common.callbacks.BaseCallback'>"},{name:"rollout_buffer",annotation:"<class 'stable_baselines3.common.buffers.RolloutBuffer'>"},{name:"n_rollout_steps",annotation:"<class 'int'>"},{name:"use_masking",default:"False",annotation:"<class 'bool'>"}],return:"<class 'bool'>"}}}),e._v(" "),t("p",[e._v("Collect experiences using the current policy and fill a "),t("code",[e._v("RolloutBuffer")]),e._v(".\nThe term rollout here refers to the model-free notion and should not\nbe used with the concept of rollout used in model-based RL or planning.")]),e._v(" "),t("p",[e._v("This method is largely identical to the implementation found in the parent class and MaskablePPO.")]),e._v(" "),t("p",[e._v(":param env: The training environment\n:param callback: Callback that will be called at each step\n(and at the beginning and end of the rollout)\n:param rollout_buffer: Buffer to fill with rollouts\n:param n_rollout_steps: Number of experiences to collect per environment\n:param use_masking: Whether to use invalid action masks during training\n:return: True if function returned with at least "),t("code",[e._v("n_rollout_steps")]),e._v("\ncollected, False if callback terminated rollout prematurely.")]),e._v(" "),t("h3",{attrs:{id:"get-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-env"}},[e._v("#")]),e._v(" get_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_env",sig:{params:[{name:"self"}],return:"typing.Optional[stable_baselines3.common.vec_env.base_vec_env.VecEnv]"}}}),e._v(" "),t("p",[e._v("Returns the current environment (can be None if not defined).")]),e._v(" "),t("p",[e._v(":return: The current environment")]),e._v(" "),t("h3",{attrs:{id:"get-parameters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-parameters"}},[e._v("#")]),e._v(" get_parameters "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Dict]"}}}),e._v(" "),t("p",[e._v("Return the parameters of the agent. This includes parameters from different networks, e.g.\ncritics (value functions) and policies (pi functions).")]),e._v(" "),t("p",[e._v(":return: Mapping of from names of the objects to PyTorch state-dicts.")]),e._v(" "),t("h3",{attrs:{id:"get-vec-normalize-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-vec-normalize-env"}},[e._v("#")]),e._v(" get_vec_normalize_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_vec_normalize_env",sig:{params:[{name:"self"}],return:"typing.Optional[stable_baselines3.common.vec_env.vec_normalize.VecNormalize]"}}}),e._v(" "),t("p",[e._v("Return the "),t("code",[e._v("VecNormalize")]),e._v(" wrapper of the training env\nif it exists.")]),e._v(" "),t("p",[e._v(":return: The "),t("code",[e._v("VecNormalize")]),e._v(" env.")]),e._v(" "),t("h3",{attrs:{id:"learn"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#learn"}},[e._v("#")]),e._v(" learn "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"learn",sig:{params:[{name:"self",annotation:"~SelfMaskablePPO"},{name:"total_timesteps",annotation:"<class 'int'>"},{name:"callback",default:"None",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"log_interval",default:"1",annotation:"<class 'int'>"},{name:"tb_log_name",default:"PPO",annotation:"<class 'str'>"},{name:"reset_num_timesteps",default:"True",annotation:"<class 'bool'>"},{name:"use_masking",default:"True",annotation:"<class 'bool'>"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"~SelfMaskablePPO"}}}),e._v(" "),t("p",[e._v("Return a trained model.")]),e._v(" "),t("p",[e._v(":param total_timesteps: The total number of samples (env steps) to train on\n:param callback: callback(s) called at every step with state of the algorithm.\n:param log_interval: for on-policy algos (e.g., PPO, A2C, ...) this is the number of\ntraining iterations (i.e., log_interval * n_steps * n_envs timesteps) before logging;\nfor off-policy algos (e.g., TD3, SAC, ...) this is the number of episodes before\nlogging.\n:param tb_log_name: the name of the run for TensorBoard logging\n:param reset_num_timesteps: whether or not to reset the current timestep number (used in logging)\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: the trained model")]),e._v(" "),t("h3",{attrs:{id:"load"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load"}},[e._v("#")]),e._v(" load "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"env",default:"None",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv'), NoneType]"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"},{name:"custom_objects",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"print_system_info",default:"False",annotation:"<class 'bool'>"},{name:"force_reset",default:"True",annotation:"<class 'bool'>"},{name:"**kwargs"}],return:"~SelfBaseAlgorithm"}}}),e._v(" "),t("p",[e._v("Load the model from a zip-file.\nWarning: "),t("code",[e._v("load")]),e._v(" re-creates the model from scratch, it does not update it in-place!\nFor an in-place load use "),t("code",[e._v("set_parameters")]),e._v(" instead.")]),e._v(" "),t("p",[e._v(":param path: path to the file (or a file-like) where to\nload the agent from\n:param env: the new environment to run the loaded model on\n(can be None if you only need prediction from a trained model) has priority over any saved environment\n:param device: Device on which the code should run.\n:param custom_objects: Dictionary of objects to replace\nupon loading. If a variable is present in this dictionary as a\nkey, it will not be deserialized and the corresponding item\nwill be used instead. Similar to custom_objects in\n"),t("code",[e._v("keras.models.load_model")]),e._v(". Useful when you have an object in\nfile that can not be deserialized.\n:param print_system_info: Whether to print system info from the saved model\nand the current system info (useful to debug loading issues)\n:param force_reset: Force call to "),t("code",[e._v("reset()")]),e._v(" before training\nto avoid unexpected behavior.\nSee https://github.com/DLR-RM/stable-baselines3/issues/597\n:param kwargs: extra arguments to change the model when loading\n:return: new model instance with loaded parameters")]),e._v(" "),t("h3",{attrs:{id:"predict"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#predict"}},[e._v("#")]),e._v(" predict "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),t("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),t("p",[e._v(":param observation: the input observation\n:param state: The last hidden states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\nthis correspond to beginning of episodes,\nwhere the hidden states of the RNN must be reset.\n:param deterministic: Whether or not to return deterministic actions.\n:return: the model's action and the next hidden state\n(used in recurrent policies)")]),e._v(" "),t("h3",{attrs:{id:"save"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#save"}},[e._v("#")]),e._v(" save "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"exclude",default:"None",annotation:"typing.Optional[typing.Iterable[str]]"},{name:"include",default:"None",annotation:"typing.Optional[typing.Iterable[str]]"}],return:null}}}),e._v(" "),t("p",[e._v("Save all the attributes of the object and the model parameters in a zip-file.")]),e._v(" "),t("p",[e._v(":param path: path to the file where the rl agent should be saved\n:param exclude: name of parameters that should be excluded in addition to the default ones\n:param include: name of parameters that might be excluded but should be included anyway")]),e._v(" "),t("h3",{attrs:{id:"set-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-env"}},[e._v("#")]),e._v(" set_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_env",sig:{params:[{name:"self"},{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"force_reset",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),t("p",[e._v("Checks the validity of the environment, and if it is coherent, set it as the current environment.\nFurthermore wrap any non vectorized env into a vectorized\nchecked parameters:")]),e._v(" "),t("ul",[t("li",[e._v("observation_space")]),e._v(" "),t("li",[e._v("action_space")])]),e._v(" "),t("p",[e._v(":param env: The environment for learning a policy\n:param force_reset: Force call to "),t("code",[e._v("reset()")]),e._v(" before training\nto avoid unexpected behavior.\nSee issue https://github.com/DLR-RM/stable-baselines3/issues/597")]),e._v(" "),t("h3",{attrs:{id:"set-logger"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-logger"}},[e._v("#")]),e._v(" set_logger "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_logger",sig:{params:[{name:"self"},{name:"logger",annotation:"<class 'stable_baselines3.common.logger.Logger'>"}],return:null}}}),e._v(" "),t("p",[e._v("Setter for for logger object.")]),e._v(" "),t("p",[e._v(".. warning::")]),e._v(" "),t("p",[e._v("When passing a custom logger object,\nthis will overwrite "),t("code",[e._v("tensorboard_log")]),e._v(" and "),t("code",[e._v("verbose")]),e._v(" settings\npassed to the constructor.")]),e._v(" "),t("h3",{attrs:{id:"set-parameters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-parameters"}},[e._v("#")]),e._v(" set_parameters "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_parameters",sig:{params:[{name:"self"},{name:"load_path_or_dict",annotation:"typing.Union[str, typing.Dict[str, torch.Tensor]]"},{name:"exact_match",default:"True",annotation:"<class 'bool'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:null}}}),e._v(" "),t("p",[e._v("Load parameters from a given zip-file or a nested dictionary containing parameters for\ndifferent modules (see "),t("code",[e._v("get_parameters")]),e._v(").")]),e._v(" "),t("p",[e._v(":param load_path_or_iter: Location of the saved data (path or file-like, see "),t("code",[e._v("save")]),e._v("), or a nested\ndictionary containing nn.Module parameters used by the policy. The dictionary maps\nobject names to a state-dictionary returned by "),t("code",[e._v("torch.nn.Module.state_dict()")]),e._v(".\n:param exact_match: If True, the given parameters should include parameters for each\nmodule and each of their parameters, otherwise raises an Exception. If set to False, this\ncan be used to update only specific parameters.\n:param device: Device on which the code should run.")]),e._v(" "),t("h3",{attrs:{id:"set-random-seed"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-random-seed"}},[e._v("#")]),e._v(" set_random_seed "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_random_seed",sig:{params:[{name:"self"},{name:"seed",default:"None",annotation:"typing.Optional[int]"}],return:null}}}),e._v(" "),t("p",[e._v("Set the seed of the pseudo-random generators\n(python, numpy, pytorch, gym, action_space)")]),e._v(" "),t("p",[e._v(":param seed:")]),e._v(" "),t("h3",{attrs:{id:"train"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#train"}},[e._v("#")]),e._v(" train "),t("Badge",{attrs:{text:"OnPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Update policy using the currently gathered rollout buffer.")]),e._v(" "),t("h3",{attrs:{id:"dump-logs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dump-logs"}},[e._v("#")]),e._v(" _dump_logs "),t("Badge",{attrs:{text:"OnPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_dump_logs",sig:{params:[{name:"self"},{name:"iteration",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),t("p",[e._v("Write log.")]),e._v(" "),t("p",[e._v(":param iteration: Current logging iteration")]),e._v(" "),t("h3",{attrs:{id:"excluded-save-params"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#excluded-save-params"}},[e._v("#")]),e._v(" _excluded_save_params "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_excluded_save_params",sig:{params:[{name:"self"}],return:"typing.List[str]"}}}),e._v(" "),t("p",[e._v("Returns the names of the parameters that should be excluded from being\nsaved by pickling. E.g. replay buffers are skipped by default\nas they take up a lot of space. PyTorch variables should be excluded\nwith this so they can be stored with "),t("code",[e._v("th.save")]),e._v(".")]),e._v(" "),t("p",[e._v(":return: List of parameters that should be excluded from being saved with pickle.")]),e._v(" "),t("h3",{attrs:{id:"get-policy-from-name"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-policy-from-name"}},[e._v("#")]),e._v(" _get_policy_from_name "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_get_policy_from_name",sig:{params:[{name:"self"},{name:"policy_name",annotation:"<class 'str'>"}],return:"typing.Type[stable_baselines3.common.policies.BasePolicy]"}}}),e._v(" "),t("p",[e._v("Get a policy class from its name representation.")]),e._v(" "),t("p",[e._v('The goal here is to standardize policy naming, e.g.\nall algorithms can call upon "MlpPolicy" or "CnnPolicy",\nand they receive respective policies that work for them.')]),e._v(" "),t("p",[e._v(":param policy_name: Alias of the policy\n:return: A policy class (type)")]),e._v(" "),t("h3",{attrs:{id:"get-torch-save-params"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-torch-save-params"}},[e._v("#")]),e._v(" _get_torch_save_params "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_get_torch_save_params",sig:{params:[{name:"self"}],return:"typing.Tuple[typing.List[str], typing.List[str]]"}}}),e._v(" "),t("p",[e._v("Get the name of the torch variables that will be saved with\nPyTorch "),t("code",[e._v("th.save")]),e._v(", "),t("code",[e._v("th.load")]),e._v(" and "),t("code",[e._v("state_dicts")]),e._v(" instead of the default\npickling strategy. This is to handle device placement correctly.")]),e._v(" "),t("p",[e._v('Names can point to specific variables under classes, e.g.\n"policy.optimizer" would point to '),t("code",[e._v("optimizer")]),e._v(" object of "),t("code",[e._v("self.policy")]),e._v("\nif this object.")]),e._v(" "),t("p",[e._v(":return:\nList of Torch variables whose state dicts to save (e.g. th.nn.Modules),\nand list of other Torch variables to store with "),t("code",[e._v("th.save")]),e._v(".")]),e._v(" "),t("h3",{attrs:{id:"init-callback"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init-callback"}},[e._v("#")]),e._v(" _init_callback "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_init_callback",sig:{params:[{name:"self"},{name:"callback",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"<class 'stable_baselines3.common.callbacks.BaseCallback'>"}}}),e._v(" "),t("p",[e._v(":param callback: Callback(s) called at every step with state of the algorithm.\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: A hybrid callback calling "),t("code",[e._v("callback")]),e._v(" and performing evaluation.")]),e._v(" "),t("h3",{attrs:{id:"setup-learn"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-learn"}},[e._v("#")]),e._v(" _setup_learn "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_learn",sig:{params:[{name:"self"},{name:"total_timesteps",annotation:"<class 'int'>"},{name:"callback",default:"None",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"reset_num_timesteps",default:"True",annotation:"<class 'bool'>"},{name:"tb_log_name",default:"run",annotation:"<class 'str'>"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[int, stable_baselines3.common.callbacks.BaseCallback]"}}}),e._v(" "),t("p",[e._v("Initialize different variables needed for training.")]),e._v(" "),t("p",[e._v(":param total_timesteps: The total number of samples (env steps) to train on\n:param callback: Callback(s) called at every step with state of the algorithm.\n:param reset_num_timesteps: Whether to reset or not the "),t("code",[e._v("num_timesteps")]),e._v(" attribute\n:param tb_log_name: the name of the run for tensorboard log\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: Total timesteps and callback(s)")]),e._v(" "),t("h3",{attrs:{id:"setup-lr-schedule"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-lr-schedule"}},[e._v("#")]),e._v(" _setup_lr_schedule "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_lr_schedule",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Transform to callable if needed.")]),e._v(" "),t("h3",{attrs:{id:"setup-model"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-model"}},[e._v("#")]),e._v(" _setup_model "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_model",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Create networks, buffer and optimizers.")]),e._v(" "),t("h3",{attrs:{id:"update-current-progress-remaining"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-current-progress-remaining"}},[e._v("#")]),e._v(" _update_current_progress_remaining "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_current_progress_remaining",sig:{params:[{name:"self"},{name:"num_timesteps",annotation:"<class 'int'>"},{name:"total_timesteps",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),t("p",[e._v("Compute current progress remaining (starts from 1 and ends to 0)")]),e._v(" "),t("p",[e._v(":param num_timesteps: current number of timesteps\n:param total_timesteps:")]),e._v(" "),t("h3",{attrs:{id:"update-info-buffer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-info-buffer"}},[e._v("#")]),e._v(" _update_info_buffer "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_info_buffer",sig:{params:[{name:"self"},{name:"infos",annotation:"typing.List[typing.Dict[str, typing.Any]]"},{name:"dones",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:null}}}),e._v(" "),t("p",[e._v("Retrieve reward, episode length, episode success and update the buffer\nif using Monitor wrapper or a GoalEnv.")]),e._v(" "),t("p",[e._v(":param infos: List of additional information about the transition.\n:param dones: Termination signals")]),e._v(" "),t("h3",{attrs:{id:"update-learning-rate"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-learning-rate"}},[e._v("#")]),e._v(" _update_learning_rate "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_learning_rate",sig:{params:[{name:"self"},{name:"optimizers",annotation:"typing.Union[typing.List[torch.optim.optimizer.Optimizer], torch.optim.optimizer.Optimizer]"}],return:null}}}),e._v(" "),t("p",[e._v("Update the optimizers learning rate using the current learning rate schedule\nand the current progress remaining (from 1 to 0).")]),e._v(" "),t("p",[e._v(":param optimizers:\nAn optimizer or a list of optimizers.")]),e._v(" "),t("h3",{attrs:{id:"wrap-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wrap-env"}},[e._v("#")]),e._v(" _wrap_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_wrap_env",sig:{params:[{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"verbose",default:"0",annotation:"<class 'int'>"},{name:"monitor_wrapper",default:"True",annotation:"<class 'bool'>"}],return:"<class 'stable_baselines3.common.vec_env.base_vec_env.VecEnv'>"}}}),e._v(" "),t("p",[e._v('"\nWrap environment with the appropriate wrappers if needed.\nFor instance, to have a vectorized environment\nor to re-order the image channels.')]),e._v(" "),t("p",[e._v(":param env:\n:param verbose: Verbosity level: 0 for no output, 1 for indicating wrappers used\n:param monitor_wrapper: Whether to wrap the env in a "),t("code",[e._v("Monitor")]),e._v(" when possible.\n:return: The wrapped environment.")]),e._v(" "),t("h2",{attrs:{id:"maskablegraph2nodeppo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#maskablegraph2nodeppo"}},[e._v("#")]),e._v(" MaskableGraph2NodePPO")]),e._v(" "),t("p",[e._v("Proximal Policy Optimization algorithm (PPO) with graph observations, node actions, and action masking.")]),e._v(" "),t("p",[e._v("It is meant to be applied to a gymnasium environment:")]),e._v(" "),t("ul",[t("li",[e._v("such that an observation is a graph represented by a "),t("code",[e._v("gymnasium.spaces.GraphInstance")]),e._v(",")]),e._v(" "),t("li",[e._v("such that an action is a node of the observation graph, represented by its index (integer between 0 and number of nodes).")]),e._v(" "),t("li",[e._v("exposing a method "),t("code",[e._v("action_masks()")]),e._v(" returning a numpy array of same length as the number of nodes,\nfilled with 0's and 1's corresponding to applicability of the action (0: not applicable, 1: applicable)")])]),e._v(" "),t("p",[e._v("So the observation space should be a "),t("code",[e._v("gymnasium.spaces.Graph")]),e._v("\nand the action space should be a "),t("code",[e._v("gymnasium.spaces.Discrete")]),e._v(" even though the actual number of actions is derived at\nruntime from the number of nodes in the current observation.")]),e._v(" "),t("p",[e._v("The policy will (see "),t("code",[e._v("MaskableGNN2NodeActorCriticPolicy")]),e._v(" for further details):")]),e._v(" "),t("ul",[t("li",[e._v("to predict the value: extract features with a GNN before applying a MLP")]),e._v(" "),t("li",[e._v("to predict the action: use another GNN whose nodes embedding will directly be used as logits associated to each node")])]),e._v(" "),t("p",[e._v("NB: The number of nodes (and the size of the action mask) may be variable from an observation to another.")]),e._v(" "),t("h3",{attrs:{id:"constructor-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#constructor-2"}},[e._v("#")]),e._v(" Constructor "),t("Badge",{attrs:{text:"MaskableGraph2NodePPO",type:"tip"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"MaskableGraph2NodePPO",sig:{params:[{name:"policy",annotation:"typing.Union[str, type[stable_baselines3.common.policies.ActorCriticPolicy]]"},{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"rollout_buffer_class",default:"None",annotation:"typing.Optional[type[stable_baselines3.common.buffers.RolloutBuffer]]"},{name:"**kwargs"}]}}}),e._v(" "),t("p",[e._v("Initialize self.  See help(type(self)) for accurate signature.")]),e._v(" "),t("h3",{attrs:{id:"collect-rollouts-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#collect-rollouts-2"}},[e._v("#")]),e._v(" collect_rollouts "),t("Badge",{attrs:{text:"OnPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"collect_rollouts",sig:{params:[{name:"self"},{name:"env",annotation:"<class 'stable_baselines3.common.vec_env.base_vec_env.VecEnv'>"},{name:"callback",annotation:"<class 'stable_baselines3.common.callbacks.BaseCallback'>"},{name:"rollout_buffer",annotation:"<class 'stable_baselines3.common.buffers.RolloutBuffer'>"},{name:"n_rollout_steps",annotation:"<class 'int'>"},{name:"use_masking",default:"False",annotation:"<class 'bool'>"}],return:"<class 'bool'>"}}}),e._v(" "),t("p",[e._v("Collect experiences using the current policy and fill a "),t("code",[e._v("RolloutBuffer")]),e._v(".\nThe term rollout here refers to the model-free notion and should not\nbe used with the concept of rollout used in model-based RL or planning.")]),e._v(" "),t("p",[e._v("This method is largely identical to the implementation found in the parent class and MaskablePPO.")]),e._v(" "),t("p",[e._v(":param env: The training environment\n:param callback: Callback that will be called at each step\n(and at the beginning and end of the rollout)\n:param rollout_buffer: Buffer to fill with rollouts\n:param n_rollout_steps: Number of experiences to collect per environment\n:param use_masking: Whether to use invalid action masks during training\n:return: True if function returned with at least "),t("code",[e._v("n_rollout_steps")]),e._v("\ncollected, False if callback terminated rollout prematurely.")]),e._v(" "),t("h3",{attrs:{id:"get-env-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-env-2"}},[e._v("#")]),e._v(" get_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_env",sig:{params:[{name:"self"}],return:"typing.Optional[stable_baselines3.common.vec_env.base_vec_env.VecEnv]"}}}),e._v(" "),t("p",[e._v("Returns the current environment (can be None if not defined).")]),e._v(" "),t("p",[e._v(":return: The current environment")]),e._v(" "),t("h3",{attrs:{id:"get-parameters-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-parameters-2"}},[e._v("#")]),e._v(" get_parameters "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Dict]"}}}),e._v(" "),t("p",[e._v("Return the parameters of the agent. This includes parameters from different networks, e.g.\ncritics (value functions) and policies (pi functions).")]),e._v(" "),t("p",[e._v(":return: Mapping of from names of the objects to PyTorch state-dicts.")]),e._v(" "),t("h3",{attrs:{id:"get-vec-normalize-env-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-vec-normalize-env-2"}},[e._v("#")]),e._v(" get_vec_normalize_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_vec_normalize_env",sig:{params:[{name:"self"}],return:"typing.Optional[stable_baselines3.common.vec_env.vec_normalize.VecNormalize]"}}}),e._v(" "),t("p",[e._v("Return the "),t("code",[e._v("VecNormalize")]),e._v(" wrapper of the training env\nif it exists.")]),e._v(" "),t("p",[e._v(":return: The "),t("code",[e._v("VecNormalize")]),e._v(" env.")]),e._v(" "),t("h3",{attrs:{id:"learn-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#learn-2"}},[e._v("#")]),e._v(" learn "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"learn",sig:{params:[{name:"self",annotation:"~SelfMaskablePPO"},{name:"total_timesteps",annotation:"<class 'int'>"},{name:"callback",default:"None",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"log_interval",default:"1",annotation:"<class 'int'>"},{name:"tb_log_name",default:"PPO",annotation:"<class 'str'>"},{name:"reset_num_timesteps",default:"True",annotation:"<class 'bool'>"},{name:"use_masking",default:"True",annotation:"<class 'bool'>"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"~SelfMaskablePPO"}}}),e._v(" "),t("p",[e._v("Return a trained model.")]),e._v(" "),t("p",[e._v(":param total_timesteps: The total number of samples (env steps) to train on\n:param callback: callback(s) called at every step with state of the algorithm.\n:param log_interval: for on-policy algos (e.g., PPO, A2C, ...) this is the number of\ntraining iterations (i.e., log_interval * n_steps * n_envs timesteps) before logging;\nfor off-policy algos (e.g., TD3, SAC, ...) this is the number of episodes before\nlogging.\n:param tb_log_name: the name of the run for TensorBoard logging\n:param reset_num_timesteps: whether or not to reset the current timestep number (used in logging)\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: the trained model")]),e._v(" "),t("h3",{attrs:{id:"load-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load-2"}},[e._v("#")]),e._v(" load "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"env",default:"None",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv'), NoneType]"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"},{name:"custom_objects",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"print_system_info",default:"False",annotation:"<class 'bool'>"},{name:"force_reset",default:"True",annotation:"<class 'bool'>"},{name:"**kwargs"}],return:"~SelfBaseAlgorithm"}}}),e._v(" "),t("p",[e._v("Load the model from a zip-file.\nWarning: "),t("code",[e._v("load")]),e._v(" re-creates the model from scratch, it does not update it in-place!\nFor an in-place load use "),t("code",[e._v("set_parameters")]),e._v(" instead.")]),e._v(" "),t("p",[e._v(":param path: path to the file (or a file-like) where to\nload the agent from\n:param env: the new environment to run the loaded model on\n(can be None if you only need prediction from a trained model) has priority over any saved environment\n:param device: Device on which the code should run.\n:param custom_objects: Dictionary of objects to replace\nupon loading. If a variable is present in this dictionary as a\nkey, it will not be deserialized and the corresponding item\nwill be used instead. Similar to custom_objects in\n"),t("code",[e._v("keras.models.load_model")]),e._v(". Useful when you have an object in\nfile that can not be deserialized.\n:param print_system_info: Whether to print system info from the saved model\nand the current system info (useful to debug loading issues)\n:param force_reset: Force call to "),t("code",[e._v("reset()")]),e._v(" before training\nto avoid unexpected behavior.\nSee https://github.com/DLR-RM/stable-baselines3/issues/597\n:param kwargs: extra arguments to change the model when loading\n:return: new model instance with loaded parameters")]),e._v(" "),t("h3",{attrs:{id:"predict-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#predict-2"}},[e._v("#")]),e._v(" predict "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),t("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),t("p",[e._v(":param observation: the input observation\n:param state: The last hidden states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\nthis correspond to beginning of episodes,\nwhere the hidden states of the RNN must be reset.\n:param deterministic: Whether or not to return deterministic actions.\n:return: the model's action and the next hidden state\n(used in recurrent policies)")]),e._v(" "),t("h3",{attrs:{id:"save-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#save-2"}},[e._v("#")]),e._v(" save "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"exclude",default:"None",annotation:"typing.Optional[typing.Iterable[str]]"},{name:"include",default:"None",annotation:"typing.Optional[typing.Iterable[str]]"}],return:null}}}),e._v(" "),t("p",[e._v("Save all the attributes of the object and the model parameters in a zip-file.")]),e._v(" "),t("p",[e._v(":param path: path to the file where the rl agent should be saved\n:param exclude: name of parameters that should be excluded in addition to the default ones\n:param include: name of parameters that might be excluded but should be included anyway")]),e._v(" "),t("h3",{attrs:{id:"set-env-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-env-2"}},[e._v("#")]),e._v(" set_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_env",sig:{params:[{name:"self"},{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"force_reset",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),t("p",[e._v("Checks the validity of the environment, and if it is coherent, set it as the current environment.\nFurthermore wrap any non vectorized env into a vectorized\nchecked parameters:")]),e._v(" "),t("ul",[t("li",[e._v("observation_space")]),e._v(" "),t("li",[e._v("action_space")])]),e._v(" "),t("p",[e._v(":param env: The environment for learning a policy\n:param force_reset: Force call to "),t("code",[e._v("reset()")]),e._v(" before training\nto avoid unexpected behavior.\nSee issue https://github.com/DLR-RM/stable-baselines3/issues/597")]),e._v(" "),t("h3",{attrs:{id:"set-logger-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-logger-2"}},[e._v("#")]),e._v(" set_logger "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_logger",sig:{params:[{name:"self"},{name:"logger",annotation:"<class 'stable_baselines3.common.logger.Logger'>"}],return:null}}}),e._v(" "),t("p",[e._v("Setter for for logger object.")]),e._v(" "),t("p",[e._v(".. warning::")]),e._v(" "),t("p",[e._v("When passing a custom logger object,\nthis will overwrite "),t("code",[e._v("tensorboard_log")]),e._v(" and "),t("code",[e._v("verbose")]),e._v(" settings\npassed to the constructor.")]),e._v(" "),t("h3",{attrs:{id:"set-parameters-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-parameters-2"}},[e._v("#")]),e._v(" set_parameters "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_parameters",sig:{params:[{name:"self"},{name:"load_path_or_dict",annotation:"typing.Union[str, typing.Dict[str, torch.Tensor]]"},{name:"exact_match",default:"True",annotation:"<class 'bool'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:null}}}),e._v(" "),t("p",[e._v("Load parameters from a given zip-file or a nested dictionary containing parameters for\ndifferent modules (see "),t("code",[e._v("get_parameters")]),e._v(").")]),e._v(" "),t("p",[e._v(":param load_path_or_iter: Location of the saved data (path or file-like, see "),t("code",[e._v("save")]),e._v("), or a nested\ndictionary containing nn.Module parameters used by the policy. The dictionary maps\nobject names to a state-dictionary returned by "),t("code",[e._v("torch.nn.Module.state_dict()")]),e._v(".\n:param exact_match: If True, the given parameters should include parameters for each\nmodule and each of their parameters, otherwise raises an Exception. If set to False, this\ncan be used to update only specific parameters.\n:param device: Device on which the code should run.")]),e._v(" "),t("h3",{attrs:{id:"set-random-seed-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-random-seed-2"}},[e._v("#")]),e._v(" set_random_seed "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_random_seed",sig:{params:[{name:"self"},{name:"seed",default:"None",annotation:"typing.Optional[int]"}],return:null}}}),e._v(" "),t("p",[e._v("Set the seed of the pseudo-random generators\n(python, numpy, pytorch, gym, action_space)")]),e._v(" "),t("p",[e._v(":param seed:")]),e._v(" "),t("h3",{attrs:{id:"train-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#train-2"}},[e._v("#")]),e._v(" train "),t("Badge",{attrs:{text:"OnPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Update policy using the currently gathered rollout buffer.")]),e._v(" "),t("h3",{attrs:{id:"dump-logs-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dump-logs-2"}},[e._v("#")]),e._v(" _dump_logs "),t("Badge",{attrs:{text:"OnPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_dump_logs",sig:{params:[{name:"self"},{name:"iteration",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),t("p",[e._v("Write log.")]),e._v(" "),t("p",[e._v(":param iteration: Current logging iteration")]),e._v(" "),t("h3",{attrs:{id:"excluded-save-params-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#excluded-save-params-2"}},[e._v("#")]),e._v(" _excluded_save_params "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_excluded_save_params",sig:{params:[{name:"self"}],return:"typing.List[str]"}}}),e._v(" "),t("p",[e._v("Returns the names of the parameters that should be excluded from being\nsaved by pickling. E.g. replay buffers are skipped by default\nas they take up a lot of space. PyTorch variables should be excluded\nwith this so they can be stored with "),t("code",[e._v("th.save")]),e._v(".")]),e._v(" "),t("p",[e._v(":return: List of parameters that should be excluded from being saved with pickle.")]),e._v(" "),t("h3",{attrs:{id:"get-policy-from-name-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-policy-from-name-2"}},[e._v("#")]),e._v(" _get_policy_from_name "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_get_policy_from_name",sig:{params:[{name:"self"},{name:"policy_name",annotation:"<class 'str'>"}],return:"typing.Type[stable_baselines3.common.policies.BasePolicy]"}}}),e._v(" "),t("p",[e._v("Get a policy class from its name representation.")]),e._v(" "),t("p",[e._v('The goal here is to standardize policy naming, e.g.\nall algorithms can call upon "MlpPolicy" or "CnnPolicy",\nand they receive respective policies that work for them.')]),e._v(" "),t("p",[e._v(":param policy_name: Alias of the policy\n:return: A policy class (type)")]),e._v(" "),t("h3",{attrs:{id:"get-torch-save-params-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-torch-save-params-2"}},[e._v("#")]),e._v(" _get_torch_save_params "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_get_torch_save_params",sig:{params:[{name:"self"}],return:"typing.Tuple[typing.List[str], typing.List[str]]"}}}),e._v(" "),t("p",[e._v("Get the name of the torch variables that will be saved with\nPyTorch "),t("code",[e._v("th.save")]),e._v(", "),t("code",[e._v("th.load")]),e._v(" and "),t("code",[e._v("state_dicts")]),e._v(" instead of the default\npickling strategy. This is to handle device placement correctly.")]),e._v(" "),t("p",[e._v('Names can point to specific variables under classes, e.g.\n"policy.optimizer" would point to '),t("code",[e._v("optimizer")]),e._v(" object of "),t("code",[e._v("self.policy")]),e._v("\nif this object.")]),e._v(" "),t("p",[e._v(":return:\nList of Torch variables whose state dicts to save (e.g. th.nn.Modules),\nand list of other Torch variables to store with "),t("code",[e._v("th.save")]),e._v(".")]),e._v(" "),t("h3",{attrs:{id:"init-callback-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init-callback-2"}},[e._v("#")]),e._v(" _init_callback "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_init_callback",sig:{params:[{name:"self"},{name:"callback",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"<class 'stable_baselines3.common.callbacks.BaseCallback'>"}}}),e._v(" "),t("p",[e._v(":param callback: Callback(s) called at every step with state of the algorithm.\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: A hybrid callback calling "),t("code",[e._v("callback")]),e._v(" and performing evaluation.")]),e._v(" "),t("h3",{attrs:{id:"setup-learn-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-learn-2"}},[e._v("#")]),e._v(" _setup_learn "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_learn",sig:{params:[{name:"self"},{name:"total_timesteps",annotation:"<class 'int'>"},{name:"callback",default:"None",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"reset_num_timesteps",default:"True",annotation:"<class 'bool'>"},{name:"tb_log_name",default:"run",annotation:"<class 'str'>"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[int, stable_baselines3.common.callbacks.BaseCallback]"}}}),e._v(" "),t("p",[e._v("Initialize different variables needed for training.")]),e._v(" "),t("p",[e._v(":param total_timesteps: The total number of samples (env steps) to train on\n:param callback: Callback(s) called at every step with state of the algorithm.\n:param reset_num_timesteps: Whether to reset or not the "),t("code",[e._v("num_timesteps")]),e._v(" attribute\n:param tb_log_name: the name of the run for tensorboard log\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: Total timesteps and callback(s)")]),e._v(" "),t("h3",{attrs:{id:"setup-lr-schedule-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-lr-schedule-2"}},[e._v("#")]),e._v(" _setup_lr_schedule "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_lr_schedule",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Transform to callable if needed.")]),e._v(" "),t("h3",{attrs:{id:"setup-model-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-model-2"}},[e._v("#")]),e._v(" _setup_model "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_model",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Create networks, buffer and optimizers.")]),e._v(" "),t("h3",{attrs:{id:"update-current-progress-remaining-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-current-progress-remaining-2"}},[e._v("#")]),e._v(" _update_current_progress_remaining "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_current_progress_remaining",sig:{params:[{name:"self"},{name:"num_timesteps",annotation:"<class 'int'>"},{name:"total_timesteps",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),t("p",[e._v("Compute current progress remaining (starts from 1 and ends to 0)")]),e._v(" "),t("p",[e._v(":param num_timesteps: current number of timesteps\n:param total_timesteps:")]),e._v(" "),t("h3",{attrs:{id:"update-info-buffer-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-info-buffer-2"}},[e._v("#")]),e._v(" _update_info_buffer "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_info_buffer",sig:{params:[{name:"self"},{name:"infos",annotation:"typing.List[typing.Dict[str, typing.Any]]"},{name:"dones",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:null}}}),e._v(" "),t("p",[e._v("Retrieve reward, episode length, episode success and update the buffer\nif using Monitor wrapper or a GoalEnv.")]),e._v(" "),t("p",[e._v(":param infos: List of additional information about the transition.\n:param dones: Termination signals")]),e._v(" "),t("h3",{attrs:{id:"update-learning-rate-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-learning-rate-2"}},[e._v("#")]),e._v(" _update_learning_rate "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_learning_rate",sig:{params:[{name:"self"},{name:"optimizers",annotation:"typing.Union[typing.List[torch.optim.optimizer.Optimizer], torch.optim.optimizer.Optimizer]"}],return:null}}}),e._v(" "),t("p",[e._v("Update the optimizers learning rate using the current learning rate schedule\nand the current progress remaining (from 1 to 0).")]),e._v(" "),t("p",[e._v(":param optimizers:\nAn optimizer or a list of optimizers.")]),e._v(" "),t("h3",{attrs:{id:"wrap-env-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wrap-env-2"}},[e._v("#")]),e._v(" _wrap_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_wrap_env",sig:{params:[{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"verbose",default:"0",annotation:"<class 'int'>"},{name:"monitor_wrapper",default:"True",annotation:"<class 'bool'>"}],return:"<class 'stable_baselines3.common.vec_env.base_vec_env.VecEnv'>"}}}),e._v(" "),t("p",[e._v('"\nWrap environment with the appropriate wrappers if needed.\nFor instance, to have a vectorized environment\nor to re-order the image channels.')]),e._v(" "),t("p",[e._v(":param env:\n:param verbose: Verbosity level: 0 for no output, 1 for indicating wrappers used\n:param monitor_wrapper: Whether to wrap the env in a "),t("code",[e._v("Monitor")]),e._v(" when possible.\n:return: The wrapped environment.")])],1)}),[],!1,null,null,null);a.default=r.exports}}]);