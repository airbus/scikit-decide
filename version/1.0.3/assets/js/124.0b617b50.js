(window.webpackJsonp=window.webpackJsonp||[]).push([[124],{645:function(e,t,a){"use strict";a.r(t);var n=a(38),r=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"hub-solver-ray-rllib-action-masking-models-torch-parametric-actions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hub-solver-ray-rllib-action-masking-models-torch-parametric-actions"}},[e._v("#")]),e._v(" hub.solver.ray_rllib.action_masking.models.torch.parametric_actions")]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("Domain specification")]),e._v(" "),a("skdecide-summary")],1),e._v(" "),a("h2",{attrs:{id:"torchparametricactionsmodel"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#torchparametricactionsmodel"}},[e._v("#")]),e._v(" TorchParametricActionsModel")]),e._v(" "),a("p",[e._v("Parametric action model that handles the dot product and masking and\nthat also learns action embeddings. PyTorch version.")]),e._v(" "),a("p",[e._v("This assumes the outputs are logits for a single Categorical action dist.")]),e._v(" "),a("h3",{attrs:{id:"constructor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"TorchParametricActionsModel",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"TorchParametricActionsModel",sig:{params:[{name:"obs_space"},{name:"action_space"},{name:"num_outputs"},{name:"model_config"},{name:"name"},{name:"**kw"}]}}}),e._v(" "),a("p",[e._v("Initialize a TorchModelV2.")]),e._v(" "),a("p",[e._v("Here is an example implementation for a subclass\n"),a("code",[e._v("MyModelClass(TorchModelV2, nn.Module)")]),e._v("::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("def __init__(self, *args, **kwargs):\n    TorchModelV2.__init__(self, *args, **kwargs)\n    nn.Module.__init__(self)\n    self._hidden_layers = nn.Sequential(...)\n    self._logits = ...\n    self._value_branch = ...\n")])])]),a("h3",{attrs:{id:"add-module"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"context"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#context"}},[e._v("#")]),e._v(" context "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"context",sig:{params:[{name:"self"}],return:"<class 'contextlib.AbstractContextManager'>"}}}),e._v(" "),a("p",[e._v("Returns a contextmanager for the current forward pass.")]),e._v(" "),a("h3",{attrs:{id:"cpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"custom-loss"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#custom-loss"}},[e._v("#")]),e._v(" custom_loss "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"custom_loss",sig:{params:[{name:"self"},{name:"policy_loss",annotation:"typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]"},{name:"loss_inputs",annotation:"typing.Dict[str, typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]]"}],return:"typing.Union[typing.List[typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]], <built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]"}}}),e._v(" "),a("p",[e._v("Override to customize the loss function used to optimize this model.")]),e._v(" "),a("p",[e._v("This can be used to incorporate self-supervised losses (by defining\na loss over existing input and output tensors of this model), and\nsupervised losses (by defining losses over a variable-sharing copy of\nthis model's layers).")]),e._v(" "),a("p",[e._v("You can find an runnable example in examples/custom_loss.py.")]),e._v(" "),a("p",[e._v("Args:\npolicy_loss: List of or single policy loss(es) from the policy.\nloss_inputs: map of input placeholders for rollout data.")]),e._v(" "),a("p",[e._v("Returns:\nList of or scalar tensor for the customized loss(es) for this\nmodel.")]),e._v(" "),a("h3",{attrs:{id:"double"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"extra-repr"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"float"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"input_dict"},{name:"state"},{name:"seq_lens"}]}}}),e._v(" "),a("p",[e._v("Call the model with the given input tensors and state.")]),e._v(" "),a("p",[e._v("Any complex observations (dicts, tuples, etc.) will be unpacked by\n"),a("strong",[e._v("call")]),e._v(' before being passed to forward(). To access the flattened\nobservation tensor, refer to input_dict["obs_flat"].')]),e._v(" "),a("p",[e._v("This method can be called any number of times. In eager execution,\neach call to forward() will eagerly evaluate the model. In symbolic\nexecution, each call to forward creates a computation graph that\noperates over the variables of this model (i.e., shares weights).")]),e._v(" "),a("p",[e._v("Custom models should override this instead of "),a("strong",[e._v("call")]),e._v(".")]),e._v(" "),a("p",[e._v('Args:\ninput_dict: dictionary of input tensors, including "obs",\n"obs_flat", "prev_action", "prev_reward", "is_training",\n"eps_id", "agent_id", "infos", and "t".\nstate: list of state tensors with sizes matching those\nreturned by get_initial_state + the batch dimension\nseq_lens: 1d tensor holding input sequence lengths')]),e._v(" "),a("p",[e._v("Returns:\nA tuple consisting of the model output tensor of size\n[BATCH, num_outputs] and the list of new RNN state(s) if any.")]),e._v(" "),a("p",[e._v(".. testcode::\n:skipif: True")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('import numpy as np\nfrom ray.rllib.models.modelv2 import ModelV2\nclass MyModel(ModelV2):\n    # ...\n    def forward(self, input_dict, state, seq_lens):\n        model_out, self._value_out = self.base_model(\n            input_dict["obs"])\n        return model_out, state\n')])])]),a("h3",{attrs:{id:"get-buffer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-initial-state"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-initial-state"}},[e._v("#")]),e._v(" get_initial_state "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_initial_state",sig:{params:[{name:"self"}],return:"typing.List[typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]]"}}}),e._v(" "),a("p",[e._v("Get the initial recurrent state values for the model.")]),e._v(" "),a("p",[e._v("Returns:\nList of np.array (for tf) or Tensor (for torch) objects containing the\ninitial hidden state of an RNN, if applicable.")]),e._v(" "),a("p",[e._v(".. testcode::\n:skipif: True")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("import numpy as np\nfrom ray.rllib.models.modelv2 import ModelV2\nclass MyModel(ModelV2):\n    # ...\n    def get_initial_state(self):\n        return [\n            np.zeros(self.cell_size, np.float32),\n            np.zeros(self.cell_size, np.float32),\n        ]\n")])])]),a("h3",{attrs:{id:"get-parameter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"ipu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-time-major"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-time-major"}},[e._v("#")]),e._v(" is_time_major "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_time_major",sig:{params:[{name:"self"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("If True, data for calling this ModelV2 must be in time-major format.")]),e._v(" "),a("p",[e._v("Returns\nWhether this ModelV2 requires a time-major (TxBx...) data\nformat.")]),e._v(" "),a("h3",{attrs:{id:"last-output"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#last-output"}},[e._v("#")]),e._v(" last_output "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"last_output",sig:{params:[{name:"self"}],return:"typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]"}}}),e._v(" "),a("p",[e._v("Returns the last output returned from calling the model.")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"metrics"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#metrics"}},[e._v("#")]),e._v(" metrics "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"metrics",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]]"}}}),e._v(" "),a("p",[e._v("Override to return custom metrics from your model.")]),e._v(" "),a("p",[e._v('The stats will be reported as part of the learner stats, i.e.,\ninfo.learner.[policy_id, e.g. "default_policy"].model.key1=metric1')]),e._v(" "),a("p",[e._v("Returns:\nThe custom metrics for this model.")]),e._v(" "),a("h3",{attrs:{id:"modules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"register-backward-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"share-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"trainable-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#trainable-variables"}},[e._v("#")]),e._v(" trainable_variables "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"trainable_variables",sig:{params:[{name:"self"},{name:"as_dict",default:"False",annotation:"<class 'bool'>"}],return:"typing.Union[typing.List[typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]], typing.Dict[str, typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]]]"}}}),e._v(" "),a("p",[e._v("Returns the list of trainable variables for this model.")]),e._v(" "),a("p",[e._v("Args:\nas_dict: Whether variables should be returned as dict-values\n(using descriptive keys).")]),e._v(" "),a("p",[e._v("Returns:\nThe list (or dict if "),a("code",[e._v("as_dict")]),e._v(" is True) of all trainable\n(tf)/requires_grad (torch) variables of this ModelV2.")]),e._v(" "),a("h3",{attrs:{id:"type"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"value-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#value-function"}},[e._v("#")]),e._v(" value_function "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"value_function",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Returns the value function output for the most recent forward pass.")]),e._v(" "),a("p",[e._v("Note that a "),a("code",[e._v("forward")]),e._v(" call has to be performed first, before this\nmethods can return anything and thus that calling this method does not\ncause an extra forward pass through the network.")]),e._v(" "),a("p",[e._v("Returns:\nValue estimate tensor of shape [BATCH].")]),e._v(" "),a("h3",{attrs:{id:"variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#variables"}},[e._v("#")]),e._v(" variables "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"variables",sig:{params:[{name:"self"},{name:"as_dict",default:"False",annotation:"<class 'bool'>"}],return:"typing.Union[typing.List[typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]], typing.Dict[str, typing.Union[<built-in function array>, ForwardRef('jnp.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor')]]]"}}}),e._v(" "),a("p",[e._v("Returns the list (or a dict) of variables for this model.")]),e._v(" "),a("p",[e._v("Args:\nas_dict: Whether variables should be returned as dict-values\n(using descriptive str keys).")]),e._v(" "),a("p",[e._v("Returns:\nThe list (or dict if "),a("code",[e._v("as_dict")]),e._v(" is True) of all variables of this\nModelV2.")]),e._v(" "),a("h3",{attrs:{id:"xpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"annotated-type"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#annotated-type"}},[e._v("#")]),e._v(" _annotated_type "),a("Badge",{attrs:{text:"ModelV2",type:"warn"}})],1),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-2"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")])],1)}),[],!1,null,null,null);t.default=r.exports}}]);