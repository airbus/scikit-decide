(window.webpackJsonp=window.webpackJsonp||[]).push([[172],{683:function(e,t,a){"use strict";a.r(t);var o=a(38),n=Object(o.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"optuna-utils"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#optuna-utils"}},[e._v("#")]),e._v(" optuna_utils")]),e._v(" "),a("p",[e._v("Utilities to create optuna studies for scikit-decide.")]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("Domain specification")]),e._v(" "),a("skdecide-summary")],1),e._v(" "),a("h2",{attrs:{id:"generic-optuna-experiment-monoproblem"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#generic-optuna-experiment-monoproblem"}},[e._v("#")]),e._v(" generic_optuna_experiment_monoproblem")]),e._v(" "),a("skdecide-signature",{attrs:{name:"generic_optuna_experiment_monoproblem",sig:{params:[{name:"domain_factory",annotation:"Callable[[], Domain]"},{name:"solver_classes",annotation:"list[type[Solver]]"},{name:"kwargs_fixed_by_solver",default:"None",annotation:"Optional[dict[type[Solver], dict[str, Any]]]"},{name:"suggest_optuna_kwargs_by_name_by_solver",default:"None",annotation:"Optional[dict[type[Solver], dict[str, dict[str, Any]]]]"},{name:"additional_hyperparameters_by_solver",default:"None",annotation:"Optional[dict[type[Solver], list[Hyperparameter]]]"},{name:"n_trials",default:"150",annotation:"int"},{name:"allow_retry_same_trial",default:"False",annotation:"bool"},{name:"rollout_num_episodes",default:"3",annotation:"int"},{name:"rollout_max_steps_by_episode",default:"1000",annotation:"int"},{name:"rollout_from_memory",default:"None",annotation:"Optional[D.T_memory[D.T_state]]"},{name:"domain_reset_is_deterministic",default:"False",annotation:"bool"},{name:"study_basename",default:"study",annotation:"str"},{name:"create_another_study",default:"True",annotation:"bool"},{name:"overwrite_study",default:"False"},{name:"storage_path",default:"./optuna-journal.log",annotation:"str"},{name:"sampler",default:"None",annotation:"Optional[BaseSampler]"},{name:"pruner",default:"None",annotation:"Optional[BasePruner]"},{name:"seed",default:"None",annotation:"Optional[int]"},{name:"objective",default:"None",annotation:"Optional[Callable[[Solver, list[tuple[list[D.T_agent[D.T_observation]], list[D.T_agent[D.T_concurrency[D.T_event]]], list[D.T_agent[Value[D.T_value]]]]]], float]]"},{name:"optuna_tuning_direction",default:"maximize",annotation:"str"},{name:"alternative_domain_factory",default:"None",annotation:"Optional[dict[type[Solver], Callable[[], Domain]]]"}],return:"optuna.Study"}}}),e._v(" "),a("p",[e._v("Create and run an optuna study to tune solvers hyperparameters for a given domain factory.")]),e._v(" "),a("p",[e._v("The optuna study will choose a solver and its hyperparameters in order to optimize\nthe cumulated reward during a rollout.")]),e._v(" "),a("p",[e._v("When")]),e._v(" "),a("ul",[a("li",[e._v("solver policy is deterministic,")]),e._v(" "),a("li",[e._v("domain treansitions are deterministic,")]),e._v(" "),a("li",[e._v("domain state to observation is deterministic,")]),e._v(" "),a("li",[e._v("and rollout starts from a specified memory or domain.reset() is deterministic,\nwe avoid repeating episode are they will be all the same.")])]),e._v(" "),a("p",[e._v("One can")]),e._v(" "),a("ul",[a("li",[e._v("freeze some hyperparameters via kwargs_fixed_by_solver")]),e._v(" "),a("li",[e._v("tune solvers."),a("strong",[e._v("init")]),e._v(" via kwargs_fixed_by_solver")]),e._v(" "),a("li",[e._v("restrict the choices/ranges for some hyperparameters via suggest_optuna_kwargs_by_name_by_solver")]),e._v(" "),a("li",[e._v("add other hyperparameters to some solvers via additional_hyperparameters_by_solver")])]),e._v(" "),a("p",[e._v("The optuna study can be monitored with optuna-dashboard with")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("optuna-dashboard optuna-journal.log\n")])])]),a("p",[e._v("(or the relevant path set by "),a("code",[e._v("storage_path")]),e._v(")")]),e._v(" "),a("h4",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[e._v("#")]),e._v(" Parameters")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("domain_factory")]),e._v(": a callable with no argument returning the domain to solve (can be a mere domain class).")]),e._v(" "),a("li",[a("strong",[e._v("solver_classes")]),e._v(": list of solvers to consider.")]),e._v(" "),a("li",[a("strong",[e._v("kwargs_fixed_by_solver")]),e._v(": fixed hyperparameters by solver. Can also be other parameters needed by solvers' "),a("strong",[e._v("init")]),e._v("().")]),e._v(" "),a("li",[a("strong",[e._v("suggest_optuna_kwargs_by_name_by_solver")]),e._v(": kwargs_by_name passed to solvers' suggest_with_optuna().\nUseful to restrict or specify choices, step, high, ...")]),e._v(" "),a("li",[a("strong",[e._v("additional_hyperparameters_by_solver")]),e._v(": additional user-defined hyperparameters by solver, to be suggested by optuna")]),e._v(" "),a("li",[a("strong",[e._v("n_trials")]),e._v(": number of trials to be run in the optuna study")]),e._v(" "),a("li",[a("strong",[e._v("allow_retry_same_trial")]),e._v(": if True, allow trial with same parameters as before to be retried (useful if solve process is random for instance)")]),e._v(" "),a("li",[a("strong",[e._v("rollout_num_episodes")]),e._v(": nb of episodes used in rollout to compute the value associated to a set of hyperparameters")]),e._v(" "),a("li",[a("strong",[e._v("rollout_max_steps_by_episode")]),e._v(": max steps by episode used in rollout to compute the value associated to a set of hyperparameters")]),e._v(" "),a("li",[a("strong",[e._v("rollout_from_memory")]),e._v(": if specified, rollout episode will start from this memory")]),e._v(" "),a("li",[a("strong",[e._v("domain_reset_is_deterministic")]),e._v(": specified whether the domain reset() method (when existing) is deterministic.\nThis information is used when rollout_from_memory is None (and thus domain.reset() is used) ,\nto decide if several episodes are needed or not, depending on whether everything is deterministic or not.")]),e._v(" "),a("li",[a("strong",[e._v("study_basename")]),e._v(": base name of the study generated.\nIf "),a("code",[e._v("create_another_study")]),e._v(" is True, a timestamp will be added to this base name.")]),e._v(" "),a("li",[a("strong",[e._v("create_another_study")]),e._v(": if "),a("code",[e._v("True")]),e._v(" a timestamp prefix will be added to the study base name in order to avoid\noverwritting or continuing a previously created study.\nShould be False, if one wants to add trials to an existing study.")]),e._v(" "),a("li",[a("strong",[e._v("overwrite_study")]),e._v(": if True, any study with the same name as the one generated here will be deleted before starting the optuna study.\nShould be False, if one wants to add trials to an existing study.")]),e._v(" "),a("li",[a("strong",[e._v("storage_path")]),e._v(": path to the journal used by optuna used to log the study. Can be a NFS path to allow parallelized optuna studies.")]),e._v(" "),a("li",[a("strong",[e._v("sampler")]),e._v(": sampler used by the optuna study. If None, a TPESampler is used with the provided "),a("code",[e._v("seed")]),e._v(".")]),e._v(" "),a("li",[a("strong",[e._v("pruner")]),e._v(": pruner used by the optuna study. if None, a MedianPruner is used.")]),e._v(" "),a("li",[a("strong",[e._v("seed")]),e._v(": used to create the sampler if "),a("code",[e._v("sampler")]),e._v(" is None. Should be set to an integer if one wants to ensure\nreproducible results.")]),e._v(" "),a("li",[a("strong",[e._v("aggreg_outcome_rewards")]),e._v(": function used to aggregate outcome.value into a scalar.\nDefault to taking "),a("code",[e._v("float(outcome.value.reward)")]),e._v(" for single agent solver,\nand to taking "),a("code",[e._v("sum(float(v.reward) for v in outcome.value.values())")]),e._v(" for multi agents solver.")]),e._v(" "),a("li",[a("strong",[e._v("objective")]),e._v(": function used to compute the scalar optimized by optuna.\nTakes solver and episodes obtain by solve + rollout as arguments and should return a float.\nEpisodes being a list of episode represented by a tuple of observations, actions, values.\nDefault to the cumulated reward other all episodes (and all agents when on a multiagent domain).")]),e._v(" "),a("li",[a("strong",[e._v("optuna_tuning_direction")]),e._v(': direction of optuna optimization ("maximize" or "minimize")')]),e._v(" "),a("li",[a("strong",[e._v("alternative_domain_factory")]),e._v(": mapping solver_class -> domain_factory when some solvers need a different domain_factory\n(e.g. width-based solvers need GymDomainForWidthSolvers instead of simple GymDomain)")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("p",[e._v("the launched optuna study.")])],1)}),[],!1,null,null,null);t.default=n.exports}}]);