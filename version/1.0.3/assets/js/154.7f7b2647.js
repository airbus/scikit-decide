(window.webpackJsonp=window.webpackJsonp||[]).push([[154],{687:function(e,t,a){"use strict";a.r(t);var n=a(38),r=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"hub-solver-stable-baselines-gnn-common-policies"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hub-solver-stable-baselines-gnn-common-policies"}},[e._v("#")]),e._v(" hub.solver.stable_baselines.gnn.common.policies")]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("Domain specification")]),e._v(" "),a("skdecide-summary")],1),e._v(" "),a("h2",{attrs:{id:"gnnactorcriticpolicy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#gnnactorcriticpolicy"}},[e._v("#")]),e._v(" GNNActorCriticPolicy")]),e._v(" "),a("p",[e._v("Policy predicting from an observation graph.")]),e._v(" "),a("p",[e._v("Features are extracted from the graph thanks to a GNN\nfollowed by a reduction layer to a fixed number of features\n(see "),a("code",[e._v("GraphFeaturesExtractor")]),e._v(" for further details).")]),e._v(" "),a("h3",{attrs:{id:"constructor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"GNNActorCriticPolicy",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"GNNActorCriticPolicy",sig:{params:[{name:"observation_space",annotation:"<class 'gymnasium.spaces.graph.Graph'>"},{name:"action_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"},{name:"net_arch",default:"None",annotation:"typing.Optional[list[typing.Union[int, dict[str, list[int]]]]]"},{name:"activation_fn",default:"<class 'torch.nn.modules.activation.Tanh'>",annotation:"type[torch.nn.modules.module.Module]"},{name:"ortho_init",default:"True",annotation:"<class 'bool'>"},{name:"use_sde",default:"False",annotation:"<class 'bool'>"},{name:"log_std_init",default:"0.0",annotation:"<class 'float'>"},{name:"full_std",default:"True",annotation:"<class 'bool'>"},{name:"use_expln",default:"False",annotation:"<class 'bool'>"},{name:"squash_output",default:"False",annotation:"<class 'bool'>"},{name:"features_extractor_class",default:"<class 'skdecide.hub.solver.stable_baselines.gnn.common.torch_layers.GraphFeaturesExtractor'>",annotation:"type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"},{name:"features_extractor_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"share_features_extractor",default:"True",annotation:"<class 'bool'>"},{name:"normalize_images",default:"True",annotation:"<class 'bool'>"},{name:"optimizer_class",default:"<class 'torch.optim.adam.Adam'>",annotation:"type[torch.optim.optimizer.Optimizer]"},{name:"optimizer_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"debug",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Initialize internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("h3",{attrs:{id:"add-module"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"cpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"double"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"evaluate-actions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluate-actions"}},[e._v("#")]),e._v(" evaluate_actions "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"evaluate_actions",sig:{params:[{name:"self"},{name:"obs",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"actions",annotation:"<class 'torch.Tensor'>"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, typing.Optional[torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Evaluate actions according to the current policy,\ngiven the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param actions: Actions\n:return: estimated value, log likelihood of taking those actions\nand entropy of the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"extra-repr"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"extract-features"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extract-features"}},[e._v("#")]),e._v(" extract_features "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extract_features",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Preprocess the observation if needed and extract features.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param features_extractor: The features extractor to use. If None, then "),a("code",[e._v("self.features_extractor")]),e._v(" is used.\n:return: The extracted features. If features extractor is not shared, returns a tuple with the\nfeatures for the actor and the features for the critic.")]),e._v(" "),a("h3",{attrs:{id:"float"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch.Tensor'>"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Forward pass in all the networks (actor and critic)")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param deterministic: Whether to sample or use deterministic actions\n:return: action, value and log probability of the action")]),e._v(" "),a("h3",{attrs:{id:"get-buffer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-parameter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"init-weights"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init-weights"}},[e._v("#")]),e._v(" init_weights "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"init_weights",sig:{params:[{name:"module",annotation:"<class 'torch.nn.modules.module.Module'>"},{name:"gain",default:"1",annotation:"<class 'float'>"}],return:null}}}),e._v(" "),a("p",[e._v("Orthogonal initialization (used in PPO and A2C)")]),e._v(" "),a("h3",{attrs:{id:"ipu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-vectorized-observation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-vectorized-observation"}},[e._v("#")]),e._v(" is_vectorized_observation "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_vectorized_observation",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("Check whether or not the observation is vectorized,\napply transposition to image (so that they are channel-first) if needed.\nThis is used in DQN when sampling random action (epsilon-greedy policy)")]),e._v(" "),a("p",[e._v(":param observation: the input observation to check\n:return: whether the given observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"load"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load"}},[e._v("#")]),e._v(" load "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"<class 'str'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:"~SelfBaseModel"}}}),e._v(" "),a("p",[e._v("Load model from path.")]),e._v(" "),a("p",[e._v(":param path:\n:param device: Device on which the policy should be loaded.\n:return:")]),e._v(" "),a("h3",{attrs:{id:"load-from-vector"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-vector"}},[e._v("#")]),e._v(" load_from_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_from_vector",sig:{params:[{name:"self"},{name:"vector",annotation:"<class 'numpy.ndarray'>"}],return:null}}}),e._v(" "),a("p",[e._v("Load parameters from a 1D vector.")]),e._v(" "),a("p",[e._v(":param vector:")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"make-features-extractor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#make-features-extractor"}},[e._v("#")]),e._v(" make_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"make_features_extractor",sig:{params:[{name:"self"}],return:"<class 'stable_baselines3.common.torch_layers.BaseFeaturesExtractor'>"}}}),e._v(" "),a("p",[e._v("Helper method to create a features extractor.")]),e._v(" "),a("h3",{attrs:{id:"modules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"obs-to-tensor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#obs-to-tensor"}},[e._v("#")]),e._v(" obs_to_tensor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"obs_to_tensor",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance], dict[str, typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance]]]]"}],return:"tuple[typing.Union[torch.Tensor, torch_geometric.data.data.Data, dict[str, typing.Union[torch.Tensor, torch_geometric.data.data.Data]]], bool]"}}}),e._v(" "),a("p",[e._v("Convert an input observation to a PyTorch tensor that can be fed to a model.\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:return: The observation as PyTorch tensor\nand whether the observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"parameters-to-vector"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-to-vector"}},[e._v("#")]),e._v(" parameters_to_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters_to_vector",sig:{params:[{name:"self"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Convert the parameters to a 1D vector.")]),e._v(" "),a("p",[e._v(":return:")]),e._v(" "),a("h3",{attrs:{id:"predict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict"}},[e._v("#")]),e._v(" predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),a("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:param state: The last hidden states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\nthis correspond to beginning of episodes,\nwhere the hidden states of the RNN must be reset.\n:param deterministic: Whether or not to return deterministic actions.\n:return: the model's action and the next hidden state\n(used in recurrent policies)")]),e._v(" "),a("h3",{attrs:{id:"register-backward-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"reset-noise"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reset-noise"}},[e._v("#")]),e._v(" reset_noise "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"reset_noise",sig:{params:[{name:"self"},{name:"n_envs",default:"1",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),a("p",[e._v("Sample new weights for the exploration matrix.")]),e._v(" "),a("p",[e._v(":param n_envs:")]),e._v(" "),a("h3",{attrs:{id:"save"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save"}},[e._v("#")]),e._v(" save "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"<class 'str'>"}],return:null}}}),e._v(" "),a("p",[e._v("Save model to a given location.")]),e._v(" "),a("p",[e._v(":param path:")]),e._v(" "),a("h3",{attrs:{id:"scale-action"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scale-action"}},[e._v("#")]),e._v(" scale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"scale_action",sig:{params:[{name:"self"},{name:"action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [low, high] to [-1, 1]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param action: Action to scale\n:return: Scaled action")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"set-training-mode"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-training-mode"}},[e._v("#")]),e._v(" set_training_mode "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_training_mode",sig:{params:[{name:"self"},{name:"mode",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Put the policy in either training or evaluation mode.")]),e._v(" "),a("p",[e._v("This affects certain modules, such as batch normalisation and dropout.")]),e._v(" "),a("p",[e._v(":param mode: if true, set to training mode, else set to evaluation mode")]),e._v(" "),a("h3",{attrs:{id:"share-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"type"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"unscale-action"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unscale-action"}},[e._v("#")]),e._v(" unscale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"unscale_action",sig:{params:[{name:"self"},{name:"scaled_action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [-1, 1] to [low, high]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param scaled_action: Action to un-scale")]),e._v(" "),a("h3",{attrs:{id:"xpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"build"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build"}},[e._v("#")]),e._v(" _build "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build",sig:{params:[{name:"self"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"}],return:null}}}),e._v(" "),a("p",[e._v("Create the networks and the optimizer.")]),e._v(" "),a("p",[e._v(":param lr_schedule: Learning rate schedule\nlr_schedule(1) is the initial learning rate")]),e._v(" "),a("h3",{attrs:{id:"build-mlp-extractor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-mlp-extractor"}},[e._v("#")]),e._v(" _build_mlp_extractor "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build_mlp_extractor",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),a("p",[e._v("Create the policy and value networks.\nPart of the layers can be shared.")]),e._v(" "),a("h3",{attrs:{id:"dummy-schedule"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dummy-schedule"}},[e._v("#")]),e._v(" _dummy_schedule "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_dummy_schedule",sig:{params:[{name:"progress_remaining",annotation:"<class 'float'>"}],return:"<class 'float'>"}}}),e._v(" "),a("p",[e._v("(float) Useful for pickling policy.")]),e._v(" "),a("h3",{attrs:{id:"get-action-dist-from-latent"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-action-dist-from-latent"}},[e._v("#")]),e._v(" _get_action_dist_from_latent "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_action_dist_from_latent",sig:{params:[{name:"self"},{name:"latent_pi",annotation:"<class 'torch.Tensor'>"}],return:"<class 'stable_baselines3.common.distributions.Distribution'>"}}}),e._v(" "),a("p",[e._v("Retrieve action distribution given the latent codes.")]),e._v(" "),a("p",[e._v(":param latent_pi: Latent code for the actor\n:return: Action distribution")]),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"get-constructor-parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-constructor-parameters"}},[e._v("#")]),e._v(" _get_constructor_parameters "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_constructor_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Get data that need to be saved in order to re-create the model when loading it from disk.")]),e._v(" "),a("p",[e._v(":return: The dictionary to pass to the as kwargs constructor when reconstruction this model.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"predict-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-2"}},[e._v("#")]),e._v(" _predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.Tensor'>"}}}),e._v(" "),a("p",[e._v("Get the action according to the policy for a given observation.")]),e._v(" "),a("p",[e._v(":param observation:\n:param deterministic: Whether to use stochastic or deterministic actions\n:return: Taken action according to the policy")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-2"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")]),e._v(" "),a("h3",{attrs:{id:"update-features-extractor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#update-features-extractor"}},[e._v("#")]),e._v(" _update_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_update_features_extractor",sig:{params:[{name:"self"},{name:"net_kwargs",annotation:"typing.Dict[str, typing.Any]"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Update the network keyword arguments and create a new features extractor object if needed.\nIf a "),a("code",[e._v("features_extractor")]),e._v(" object is passed, then it will be shared.")]),e._v(" "),a("p",[e._v(":param net_kwargs: the base network keyword arguments, without the ones\nrelated to features extractor\n:param features_extractor: a features extractor object.\nIf None, a new object will be created.\n:return: The updated keyword arguments")]),e._v(" "),a("h2",{attrs:{id:"multiinputgnnactorcriticpolicy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#multiinputgnnactorcriticpolicy"}},[e._v("#")]),e._v(" MultiInputGNNActorCriticPolicy")]),e._v(" "),a("p",[e._v("Policy predicting from a dict containing potentially graphs.")]),e._v(" "),a("p",[e._v("Features are extracted from graphs as in "),a("code",[e._v("GNNActorCriticPolicy")]),e._v(" thanks to a GNN\nfollowed by a reduction layer to a fixed number of features\n(see "),a("code",[e._v("GraphFeaturesExtractor")]),e._v(" for further details).")]),e._v(" "),a("h3",{attrs:{id:"constructor-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor-2"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"MultiInputGNNActorCriticPolicy",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"MultiInputGNNActorCriticPolicy",sig:{params:[{name:"observation_space",annotation:"<class 'gymnasium.spaces.dict.Dict'>"},{name:"action_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"},{name:"net_arch",default:"None",annotation:"typing.Optional[list[typing.Union[int, dict[str, list[int]]]]]"},{name:"activation_fn",default:"<class 'torch.nn.modules.activation.Tanh'>",annotation:"type[torch.nn.modules.module.Module]"},{name:"ortho_init",default:"True",annotation:"<class 'bool'>"},{name:"use_sde",default:"False",annotation:"<class 'bool'>"},{name:"log_std_init",default:"0.0",annotation:"<class 'float'>"},{name:"full_std",default:"True",annotation:"<class 'bool'>"},{name:"use_expln",default:"False",annotation:"<class 'bool'>"},{name:"squash_output",default:"False",annotation:"<class 'bool'>"},{name:"features_extractor_class",default:"<class 'skdecide.hub.solver.stable_baselines.gnn.common.torch_layers.CombinedFeaturesExtractor'>",annotation:"type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"},{name:"features_extractor_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"share_features_extractor",default:"True",annotation:"<class 'bool'>"},{name:"normalize_images",default:"True",annotation:"<class 'bool'>"},{name:"optimizer_class",default:"<class 'torch.optim.adam.Adam'>",annotation:"type[torch.optim.optimizer.Optimizer]"},{name:"optimizer_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"}]}}}),e._v(" "),a("p",[e._v("Initialize internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("h3",{attrs:{id:"add-module-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module-2"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply-2"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16-2"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers-2"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children-2"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-2"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"cpu-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu-2"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda-2"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"double-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double-2"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval-2"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"evaluate-actions-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluate-actions-2"}},[e._v("#")]),e._v(" evaluate_actions "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"evaluate_actions",sig:{params:[{name:"self"},{name:"obs",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"actions",annotation:"<class 'torch.Tensor'>"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, typing.Optional[torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Evaluate actions according to the current policy,\ngiven the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param actions: Actions\n:return: estimated value, log likelihood of taking those actions\nand entropy of the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"extra-repr-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr-2"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"extract-features-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extract-features-2"}},[e._v("#")]),e._v(" extract_features "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extract_features",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Preprocess the observation if needed and extract features.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param features_extractor: The features extractor to use. If None, then "),a("code",[e._v("self.features_extractor")]),e._v(" is used.\n:return: The extracted features. If features extractor is not shared, returns a tuple with the\nfeatures for the actor and the features for the critic.")]),e._v(" "),a("h3",{attrs:{id:"float-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float-2"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward-2"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch.Tensor'>"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Forward pass in all the networks (actor and critic)")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param deterministic: Whether to sample or use deterministic actions\n:return: action, value and log probability of the action")]),e._v(" "),a("h3",{attrs:{id:"get-buffer-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer-2"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state-2"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-parameter-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter-2"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule-2"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half-2"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"init-weights-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init-weights-2"}},[e._v("#")]),e._v(" init_weights "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"init_weights",sig:{params:[{name:"module",annotation:"<class 'torch.nn.modules.module.Module'>"},{name:"gain",default:"1",annotation:"<class 'float'>"}],return:null}}}),e._v(" "),a("p",[e._v("Orthogonal initialization (used in PPO and A2C)")]),e._v(" "),a("h3",{attrs:{id:"ipu-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu-2"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-vectorized-observation-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-vectorized-observation-2"}},[e._v("#")]),e._v(" is_vectorized_observation "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_vectorized_observation",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("Check whether or not the observation is vectorized,\napply transposition to image (so that they are channel-first) if needed.\nThis is used in DQN when sampling random action (epsilon-greedy policy)")]),e._v(" "),a("p",[e._v(":param observation: the input observation to check\n:return: whether the given observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"load-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-2"}},[e._v("#")]),e._v(" load "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"<class 'str'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:"~SelfBaseModel"}}}),e._v(" "),a("p",[e._v("Load model from path.")]),e._v(" "),a("p",[e._v(":param path:\n:param device: Device on which the policy should be loaded.\n:return:")]),e._v(" "),a("h3",{attrs:{id:"load-from-vector-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-vector-2"}},[e._v("#")]),e._v(" load_from_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_from_vector",sig:{params:[{name:"self"},{name:"vector",annotation:"<class 'numpy.ndarray'>"}],return:null}}}),e._v(" "),a("p",[e._v("Load parameters from a 1D vector.")]),e._v(" "),a("p",[e._v(":param vector:")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict-2"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"make-features-extractor-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#make-features-extractor-2"}},[e._v("#")]),e._v(" make_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"make_features_extractor",sig:{params:[{name:"self"}],return:"<class 'stable_baselines3.common.torch_layers.BaseFeaturesExtractor'>"}}}),e._v(" "),a("p",[e._v("Helper method to create a features extractor.")]),e._v(" "),a("h3",{attrs:{id:"modules-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules-2"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia-2"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers-2"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children-2"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules-2"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters-2"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"obs-to-tensor-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#obs-to-tensor-2"}},[e._v("#")]),e._v(" obs_to_tensor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"obs_to_tensor",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance], dict[str, typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance]]]]"}],return:"tuple[typing.Union[torch.Tensor, torch_geometric.data.data.Data, dict[str, typing.Union[torch.Tensor, torch_geometric.data.data.Data]]], bool]"}}}),e._v(" "),a("p",[e._v("Convert an input observation to a PyTorch tensor that can be fed to a model.\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:return: The observation as PyTorch tensor\nand whether the observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"parameters-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-2"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"parameters-to-vector-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-to-vector-2"}},[e._v("#")]),e._v(" parameters_to_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters_to_vector",sig:{params:[{name:"self"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Convert the parameters to a 1D vector.")]),e._v(" "),a("p",[e._v(":return:")]),e._v(" "),a("h3",{attrs:{id:"predict-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-3"}},[e._v("#")]),e._v(" predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),a("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:param state: The last hidden states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\nthis correspond to beginning of episodes,\nwhere the hidden states of the RNN must be reset.\n:param deterministic: Whether or not to return deterministic actions.\n:return: the model's action and the next hidden state\n(used in recurrent policies)")]),e._v(" "),a("h3",{attrs:{id:"register-backward-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook-2"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer-2"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook-2"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook-2"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook-2"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook-2"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook-2"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-3"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module-2"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter-2"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook-2"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook-2"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad-2"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"reset-noise-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reset-noise-2"}},[e._v("#")]),e._v(" reset_noise "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"reset_noise",sig:{params:[{name:"self"},{name:"n_envs",default:"1",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),a("p",[e._v("Sample new weights for the exploration matrix.")]),e._v(" "),a("p",[e._v(":param n_envs:")]),e._v(" "),a("h3",{attrs:{id:"save-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-2"}},[e._v("#")]),e._v(" save "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"<class 'str'>"}],return:null}}}),e._v(" "),a("p",[e._v("Save model to a given location.")]),e._v(" "),a("p",[e._v(":param path:")]),e._v(" "),a("h3",{attrs:{id:"scale-action-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scale-action-2"}},[e._v("#")]),e._v(" scale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"scale_action",sig:{params:[{name:"self"},{name:"action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [low, high] to [-1, 1]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param action: Action to scale\n:return: Scaled action")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state-2"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule-2"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"set-training-mode-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-training-mode-2"}},[e._v("#")]),e._v(" set_training_mode "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_training_mode",sig:{params:[{name:"self"},{name:"mode",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Put the policy in either training or evaluation mode.")]),e._v(" "),a("p",[e._v("This affects certain modules, such as batch normalisation and dropout.")]),e._v(" "),a("p",[e._v(":param mode: if true, set to training mode, else set to evaluation mode")]),e._v(" "),a("h3",{attrs:{id:"share-memory-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory-2"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict-2"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-2"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty-2"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train-2"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"type-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type-2"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"unscale-action-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unscale-action-2"}},[e._v("#")]),e._v(" unscale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"unscale_action",sig:{params:[{name:"self"},{name:"scaled_action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [-1, 1] to [low, high]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param scaled_action: Action to un-scale")]),e._v(" "),a("h3",{attrs:{id:"xpu-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu-2"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad-2"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"build-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-2"}},[e._v("#")]),e._v(" _build "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build",sig:{params:[{name:"self"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"}],return:null}}}),e._v(" "),a("p",[e._v("Create the networks and the optimizer.")]),e._v(" "),a("p",[e._v(":param lr_schedule: Learning rate schedule\nlr_schedule(1) is the initial learning rate")]),e._v(" "),a("h3",{attrs:{id:"build-mlp-extractor-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-mlp-extractor-2"}},[e._v("#")]),e._v(" _build_mlp_extractor "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build_mlp_extractor",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),a("p",[e._v("Create the policy and value networks.\nPart of the layers can be shared.")]),e._v(" "),a("h3",{attrs:{id:"dummy-schedule-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dummy-schedule-2"}},[e._v("#")]),e._v(" _dummy_schedule "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_dummy_schedule",sig:{params:[{name:"progress_remaining",annotation:"<class 'float'>"}],return:"<class 'float'>"}}}),e._v(" "),a("p",[e._v("(float) Useful for pickling policy.")]),e._v(" "),a("h3",{attrs:{id:"get-action-dist-from-latent-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-action-dist-from-latent-2"}},[e._v("#")]),e._v(" _get_action_dist_from_latent "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_action_dist_from_latent",sig:{params:[{name:"self"},{name:"latent_pi",annotation:"<class 'torch.Tensor'>"}],return:"<class 'stable_baselines3.common.distributions.Distribution'>"}}}),e._v(" "),a("p",[e._v("Retrieve action distribution given the latent codes.")]),e._v(" "),a("p",[e._v(":param latent_pi: Latent code for the actor\n:return: Action distribution")]),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks-2"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"get-constructor-parameters-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-constructor-parameters-2"}},[e._v("#")]),e._v(" _get_constructor_parameters "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_constructor_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Get data that need to be saved in order to re-create the model when loading it from disk.")]),e._v(" "),a("p",[e._v(":return: The dictionary to pass to the as kwargs constructor when reconstruction this model.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict-2"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members-2"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"predict-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-4"}},[e._v("#")]),e._v(" _predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.Tensor'>"}}}),e._v(" "),a("p",[e._v("Get the action according to the policy for a given observation.")]),e._v(" "),a("p",[e._v(":param observation:\n:param deterministic: Whether to use stochastic or deterministic actions\n:return: Taken action according to the policy")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-4"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook-2"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict-2"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")]),e._v(" "),a("h3",{attrs:{id:"update-features-extractor-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#update-features-extractor-2"}},[e._v("#")]),e._v(" _update_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_update_features_extractor",sig:{params:[{name:"self"},{name:"net_kwargs",annotation:"typing.Dict[str, typing.Any]"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Update the network keyword arguments and create a new features extractor object if needed.\nIf a "),a("code",[e._v("features_extractor")]),e._v(" object is passed, then it will be shared.")]),e._v(" "),a("p",[e._v(":param net_kwargs: the base network keyword arguments, without the ones\nrelated to features extractor\n:param features_extractor: a features extractor object.\nIf None, a new object will be created.\n:return: The updated keyword arguments")]),e._v(" "),a("h2",{attrs:{id:"maskablegnnactorcriticpolicy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#maskablegnnactorcriticpolicy"}},[e._v("#")]),e._v(" MaskableGNNActorCriticPolicy")]),e._v(" "),a("p",[e._v("Policy predicting from an observation graph + an action mask.")]),e._v(" "),a("p",[e._v("Features are extracted from the graph thanks to a GNN\nfollowed by a reduction layer to a fixed number of features\n(see "),a("code",[e._v("GraphFeaturesExtractor")]),e._v(" for further details).")]),e._v(" "),a("h3",{attrs:{id:"constructor-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor-3"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"MaskableGNNActorCriticPolicy",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"MaskableGNNActorCriticPolicy",sig:{params:[{name:"observation_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"action_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"},{name:"net_arch",default:"None",annotation:"typing.Union[list[int], dict[str, list[int]], NoneType]"},{name:"activation_fn",default:"<class 'torch.nn.modules.activation.Tanh'>",annotation:"type[torch.nn.modules.module.Module]"},{name:"ortho_init",default:"True",annotation:"<class 'bool'>"},{name:"features_extractor_class",default:"<class 'skdecide.hub.solver.stable_baselines.gnn.common.torch_layers.GraphFeaturesExtractor'>",annotation:"type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"},{name:"features_extractor_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"share_features_extractor",default:"True",annotation:"<class 'bool'>"},{name:"normalize_images",default:"True",annotation:"<class 'bool'>"},{name:"optimizer_class",default:"<class 'torch.optim.adam.Adam'>",annotation:"type[torch.optim.optimizer.Optimizer]"},{name:"optimizer_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"}]}}}),e._v(" "),a("p",[e._v("Initialize internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("h3",{attrs:{id:"add-module-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module-3"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply-3"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16-3"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers-3"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children-3"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-3"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"cpu-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu-3"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda-3"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"double-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double-3"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval-3"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"evaluate-actions-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluate-actions-3"}},[e._v("#")]),e._v(" evaluate_actions "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"evaluate_actions",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch.Tensor'>"},{name:"actions",annotation:"<class 'torch.Tensor'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[torch.Tensor]"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, typing.Optional[torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Evaluate actions according to the current policy,\ngiven the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param actions: Actions\n:return: estimated value, log likelihood of taking those actions\nand entropy of the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"extra-repr-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr-3"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"extract-features-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extract-features-3"}},[e._v("#")]),e._v(" extract_features "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extract_features",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Preprocess the observation if needed and extract features.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param features_extractor: The features extractor to use. If None, then "),a("code",[e._v("self.features_extractor")]),e._v(" is used.\n:return: The extracted features. If features extractor is not shared, returns a tuple with the\nfeatures for the actor and the features for the critic.")]),e._v(" "),a("h3",{attrs:{id:"float-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float-3"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward-3"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch.Tensor'>"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Forward pass in all the networks (actor and critic)")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param deterministic: Whether to sample or use deterministic actions\n:param action_masks: Action masks to apply to the action distribution\n:return: action, value and log probability of the action")]),e._v(" "),a("h3",{attrs:{id:"get-buffer-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer-3"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-distribution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-distribution"}},[e._v("#")]),e._v(" get_distribution "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_distribution",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"<class 'sb3_contrib.common.maskable.distributions.MaskableDistribution'>"}}}),e._v(" "),a("p",[e._v("Get the current policy distribution given the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param action_masks: Actions' mask\n:return: the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state-3"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-parameter-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter-3"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule-3"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half-3"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"init-weights-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init-weights-3"}},[e._v("#")]),e._v(" init_weights "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"init_weights",sig:{params:[{name:"module",annotation:"<class 'torch.nn.modules.module.Module'>"},{name:"gain",default:"1",annotation:"<class 'float'>"}],return:null}}}),e._v(" "),a("p",[e._v("Orthogonal initialization (used in PPO and A2C)")]),e._v(" "),a("h3",{attrs:{id:"ipu-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu-3"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-vectorized-observation-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-vectorized-observation-3"}},[e._v("#")]),e._v(" is_vectorized_observation "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_vectorized_observation",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("Check whether or not the observation is vectorized,\napply transposition to image (so that they are channel-first) if needed.\nThis is used in DQN when sampling random action (epsilon-greedy policy)")]),e._v(" "),a("p",[e._v(":param observation: the input observation to check\n:return: whether the given observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"load-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-3"}},[e._v("#")]),e._v(" load "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"<class 'str'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:"~SelfBaseModel"}}}),e._v(" "),a("p",[e._v("Load model from path.")]),e._v(" "),a("p",[e._v(":param path:\n:param device: Device on which the policy should be loaded.\n:return:")]),e._v(" "),a("h3",{attrs:{id:"load-from-vector-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-vector-3"}},[e._v("#")]),e._v(" load_from_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_from_vector",sig:{params:[{name:"self"},{name:"vector",annotation:"<class 'numpy.ndarray'>"}],return:null}}}),e._v(" "),a("p",[e._v("Load parameters from a 1D vector.")]),e._v(" "),a("p",[e._v(":param vector:")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict-3"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"make-features-extractor-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#make-features-extractor-3"}},[e._v("#")]),e._v(" make_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"make_features_extractor",sig:{params:[{name:"self"}],return:"<class 'stable_baselines3.common.torch_layers.BaseFeaturesExtractor'>"}}}),e._v(" "),a("p",[e._v("Helper method to create a features extractor.")]),e._v(" "),a("h3",{attrs:{id:"modules-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules-3"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia-3"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers-3"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children-3"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules-3"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters-3"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"obs-to-tensor-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#obs-to-tensor-3"}},[e._v("#")]),e._v(" obs_to_tensor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"obs_to_tensor",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance], dict[str, typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance]]]]"}],return:"tuple[typing.Union[torch.Tensor, torch_geometric.data.data.Data, dict[str, typing.Union[torch.Tensor, torch_geometric.data.data.Data]]], bool]"}}}),e._v(" "),a("p",[e._v("Convert an input observation to a PyTorch tensor that can be fed to a model.\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:return: The observation as PyTorch tensor\nand whether the observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"parameters-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-3"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"parameters-to-vector-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-to-vector-3"}},[e._v("#")]),e._v(" parameters_to_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters_to_vector",sig:{params:[{name:"self"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Convert the parameters to a 1D vector.")]),e._v(" "),a("p",[e._v(":return:")]),e._v(" "),a("h3",{attrs:{id:"predict-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-5"}},[e._v("#")]),e._v(" predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),a("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:param state: The last states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\n:param deterministic: Whether or not to return deterministic actions.\n:param action_masks: Action masks to apply to the action distribution\n:return: the model's action and the next state\n(used in recurrent policies)")]),e._v(" "),a("h3",{attrs:{id:"register-backward-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook-3"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer-3"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook-3"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook-3"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook-3"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook-3"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook-3"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-5"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module-3"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter-3"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook-3"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook-3"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad-3"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"save-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-3"}},[e._v("#")]),e._v(" save "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"<class 'str'>"}],return:null}}}),e._v(" "),a("p",[e._v("Save model to a given location.")]),e._v(" "),a("p",[e._v(":param path:")]),e._v(" "),a("h3",{attrs:{id:"scale-action-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scale-action-3"}},[e._v("#")]),e._v(" scale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"scale_action",sig:{params:[{name:"self"},{name:"action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [low, high] to [-1, 1]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param action: Action to scale\n:return: Scaled action")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state-3"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule-3"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"set-training-mode-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-training-mode-3"}},[e._v("#")]),e._v(" set_training_mode "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_training_mode",sig:{params:[{name:"self"},{name:"mode",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Put the policy in either training or evaluation mode.")]),e._v(" "),a("p",[e._v("This affects certain modules, such as batch normalisation and dropout.")]),e._v(" "),a("p",[e._v(":param mode: if true, set to training mode, else set to evaluation mode")]),e._v(" "),a("h3",{attrs:{id:"share-memory-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory-3"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict-3"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-3"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty-3"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train-3"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"type-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type-3"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"unscale-action-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unscale-action-3"}},[e._v("#")]),e._v(" unscale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"unscale_action",sig:{params:[{name:"self"},{name:"scaled_action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [-1, 1] to [low, high]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param scaled_action: Action to un-scale")]),e._v(" "),a("h3",{attrs:{id:"xpu-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu-3"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad-3"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"build-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-3"}},[e._v("#")]),e._v(" _build "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build",sig:{params:[{name:"self"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"}],return:null}}}),e._v(" "),a("p",[e._v("Create the networks and the optimizer.")]),e._v(" "),a("p",[e._v(":param lr_schedule: Learning rate schedule\nlr_schedule(1) is the initial learning rate")]),e._v(" "),a("h3",{attrs:{id:"build-mlp-extractor-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-mlp-extractor-3"}},[e._v("#")]),e._v(" _build_mlp_extractor "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build_mlp_extractor",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),a("p",[e._v("Create the policy and value networks.\nPart of the layers can be shared.")]),e._v(" "),a("h3",{attrs:{id:"dummy-schedule-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dummy-schedule-3"}},[e._v("#")]),e._v(" _dummy_schedule "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_dummy_schedule",sig:{params:[{name:"progress_remaining",annotation:"<class 'float'>"}],return:"<class 'float'>"}}}),e._v(" "),a("p",[e._v("(float) Useful for pickling policy.")]),e._v(" "),a("h3",{attrs:{id:"get-action-dist-from-latent-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-action-dist-from-latent-3"}},[e._v("#")]),e._v(" _get_action_dist_from_latent "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_action_dist_from_latent",sig:{params:[{name:"self"},{name:"latent_pi",annotation:"<class 'torch.Tensor'>"}],return:"<class 'sb3_contrib.common.maskable.distributions.MaskableDistribution'>"}}}),e._v(" "),a("p",[e._v("Retrieve action distribution given the latent codes.")]),e._v(" "),a("p",[e._v(":param latent_pi: Latent code for the actor\n:return: Action distribution")]),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks-3"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"get-constructor-parameters-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-constructor-parameters-3"}},[e._v("#")]),e._v(" _get_constructor_parameters "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_constructor_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Get data that need to be saved in order to re-create the model when loading it from disk.")]),e._v(" "),a("p",[e._v(":return: The dictionary to pass to the as kwargs constructor when reconstruction this model.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict-3"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members-3"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"predict-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-6"}},[e._v("#")]),e._v(" _predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"<class 'torch.Tensor'>"}}}),e._v(" "),a("p",[e._v("Get the action according to the policy for a given observation.")]),e._v(" "),a("p",[e._v(":param observation:\n:param deterministic: Whether to use stochastic or deterministic actions\n:param action_masks: Action masks to apply to the action distribution\n:return: Taken action according to the policy")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-6"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook-3"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict-3"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")]),e._v(" "),a("h3",{attrs:{id:"update-features-extractor-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#update-features-extractor-3"}},[e._v("#")]),e._v(" _update_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_update_features_extractor",sig:{params:[{name:"self"},{name:"net_kwargs",annotation:"typing.Dict[str, typing.Any]"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Update the network keyword arguments and create a new features extractor object if needed.\nIf a "),a("code",[e._v("features_extractor")]),e._v(" object is passed, then it will be shared.")]),e._v(" "),a("p",[e._v(":param net_kwargs: the base network keyword arguments, without the ones\nrelated to features extractor\n:param features_extractor: a features extractor object.\nIf None, a new object will be created.\n:return: The updated keyword arguments")]),e._v(" "),a("h2",{attrs:{id:"maskablemultiinputgnnactorcriticpolicy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#maskablemultiinputgnnactorcriticpolicy"}},[e._v("#")]),e._v(" MaskableMultiInputGNNActorCriticPolicy")]),e._v(" "),a("p",[e._v("Policy predicting from a dict containing potentially graphs + an action mask.")]),e._v(" "),a("p",[e._v("Features are extracted from graphs as in "),a("code",[e._v("GNNActorCriticPolicy")]),e._v(" thanks to a GNN\nfollowed by a reduction layer to a fixed number of features\n(see "),a("code",[e._v("GraphFeaturesExtractor")]),e._v(" for further details).")]),e._v(" "),a("h3",{attrs:{id:"constructor-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor-4"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"MaskableMultiInputGNNActorCriticPolicy",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"MaskableMultiInputGNNActorCriticPolicy",sig:{params:[{name:"observation_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"action_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"},{name:"net_arch",default:"None",annotation:"typing.Union[list[int], dict[str, list[int]], NoneType]"},{name:"activation_fn",default:"<class 'torch.nn.modules.activation.Tanh'>",annotation:"type[torch.nn.modules.module.Module]"},{name:"ortho_init",default:"True",annotation:"<class 'bool'>"},{name:"features_extractor_class",default:"<class 'skdecide.hub.solver.stable_baselines.gnn.common.torch_layers.CombinedFeaturesExtractor'>",annotation:"type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"},{name:"features_extractor_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"share_features_extractor",default:"True",annotation:"<class 'bool'>"},{name:"normalize_images",default:"True",annotation:"<class 'bool'>"},{name:"optimizer_class",default:"<class 'torch.optim.adam.Adam'>",annotation:"type[torch.optim.optimizer.Optimizer]"},{name:"optimizer_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"}]}}}),e._v(" "),a("p",[e._v("Initialize internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("h3",{attrs:{id:"add-module-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module-4"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply-4"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16-4"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers-4"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children-4"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-4"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"cpu-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu-4"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda-4"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"double-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double-4"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval-4"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"evaluate-actions-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluate-actions-4"}},[e._v("#")]),e._v(" evaluate_actions "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"evaluate_actions",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch.Tensor'>"},{name:"actions",annotation:"<class 'torch.Tensor'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[torch.Tensor]"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, typing.Optional[torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Evaluate actions according to the current policy,\ngiven the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param actions: Actions\n:return: estimated value, log likelihood of taking those actions\nand entropy of the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"extra-repr-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr-4"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"extract-features-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extract-features-4"}},[e._v("#")]),e._v(" extract_features "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extract_features",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Preprocess the observation if needed and extract features.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param features_extractor: The features extractor to use. If None, then "),a("code",[e._v("self.features_extractor")]),e._v(" is used.\n:return: The extracted features. If features extractor is not shared, returns a tuple with the\nfeatures for the actor and the features for the critic.")]),e._v(" "),a("h3",{attrs:{id:"float-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float-4"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward-4"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch.Tensor'>"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Forward pass in all the networks (actor and critic)")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param deterministic: Whether to sample or use deterministic actions\n:param action_masks: Action masks to apply to the action distribution\n:return: action, value and log probability of the action")]),e._v(" "),a("h3",{attrs:{id:"get-buffer-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer-4"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-distribution-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-distribution-2"}},[e._v("#")]),e._v(" get_distribution "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_distribution",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"<class 'sb3_contrib.common.maskable.distributions.MaskableDistribution'>"}}}),e._v(" "),a("p",[e._v("Get the current policy distribution given the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param action_masks: Actions' mask\n:return: the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state-4"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-parameter-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter-4"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule-4"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half-4"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"init-weights-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init-weights-4"}},[e._v("#")]),e._v(" init_weights "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"init_weights",sig:{params:[{name:"module",annotation:"<class 'torch.nn.modules.module.Module'>"},{name:"gain",default:"1",annotation:"<class 'float'>"}],return:null}}}),e._v(" "),a("p",[e._v("Orthogonal initialization (used in PPO and A2C)")]),e._v(" "),a("h3",{attrs:{id:"ipu-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu-4"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-vectorized-observation-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-vectorized-observation-4"}},[e._v("#")]),e._v(" is_vectorized_observation "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_vectorized_observation",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("Check whether or not the observation is vectorized,\napply transposition to image (so that they are channel-first) if needed.\nThis is used in DQN when sampling random action (epsilon-greedy policy)")]),e._v(" "),a("p",[e._v(":param observation: the input observation to check\n:return: whether the given observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"load-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-4"}},[e._v("#")]),e._v(" load "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"<class 'str'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:"~SelfBaseModel"}}}),e._v(" "),a("p",[e._v("Load model from path.")]),e._v(" "),a("p",[e._v(":param path:\n:param device: Device on which the policy should be loaded.\n:return:")]),e._v(" "),a("h3",{attrs:{id:"load-from-vector-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-vector-4"}},[e._v("#")]),e._v(" load_from_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_from_vector",sig:{params:[{name:"self"},{name:"vector",annotation:"<class 'numpy.ndarray'>"}],return:null}}}),e._v(" "),a("p",[e._v("Load parameters from a 1D vector.")]),e._v(" "),a("p",[e._v(":param vector:")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict-4"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"make-features-extractor-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#make-features-extractor-4"}},[e._v("#")]),e._v(" make_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"make_features_extractor",sig:{params:[{name:"self"}],return:"<class 'stable_baselines3.common.torch_layers.BaseFeaturesExtractor'>"}}}),e._v(" "),a("p",[e._v("Helper method to create a features extractor.")]),e._v(" "),a("h3",{attrs:{id:"modules-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules-4"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia-4"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers-4"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children-4"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules-4"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters-4"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"obs-to-tensor-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#obs-to-tensor-4"}},[e._v("#")]),e._v(" obs_to_tensor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"obs_to_tensor",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance], dict[str, typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance]]]]"}],return:"tuple[typing.Union[torch.Tensor, torch_geometric.data.data.Data, dict[str, typing.Union[torch.Tensor, torch_geometric.data.data.Data]]], bool]"}}}),e._v(" "),a("p",[e._v("Convert an input observation to a PyTorch tensor that can be fed to a model.\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:return: The observation as PyTorch tensor\nand whether the observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"parameters-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-4"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"parameters-to-vector-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-to-vector-4"}},[e._v("#")]),e._v(" parameters_to_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters_to_vector",sig:{params:[{name:"self"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Convert the parameters to a 1D vector.")]),e._v(" "),a("p",[e._v(":return:")]),e._v(" "),a("h3",{attrs:{id:"predict-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-7"}},[e._v("#")]),e._v(" predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),a("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:param state: The last states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\n:param deterministic: Whether or not to return deterministic actions.\n:param action_masks: Action masks to apply to the action distribution\n:return: the model's action and the next state\n(used in recurrent policies)")]),e._v(" "),a("h3",{attrs:{id:"register-backward-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook-4"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer-4"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook-4"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook-4"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook-4"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook-4"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook-4"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-7"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module-4"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter-4"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook-4"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook-4"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad-4"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"save-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-4"}},[e._v("#")]),e._v(" save "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"<class 'str'>"}],return:null}}}),e._v(" "),a("p",[e._v("Save model to a given location.")]),e._v(" "),a("p",[e._v(":param path:")]),e._v(" "),a("h3",{attrs:{id:"scale-action-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scale-action-4"}},[e._v("#")]),e._v(" scale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"scale_action",sig:{params:[{name:"self"},{name:"action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [low, high] to [-1, 1]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param action: Action to scale\n:return: Scaled action")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state-4"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule-4"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"set-training-mode-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-training-mode-4"}},[e._v("#")]),e._v(" set_training_mode "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_training_mode",sig:{params:[{name:"self"},{name:"mode",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Put the policy in either training or evaluation mode.")]),e._v(" "),a("p",[e._v("This affects certain modules, such as batch normalisation and dropout.")]),e._v(" "),a("p",[e._v(":param mode: if true, set to training mode, else set to evaluation mode")]),e._v(" "),a("h3",{attrs:{id:"share-memory-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory-4"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict-4"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-4"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty-4"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train-4"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"type-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type-4"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"unscale-action-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unscale-action-4"}},[e._v("#")]),e._v(" unscale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"unscale_action",sig:{params:[{name:"self"},{name:"scaled_action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [-1, 1] to [low, high]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param scaled_action: Action to un-scale")]),e._v(" "),a("h3",{attrs:{id:"xpu-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu-4"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad-4"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"build-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-4"}},[e._v("#")]),e._v(" _build "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build",sig:{params:[{name:"self"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"}],return:null}}}),e._v(" "),a("p",[e._v("Create the networks and the optimizer.")]),e._v(" "),a("p",[e._v(":param lr_schedule: Learning rate schedule\nlr_schedule(1) is the initial learning rate")]),e._v(" "),a("h3",{attrs:{id:"build-mlp-extractor-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-mlp-extractor-4"}},[e._v("#")]),e._v(" _build_mlp_extractor "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build_mlp_extractor",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),a("p",[e._v("Create the policy and value networks.\nPart of the layers can be shared.")]),e._v(" "),a("h3",{attrs:{id:"dummy-schedule-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dummy-schedule-4"}},[e._v("#")]),e._v(" _dummy_schedule "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_dummy_schedule",sig:{params:[{name:"progress_remaining",annotation:"<class 'float'>"}],return:"<class 'float'>"}}}),e._v(" "),a("p",[e._v("(float) Useful for pickling policy.")]),e._v(" "),a("h3",{attrs:{id:"get-action-dist-from-latent-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-action-dist-from-latent-4"}},[e._v("#")]),e._v(" _get_action_dist_from_latent "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_action_dist_from_latent",sig:{params:[{name:"self"},{name:"latent_pi",annotation:"<class 'torch.Tensor'>"}],return:"<class 'sb3_contrib.common.maskable.distributions.MaskableDistribution'>"}}}),e._v(" "),a("p",[e._v("Retrieve action distribution given the latent codes.")]),e._v(" "),a("p",[e._v(":param latent_pi: Latent code for the actor\n:return: Action distribution")]),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks-4"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"get-constructor-parameters-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-constructor-parameters-4"}},[e._v("#")]),e._v(" _get_constructor_parameters "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_constructor_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Get data that need to be saved in order to re-create the model when loading it from disk.")]),e._v(" "),a("p",[e._v(":return: The dictionary to pass to the as kwargs constructor when reconstruction this model.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict-4"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members-4"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"predict-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-8"}},[e._v("#")]),e._v(" _predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"<class 'torch.Tensor'>"}}}),e._v(" "),a("p",[e._v("Get the action according to the policy for a given observation.")]),e._v(" "),a("p",[e._v(":param observation:\n:param deterministic: Whether to use stochastic or deterministic actions\n:param action_masks: Action masks to apply to the action distribution\n:return: Taken action according to the policy")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-8"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook-4"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict-4"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")]),e._v(" "),a("h3",{attrs:{id:"update-features-extractor-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#update-features-extractor-4"}},[e._v("#")]),e._v(" _update_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_update_features_extractor",sig:{params:[{name:"self"},{name:"net_kwargs",annotation:"typing.Dict[str, typing.Any]"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Update the network keyword arguments and create a new features extractor object if needed.\nIf a "),a("code",[e._v("features_extractor")]),e._v(" object is passed, then it will be shared.")]),e._v(" "),a("p",[e._v(":param net_kwargs: the base network keyword arguments, without the ones\nrelated to features extractor\n:param features_extractor: a features extractor object.\nIf None, a new object will be created.\n:return: The updated keyword arguments")]),e._v(" "),a("h2",{attrs:{id:"gnn2nodeactorcriticpolicy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#gnn2nodeactorcriticpolicy"}},[e._v("#")]),e._v(" GNN2NodeActorCriticPolicy")]),e._v(" "),a("p",[e._v("Policy mapping a graph to one of its node.")]),e._v(" "),a("p",[e._v("The action is predicted by a GNN without additional reduction layer.\nThe value is modelled as in "),a("code",[e._v("GNNActorCriticPolicy")]),e._v(" by a GNN followed by a reduction layer.")]),e._v(" "),a("p",[e._v("Intended to be used with environment where an observation is a graph and an action\nis the choice of a node in this graph.")]),e._v(" "),a("h3",{attrs:{id:"constructor-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor-5"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"GNN2NodeActorCriticPolicy",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"GNN2NodeActorCriticPolicy",sig:{params:[{name:"observation_space",annotation:"<class 'gymnasium.spaces.graph.Graph'>"},{name:"action_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"},{name:"net_arch",default:"None",annotation:"typing.Union[typing.List[int], typing.Dict[str, typing.List[int]], NoneType]"},{name:"activation_fn",default:"<class 'torch.nn.modules.activation.Tanh'>",annotation:"typing.Type[torch.nn.modules.module.Module]"},{name:"ortho_init",default:"True",annotation:"<class 'bool'>"},{name:"log_std_init",default:"0.0",annotation:"<class 'float'>"},{name:"features_extractor_class",default:"<class 'skdecide.hub.solver.stable_baselines.gnn.common.torch_layers.GraphFeaturesExtractor'>",annotation:"typing.Type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"},{name:"features_extractor_kwargs",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"action_gnn_class",default:"None",annotation:"typing.Optional[type[torch.nn.modules.module.Module]]"},{name:"action_gnn_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"normalize_images",default:"True",annotation:"<class 'bool'>"},{name:"optimizer_class",default:"<class 'torch.optim.adam.Adam'>",annotation:"typing.Type[torch.optim.optimizer.Optimizer]"},{name:"optimizer_kwargs",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"debug",default:"False",annotation:"<class 'bool'>"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Initialize internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("h3",{attrs:{id:"add-module-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module-5"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply-5"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16-5"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers-5"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children-5"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-5"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"cpu-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu-5"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda-5"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"double-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double-5"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval-5"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"extra-repr-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr-5"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"extract-features-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extract-features-5"}},[e._v("#")]),e._v(" extract_features "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extract_features",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Preprocess the observation if needed and extract features.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param features_extractor: The features extractor to use. If None, then "),a("code",[e._v("self.features_extractor")]),e._v(" is used.\n:return: The extracted features. If features extractor is not shared, returns a tuple with the\nfeatures for the actor and the features for the critic.")]),e._v(" "),a("h3",{attrs:{id:"float-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float-5"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward-5"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Define the computation performed at every call.")]),e._v(" "),a("p",[e._v("Should be overridden by all subclasses.")]),e._v(" "),a("p",[e._v(".. note::\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the :class:"),a("code",[e._v("Module")]),e._v(" instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.")]),e._v(" "),a("h3",{attrs:{id:"get-buffer-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer-5"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-distribution-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-distribution-3"}},[e._v("#")]),e._v(" get_distribution "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_distribution",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"}],return:"<class 'stable_baselines3.common.distributions.Distribution'>"}}}),e._v(" "),a("p",[e._v("Get the current policy distribution given the observations.")]),e._v(" "),a("p",[e._v(":param obs:\n:return: the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state-5"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-parameter-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter-5"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule-5"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half-5"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"init-weights-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init-weights-5"}},[e._v("#")]),e._v(" init_weights "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"init_weights",sig:{params:[{name:"module",annotation:"<class 'torch.nn.modules.module.Module'>"},{name:"gain",default:"1",annotation:"<class 'float'>"}],return:null}}}),e._v(" "),a("p",[e._v("Orthogonal initialization (used in PPO and A2C)")]),e._v(" "),a("h3",{attrs:{id:"ipu-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu-5"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-vectorized-observation-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-vectorized-observation-5"}},[e._v("#")]),e._v(" is_vectorized_observation "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_vectorized_observation",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("Check whether or not the observation is vectorized,\napply transposition to image (so that they are channel-first) if needed.\nThis is used in DQN when sampling random action (epsilon-greedy policy)")]),e._v(" "),a("p",[e._v(":param observation: the input observation to check\n:return: whether the given observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"load-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-5"}},[e._v("#")]),e._v(" load "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"<class 'str'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:"~SelfBaseModel"}}}),e._v(" "),a("p",[e._v("Load model from path.")]),e._v(" "),a("p",[e._v(":param path:\n:param device: Device on which the policy should be loaded.\n:return:")]),e._v(" "),a("h3",{attrs:{id:"load-from-vector-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-vector-5"}},[e._v("#")]),e._v(" load_from_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_from_vector",sig:{params:[{name:"self"},{name:"vector",annotation:"<class 'numpy.ndarray'>"}],return:null}}}),e._v(" "),a("p",[e._v("Load parameters from a 1D vector.")]),e._v(" "),a("p",[e._v(":param vector:")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict-5"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"make-features-extractor-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#make-features-extractor-5"}},[e._v("#")]),e._v(" make_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"make_features_extractor",sig:{params:[{name:"self"}],return:"<class 'stable_baselines3.common.torch_layers.BaseFeaturesExtractor'>"}}}),e._v(" "),a("p",[e._v("Helper method to create a features extractor.")]),e._v(" "),a("h3",{attrs:{id:"modules-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules-5"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia-5"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers-5"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children-5"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules-5"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters-5"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"obs-to-tensor-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#obs-to-tensor-5"}},[e._v("#")]),e._v(" obs_to_tensor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"obs_to_tensor",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance], dict[str, typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance]]]]"}],return:"tuple[typing.Union[torch.Tensor, torch_geometric.data.data.Data, dict[str, typing.Union[torch.Tensor, torch_geometric.data.data.Data]]], bool]"}}}),e._v(" "),a("p",[e._v("Convert an input observation to a PyTorch tensor that can be fed to a model.\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:return: The observation as PyTorch tensor\nand whether the observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"parameters-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-5"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"parameters-to-vector-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-to-vector-5"}},[e._v("#")]),e._v(" parameters_to_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters_to_vector",sig:{params:[{name:"self"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Convert the parameters to a 1D vector.")]),e._v(" "),a("p",[e._v(":return:")]),e._v(" "),a("h3",{attrs:{id:"predict-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-9"}},[e._v("#")]),e._v(" predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),a("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:param state: The last hidden states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\nthis correspond to beginning of episodes,\nwhere the hidden states of the RNN must be reset.\n:param deterministic: Whether or not to return deterministic actions.\n:return: the model's action and the next hidden state\n(used in recurrent policies)")]),e._v(" "),a("h3",{attrs:{id:"register-backward-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook-5"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer-5"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook-5"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook-5"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook-5"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook-5"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook-5"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-9"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module-5"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter-5"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook-5"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook-5"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad-5"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"reset-noise-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reset-noise-3"}},[e._v("#")]),e._v(" reset_noise "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"reset_noise",sig:{params:[{name:"self"},{name:"n_envs",default:"1",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),a("p",[e._v("Sample new weights for the exploration matrix.")]),e._v(" "),a("p",[e._v(":param n_envs:")]),e._v(" "),a("h3",{attrs:{id:"save-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-5"}},[e._v("#")]),e._v(" save "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"<class 'str'>"}],return:null}}}),e._v(" "),a("p",[e._v("Save model to a given location.")]),e._v(" "),a("p",[e._v(":param path:")]),e._v(" "),a("h3",{attrs:{id:"scale-action-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scale-action-5"}},[e._v("#")]),e._v(" scale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"scale_action",sig:{params:[{name:"self"},{name:"action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [low, high] to [-1, 1]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param action: Action to scale\n:return: Scaled action")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state-5"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule-5"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"set-training-mode-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-training-mode-5"}},[e._v("#")]),e._v(" set_training_mode "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_training_mode",sig:{params:[{name:"self"},{name:"mode",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Put the policy in either training or evaluation mode.")]),e._v(" "),a("p",[e._v("This affects certain modules, such as batch normalisation and dropout.")]),e._v(" "),a("p",[e._v(":param mode: if true, set to training mode, else set to evaluation mode")]),e._v(" "),a("h3",{attrs:{id:"share-memory-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory-5"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict-5"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-5"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty-5"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train-5"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"type-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type-5"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"unscale-action-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unscale-action-5"}},[e._v("#")]),e._v(" unscale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"unscale_action",sig:{params:[{name:"self"},{name:"scaled_action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [-1, 1] to [low, high]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param scaled_action: Action to un-scale")]),e._v(" "),a("h3",{attrs:{id:"xpu-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu-5"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad-5"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"build-mlp-extractor-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-mlp-extractor-5"}},[e._v("#")]),e._v(" _build_mlp_extractor "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build_mlp_extractor",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),a("p",[e._v("Create the policy and value networks.\nPart of the layers can be shared.")]),e._v(" "),a("h3",{attrs:{id:"dummy-schedule-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dummy-schedule-5"}},[e._v("#")]),e._v(" _dummy_schedule "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_dummy_schedule",sig:{params:[{name:"progress_remaining",annotation:"<class 'float'>"}],return:"<class 'float'>"}}}),e._v(" "),a("p",[e._v("(float) Useful for pickling policy.")]),e._v(" "),a("h3",{attrs:{id:"get-action-dist-from-latent-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-action-dist-from-latent-5"}},[e._v("#")]),e._v(" _get_action_dist_from_latent "),a("Badge",{attrs:{text:"ActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_action_dist_from_latent",sig:{params:[{name:"self"},{name:"latent_pi",annotation:"<class 'torch.Tensor'>"}],return:"<class 'stable_baselines3.common.distributions.Distribution'>"}}}),e._v(" "),a("p",[e._v("Retrieve action distribution given the latent codes.")]),e._v(" "),a("p",[e._v(":param latent_pi: Latent code for the actor\n:return: Action distribution")]),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks-5"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"get-constructor-parameters-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-constructor-parameters-5"}},[e._v("#")]),e._v(" _get_constructor_parameters "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_constructor_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Get data that need to be saved in order to re-create the model when loading it from disk.")]),e._v(" "),a("p",[e._v(":return: The dictionary to pass to the as kwargs constructor when reconstruction this model.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict-5"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members-5"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"predict-10"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-10"}},[e._v("#")]),e._v(" _predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.Tensor'>"}}}),e._v(" "),a("p",[e._v("Get the action according to the policy for a given observation.")]),e._v(" "),a("p",[e._v(":param observation:\n:param deterministic: Whether to use stochastic or deterministic actions\n:return: Taken action according to the policy")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-10"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-10"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook-5"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict-5"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")]),e._v(" "),a("h3",{attrs:{id:"update-features-extractor-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#update-features-extractor-5"}},[e._v("#")]),e._v(" _update_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_update_features_extractor",sig:{params:[{name:"self"},{name:"net_kwargs",annotation:"typing.Dict[str, typing.Any]"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Update the network keyword arguments and create a new features extractor object if needed.\nIf a "),a("code",[e._v("features_extractor")]),e._v(" object is passed, then it will be shared.")]),e._v(" "),a("p",[e._v(":param net_kwargs: the base network keyword arguments, without the ones\nrelated to features extractor\n:param features_extractor: a features extractor object.\nIf None, a new object will be created.\n:return: The updated keyword arguments")]),e._v(" "),a("h2",{attrs:{id:"maskablegnn2nodeactorcriticpolicy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#maskablegnn2nodeactorcriticpolicy"}},[e._v("#")]),e._v(" MaskableGNN2NodeActorCriticPolicy")]),e._v(" "),a("p",[e._v("Policy mapping a graph to one of its node and using action masking.")]),e._v(" "),a("p",[e._v("The action is predicted by a GNN without additional reduction layer.\nThe value is modelled as in "),a("code",[e._v("GNNActorCriticPolicy")]),e._v(" by a GNN followed by a reduction layer.")]),e._v(" "),a("p",[e._v("Intended to be used with environment where an observation is a graph and an action\nis the choice of a node in this graph.")]),e._v(" "),a("p",[e._v("NB: here the action space is actually variable (as number of nodes can vary from an observation to another)\nand thus the action mask length is also variable (and should match the number of nodes of the observation graph).")]),e._v(" "),a("h3",{attrs:{id:"constructor-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor-6"}},[e._v("#")]),e._v(" Constructor "),a("Badge",{attrs:{text:"MaskableGNN2NodeActorCriticPolicy",type:"tip"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"MaskableGNN2NodeActorCriticPolicy",sig:{params:[{name:"observation_space",annotation:"<class 'gymnasium.spaces.graph.Graph'>"},{name:"action_space",annotation:"<class 'gymnasium.spaces.space.Space'>"},{name:"lr_schedule",annotation:"typing.Callable[[float], float]"},{name:"net_arch",default:"None",annotation:"typing.Union[typing.List[int], typing.Dict[str, typing.List[int]], NoneType]"},{name:"activation_fn",default:"<class 'torch.nn.modules.activation.Tanh'>",annotation:"typing.Type[torch.nn.modules.module.Module]"},{name:"ortho_init",default:"True",annotation:"<class 'bool'>"},{name:"log_std_init",default:"0.0",annotation:"<class 'float'>"},{name:"features_extractor_class",default:"<class 'skdecide.hub.solver.stable_baselines.gnn.common.torch_layers.GraphFeaturesExtractor'>",annotation:"typing.Type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"},{name:"features_extractor_kwargs",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"action_gnn_class",default:"None",annotation:"typing.Optional[type[torch.nn.modules.module.Module]]"},{name:"action_gnn_kwargs",default:"None",annotation:"typing.Optional[dict[str, typing.Any]]"},{name:"normalize_images",default:"True",annotation:"<class 'bool'>"},{name:"optimizer_class",default:"<class 'torch.optim.adam.Adam'>",annotation:"typing.Type[torch.optim.optimizer.Optimizer]"},{name:"optimizer_kwargs",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"debug",default:"False",annotation:"<class 'bool'>"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Initialize internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("h3",{attrs:{id:"add-module-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#add-module-6"}},[e._v("#")]),e._v(" add_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"add_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a child module to the current module.")]),e._v(" "),a("p",[e._v("The module can be accessed as an attribute using the given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.")]),e._v(" "),a("h3",{attrs:{id:"apply-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#apply-6"}},[e._v("#")]),e._v(" apply "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"apply",sig:{params:[{name:"self",annotation:"~T"},{name:"fn",annotation:"typing.Callable[[ForwardRef('Module')], NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Apply "),a("code",[e._v("fn")]),e._v(" recursively to every submodule (as returned by "),a("code",[e._v(".children()")]),e._v(") as well as self.")]),e._v(" "),a("p",[e._v("Typical use includes initializing the parameters of a model\n(see also :ref:"),a("code",[e._v("nn-init-doc")]),e._v(").")]),e._v(" "),a("p",[e._v("Args:\nfn (:class:"),a("code",[e._v("Module")]),e._v(" -> None): function to be applied to each submodule")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n")])])]),a("h3",{attrs:{id:"bfloat16-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfloat16-6"}},[e._v("#")]),e._v(" bfloat16 "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"bfloat16",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("bfloat16")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"buffers-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buffers-6"}},[e._v("#")]),e._v(" buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"buffers",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\ntorch.Tensor: module buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"children-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#children-6"}},[e._v("#")]),e._v(" children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a child module")]),e._v(" "),a("h3",{attrs:{id:"compile-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-6"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"compile",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Compile this Module's forward using :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("This Module's "),a("code",[e._v("__call__")]),e._v(" method is compiled and all arguments are passed as-is\nto :func:"),a("code",[e._v("torch.compile")]),e._v(".")]),e._v(" "),a("p",[e._v("See :func:"),a("code",[e._v("torch.compile")]),e._v(" for details on the arguments for this function.")]),e._v(" "),a("h3",{attrs:{id:"cpu-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu-6"}},[e._v("#")]),e._v(" cpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cpu",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the CPU.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"cuda-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda-6"}},[e._v("#")]),e._v(" cuda "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"cuda",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the GPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"double-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#double-6"}},[e._v("#")]),e._v(" double "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"double",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("double")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"eval-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval-6"}},[e._v("#")]),e._v(" eval "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"eval",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in evaluation mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("This is equivalent with :meth:"),a("code",[e._v("self.train(False) \\<torch.nn.Module.train>")]),e._v(".")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".eval()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"evaluate-actions-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluate-actions-5"}},[e._v("#")]),e._v(" evaluate_actions "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"evaluate_actions",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"actions",annotation:"<class 'torch.Tensor'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[torch.Tensor]"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, typing.Optional[torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Evaluate actions according to the current policy,\ngiven the observations.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param actions: Actions\n:return: estimated value, log likelihood of taking those actions\nand entropy of the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"extra-repr-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extra-repr-6"}},[e._v("#")]),e._v(" extra_repr "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extra_repr",sig:{params:[{name:"self"}],return:"<class 'str'>"}}}),e._v(" "),a("p",[e._v("Return the extra representation of the module.")]),e._v(" "),a("p",[e._v("To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.")]),e._v(" "),a("h3",{attrs:{id:"extract-features-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extract-features-6"}},[e._v("#")]),e._v(" extract_features "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"extract_features",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Preprocess the observation if needed and extract features.")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param features_extractor: The features extractor to use. If None, then "),a("code",[e._v("self.features_extractor")]),e._v(" is used.\n:return: The extracted features. If features extractor is not shared, returns a tuple with the\nfeatures for the actor and the features for the critic.")]),e._v(" "),a("h3",{attrs:{id:"float-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#float-6"}},[e._v("#")]),e._v(" float "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"float",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("float")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"forward-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward-6"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"forward",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]"}}}),e._v(" "),a("p",[e._v("Forward pass in all the networks (actor and critic)")]),e._v(" "),a("p",[e._v(":param obs: Observation\n:param deterministic: Whether to sample or use deterministic actions\n:param action_masks: Action masks to apply to the action distribution\n:return: action, value and log probability of the action")]),e._v(" "),a("h3",{attrs:{id:"get-buffer-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-buffer-6"}},[e._v("#")]),e._v(" get_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_buffer",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Tensor"}}}),e._v(" "),a("p",[e._v("Return the buffer given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the buffer\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.Tensor: The buffer referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer")]),e._v(" "),a("h3",{attrs:{id:"get-distribution-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-distribution-4"}},[e._v("#")]),e._v(" get_distribution "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_distribution",sig:{params:[{name:"self"},{name:"obs",annotation:"<class 'torch_geometric.data.data.Data'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"<class 'sb3_contrib.common.maskable.distributions.MaskableDistribution'>"}}}),e._v(" "),a("p",[e._v("Get the current policy distribution given the observations.")]),e._v(" "),a("p",[e._v(":param obs:\n:return: the action distribution.")]),e._v(" "),a("h3",{attrs:{id:"get-extra-state-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-extra-state-6"}},[e._v("#")]),e._v(" get_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_extra_state",sig:{params:[{name:"self"}],return:"typing.Any"}}}),e._v(" "),a("p",[e._v("Return any extra state to include in the module's state_dict.")]),e._v(" "),a("p",[e._v("Implement this and a corresponding :func:"),a("code",[e._v("set_extra_state")]),e._v(" for your module\nif you need to store extra state. This function is called when building the\nmodule's "),a("code",[e._v("state_dict()")]),e._v(".")]),e._v(" "),a("p",[e._v("Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.")]),e._v(" "),a("p",[e._v("Returns:\nobject: Any extra state to store in the module's state_dict")]),e._v(" "),a("h3",{attrs:{id:"get-parameter-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-parameter-6"}},[e._v("#")]),e._v(" get_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_parameter",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Parameter"}}}),e._v(" "),a("p",[e._v("Return the parameter given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("See the docstring for "),a("code",[e._v("get_submodule")]),e._v(" for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify "),a("code",[e._v("target")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See "),a("code",[e._v("get_submodule")]),e._v(" for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Parameter: The Parameter referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n"),a("code",[e._v("nn.Parameter")])]),e._v(" "),a("h3",{attrs:{id:"get-submodule-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-submodule-6"}},[e._v("#")]),e._v(" get_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"get_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"}],return:"Module"}}}),e._v(" "),a("p",[e._v("Return the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" which has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To check whether or not we have the "),a("code",[e._v("linear")]),e._v(" submodule, we\nwould call "),a("code",[e._v('get_submodule("net_b.linear")')]),e._v(". To check whether\nwe have the "),a("code",[e._v("conv")]),e._v(" submodule, we would call\n"),a("code",[e._v('get_submodule("net_b.net_c.conv")')]),e._v(".")]),e._v(" "),a("p",[e._v("The runtime of "),a("code",[e._v("get_submodule")]),e._v(" is bounded by the degree\nof module nesting in "),a("code",[e._v("target")]),e._v(". A query against\n"),a("code",[e._v("named_modules")]),e._v(" achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, "),a("code",[e._v("get_submodule")]),e._v(" should always be\nused.")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)")]),e._v(" "),a("p",[e._v("Returns:\ntorch.nn.Module: The submodule referenced by "),a("code",[e._v("target")])]),e._v(" "),a("p",[e._v("Raises:\nAttributeError: If at any point along the path resulting from\nthe target string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"half-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#half-6"}},[e._v("#")]),e._v(" half "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"half",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all floating point parameters and buffers to "),a("code",[e._v("half")]),e._v(" datatype.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"init-weights-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init-weights-6"}},[e._v("#")]),e._v(" init_weights "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"init_weights",sig:{params:[{name:"module",annotation:"<class 'torch.nn.modules.module.Module'>"},{name:"gain",default:"1",annotation:"<class 'float'>"}],return:null}}}),e._v(" "),a("p",[e._v("Orthogonal initialization (used in PPO and A2C)")]),e._v(" "),a("h3",{attrs:{id:"ipu-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ipu-6"}},[e._v("#")]),e._v(" ipu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"ipu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the IPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"is-vectorized-observation-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-vectorized-observation-6"}},[e._v("#")]),e._v(" is_vectorized_observation "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"is_vectorized_observation",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"}],return:"<class 'bool'>"}}}),e._v(" "),a("p",[e._v("Check whether or not the observation is vectorized,\napply transposition to image (so that they are channel-first) if needed.\nThis is used in DQN when sampling random action (epsilon-greedy policy)")]),e._v(" "),a("p",[e._v(":param observation: the input observation to check\n:return: whether the given observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"load-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-6"}},[e._v("#")]),e._v(" load "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"<class 'str'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:"~SelfBaseModel"}}}),e._v(" "),a("p",[e._v("Load model from path.")]),e._v(" "),a("p",[e._v(":param path:\n:param device: Device on which the policy should be loaded.\n:return:")]),e._v(" "),a("h3",{attrs:{id:"load-from-vector-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-vector-6"}},[e._v("#")]),e._v(" load_from_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_from_vector",sig:{params:[{name:"self"},{name:"vector",annotation:"<class 'numpy.ndarray'>"}],return:null}}}),e._v(" "),a("p",[e._v("Load parameters from a 1D vector.")]),e._v(" "),a("p",[e._v(":param vector:")]),e._v(" "),a("h3",{attrs:{id:"load-state-dict-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-state-dict-6"}},[e._v("#")]),e._v(" load_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"load_state_dict",sig:{params:[{name:"self"},{name:"state_dict",annotation:"collections.abc.Mapping[str, typing.Any]"},{name:"strict",default:"True",annotation:"<class 'bool'>"},{name:"assign",default:"False",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into this module and its descendants.")]),e._v(" "),a("p",[e._v("If :attr:"),a("code",[e._v("strict")]),e._v(" is "),a("code",[e._v("True")]),e._v(", then\nthe keys of :attr:"),a("code",[e._v("state_dict")]),e._v(" must exactly match the keys returned\nby this module's :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function.")]),e._v(" "),a("p",[e._v(".. warning::\nIf :attr:"),a("code",[e._v("assign")]),e._v(" is "),a("code",[e._v("True")]),e._v(" the optimizer must be created after\nthe call to :attr:"),a("code",[e._v("load_state_dict")]),e._v(" unless\n:func:"),a("code",[e._v("~torch.__future__.get_swap_module_params_on_conversion")]),e._v(" is "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:"),a("code",[e._v("state_dict")]),e._v(" match the keys returned by this module's\n:meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" function. Default: "),a("code",[e._v("True")]),e._v("\nassign (bool, optional): When set to "),a("code",[e._v("False")]),e._v(", the properties of the tensors\nin the current module are preserved whereas setting it to "),a("code",[e._v("True")]),e._v(" preserves\nproperties of the Tensors in the state dict. The only\nexception is the "),a("code",[e._v("requires_grad")]),e._v(" field of :class:"),a("code",[e._v("~torch.nn.Parameter")]),e._v("s\nfor which the value from the module is preserved.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n"),a("code",[e._v("NamedTuple")]),e._v(" with "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(" fields:\n* "),a("strong",[e._v("missing_keys")]),e._v(" is a list of str containing any keys that are expected\nby this module but missing from the provided "),a("code",[e._v("state_dict")]),e._v(".\n* "),a("strong",[e._v("unexpected_keys")]),e._v(" is a list of str containing the keys that are not\nexpected by this module but present in the provided "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Note:\nIf a parameter or buffer is registered as "),a("code",[e._v("None")]),e._v(" and its corresponding key\nexists in :attr:"),a("code",[e._v("state_dict")]),e._v(", :meth:"),a("code",[e._v("load_state_dict")]),e._v(" will raise a\n"),a("code",[e._v("RuntimeError")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"make-features-extractor-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#make-features-extractor-6"}},[e._v("#")]),e._v(" make_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"make_features_extractor",sig:{params:[{name:"self"}],return:"<class 'stable_baselines3.common.torch_layers.BaseFeaturesExtractor'>"}}}),e._v(" "),a("p",[e._v("Helper method to create a features extractor.")]),e._v(" "),a("h3",{attrs:{id:"modules-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modules-6"}},[e._v("#")]),e._v(" modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"modules",sig:{params:[{name:"self"}],return:"collections.abc.Iterator['Module']"}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network.")]),e._v(" "),a("p",[e._v("Yields:\nModule: a module in the network")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n")])])]),a("h3",{attrs:{id:"mtia-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtia-6"}},[e._v("#")]),e._v(" mtia "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"mtia",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the MTIA.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"named-buffers-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-buffers-6"}},[e._v("#")]),e._v(" named_buffers "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_buffers",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.Tensor]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, torch.Tensor): Tuple containing the name and buffer")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n")])])]),a("h3",{attrs:{id:"named-children-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-children-6"}},[e._v("#")]),e._v(" named_children "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_children",sig:{params:[{name:"self"}],return:"collections.abc.Iterator[tuple[str, 'Module']]"}}}),e._v(" "),a("p",[e._v("Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple containing a name and child module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n")])])]),a("h3",{attrs:{id:"named-modules-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-modules-6"}},[e._v("#")]),e._v(" named_modules "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_modules",sig:{params:[{name:"self"},{name:"memo",default:"None",annotation:"typing.Optional[set['Module']]"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.")]),e._v(" "),a("p",[e._v("Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not")]),e._v(" "),a("p",[e._v("Yields:\n(str, Module): Tuple of name and module")]),e._v(" "),a("p",[e._v("Note:\nDuplicate modules are returned only once. In the following\nexample, "),a("code",[e._v("l")]),e._v(" will be returned only once.")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n")])])]),a("h3",{attrs:{id:"named-parameters-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-parameters-6"}},[e._v("#")]),e._v(" named_parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"named_parameters",sig:{params:[{name:"self"},{name:"prefix",default:"",annotation:"<class 'str'>"},{name:"recurse",default:"True",annotation:"<class 'bool'>"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.")]),e._v(" "),a("p",[e._v("Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.")]),e._v(" "),a("p",[e._v("Yields:\n(str, Parameter): Tuple containing the name and parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n")])])]),a("h3",{attrs:{id:"obs-to-tensor-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#obs-to-tensor-6"}},[e._v("#")]),e._v(" obs_to_tensor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"obs_to_tensor",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance], dict[str, typing.Union[numpy.ndarray, gymnasium.spaces.graph.GraphInstance, list[gymnasium.spaces.graph.GraphInstance]]]]"}],return:"tuple[typing.Union[torch.Tensor, torch_geometric.data.data.Data, dict[str, typing.Union[torch.Tensor, torch_geometric.data.data.Data]]], bool]"}}}),e._v(" "),a("p",[e._v("Convert an input observation to a PyTorch tensor that can be fed to a model.\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:return: The observation as PyTorch tensor\nand whether the observation is vectorized or not")]),e._v(" "),a("h3",{attrs:{id:"parameters-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-6"}},[e._v("#")]),e._v(" parameters "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters",sig:{params:[{name:"self"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"collections.abc.Iterator[torch.nn.parameter.Parameter]"}}}),e._v(" "),a("p",[e._v("Return an iterator over module parameters.")]),e._v(" "),a("p",[e._v("This is typically passed to an optimizer.")]),e._v(" "),a("p",[e._v("Args:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.")]),e._v(" "),a("p",[e._v("Yields:\nParameter: module parameter")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n\\<class 'torch.Tensor'> (20L,)\n\\<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n")])])]),a("h3",{attrs:{id:"parameters-to-vector-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-to-vector-6"}},[e._v("#")]),e._v(" parameters_to_vector "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"parameters_to_vector",sig:{params:[{name:"self"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Convert the parameters to a 1D vector.")]),e._v(" "),a("p",[e._v(":return:")]),e._v(" "),a("h3",{attrs:{id:"predict-11"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-11"}},[e._v("#")]),e._v(" predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),a("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),a("p",[e._v(":param observation: the input observation\n:param state: The last states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\n:param deterministic: Whether or not to return deterministic actions.\n:param action_masks: Action masks to apply to the action distribution\n:return: the model's action and the next state\n(used in recurrent policies)")]),e._v(" "),a("h3",{attrs:{id:"register-backward-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-backward-hook-6"}},[e._v("#")]),e._v(" register_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("This function is deprecated in favor of :meth:"),a("code",[e._v("~torch.nn.Module.register_full_backward_hook")]),e._v(" and\nthe behavior of this function will change in future versions.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-buffer-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-buffer-6"}},[e._v("#")]),e._v(" register_buffer "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_buffer",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"tensor",annotation:"typing.Optional[torch.Tensor]"},{name:"persistent",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Add a buffer to the module.")]),e._v(" "),a("p",[e._v("This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's "),a("code",[e._v("running_mean")]),e._v("\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:"),a("code",[e._v("persistent")]),e._v(" to "),a("code",[e._v("False")]),e._v(". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Buffers can be accessed as attributes using given names.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If "),a("code",[e._v("None")]),e._v(", then operations\nthat run on buffers, such as :attr:"),a("code",[e._v("cuda")]),e._v(", are ignored. If "),a("code",[e._v("None")]),e._v(",\nthe buffer is "),a("strong",[e._v("not")]),e._v(" included in the module's :attr:"),a("code",[e._v("state_dict")]),e._v(".\npersistent (bool): whether the buffer is part of this module's\n:attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n")])])]),a("h3",{attrs:{id:"register-forward-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-hook-6"}},[e._v("#")]),e._v(" register_forward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Optional[typing.Any]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"},{name:"always_call",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time after :func:"),a("code",[e._v("forward")]),e._v(" has computed an output.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("False")]),e._v(" or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:"),a("code",[e._v("forward")]),e._v(" is called. The hook\nshould have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, output) -> None or modified output\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is "),a("code",[e._v("True")]),e._v(", the forward hook will be passed the\n"),a("code",[e._v("kwargs")]),e._v(" given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs, output) -> None or modified output\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If "),a("code",[e._v("True")]),e._v(", the provided "),a("code",[e._v("hook")]),e._v(" will be fired\nbefore all existing "),a("code",[e._v("forward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_hook")]),e._v(" will fire before all hooks\nregistered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If "),a("code",[e._v("True")]),e._v(", the "),a("code",[e._v("hook")]),e._v(" will be passed the\nkwargs given to the forward function.\nDefault: "),a("code",[e._v("False")]),e._v("\nalways_call (bool): If "),a("code",[e._v("True")]),e._v(" the "),a("code",[e._v("hook")]),e._v(" will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-forward-pre-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-forward-pre-hook-6"}},[e._v("#")]),e._v(" register_forward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_forward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Union[typing.Callable[[~T, tuple[typing.Any, ...]], typing.Optional[typing.Any]], typing.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], typing.Optional[tuple[typing.Any, dict[str, typing.Any]]]]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"},{name:"with_kwargs",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a forward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time before :func:"),a("code",[e._v("forward")]),e._v(" is invoked.")]),e._v(" "),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the "),a("code",[e._v("forward")]),e._v(". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args) -> None or modified input\n")])])]),a("p",[e._v("If "),a("code",[e._v("with_kwargs")]),e._v(" is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n")])])]),a("p",[e._v("Args:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("forward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("forward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("forward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_forward_pre_hook")]),e._v(" will fire before all\nhooks registered by this method.\nDefault: "),a("code",[e._v("False")]),e._v("\nwith_kwargs (bool): If true, the "),a("code",[e._v("hook")]),e._v(" will be passed the kwargs\ngiven to the forward function.\nDefault: "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-hook-6"}},[e._v("#")]),e._v(" register_full_backward_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:"),a("code",[e._v("grad_input")]),e._v(" in\nsubsequent computations. :attr:"),a("code",[e._v("grad_input")]),e._v(" will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:"),a("code",[e._v("grad_input")]),e._v(" and :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for all non-Tensor\narguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward")]),e._v(" hooks on\nthis :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-full-backward-pre-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-full-backward-pre-hook-6"}},[e._v("#")]),e._v(" register_full_backward_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_full_backward_pre_hook",sig:{params:[{name:"self"},{name:"hook",annotation:"typing.Callable[[ForwardRef('Module'), typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]"},{name:"prepend",default:"False",annotation:"<class 'bool'>"}],return:"<class 'torch.utils.hooks.RemovableHandle'>"}}}),e._v(" "),a("p",[e._v("Register a backward pre-hook on the module.")]),e._v(" "),a("p",[e._v("The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("hook(module, grad_output) -> tuple[Tensor] or None\n")])])]),a("p",[e._v("The :attr:"),a("code",[e._v("grad_output")]),e._v(" is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:"),a("code",[e._v("grad_output")]),e._v(" in\nsubsequent computations. Entries in :attr:"),a("code",[e._v("grad_output")]),e._v(" will be "),a("code",[e._v("None")]),e._v(" for\nall non-Tensor arguments.")]),e._v(" "),a("p",[e._v("For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.")]),e._v(" "),a("p",[e._v(".. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.")]),e._v(" "),a("p",[e._v("Args:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided "),a("code",[e._v("hook")]),e._v(" will be fired before\nall existing "),a("code",[e._v("backward_pre")]),e._v(" hooks on this\n:class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Otherwise, the provided\n"),a("code",[e._v("hook")]),e._v(" will be fired after all existing "),a("code",[e._v("backward_pre")]),e._v(" hooks\non this :class:"),a("code",[e._v("torch.nn.Module")]),e._v(". Note that global\n"),a("code",[e._v("backward_pre")]),e._v(" hooks registered with\n:func:"),a("code",[e._v("register_module_full_backward_pre_hook")]),e._v(" will fire before\nall hooks registered by this method.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-post-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-post-hook-6"}},[e._v("#")]),e._v(" register_load_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook to be run after module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, incompatible_keys) -> None")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("module")]),e._v(" argument is the current module that this hook is registered\non, and the "),a("code",[e._v("incompatible_keys")]),e._v(" argument is a "),a("code",[e._v("NamedTuple")]),e._v(" consisting\nof attributes "),a("code",[e._v("missing_keys")]),e._v(" and "),a("code",[e._v("unexpected_keys")]),e._v(". "),a("code",[e._v("missing_keys")]),e._v("\nis a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the missing keys and\n"),a("code",[e._v("unexpected_keys")]),e._v(" is a "),a("code",[e._v("list")]),e._v(" of "),a("code",[e._v("str")]),e._v(" containing the unexpected keys.")]),e._v(" "),a("p",[e._v("The given incompatible_keys can be modified inplace if needed.")]),e._v(" "),a("p",[e._v("Note that the checks performed when calling :func:"),a("code",[e._v("load_state_dict")]),e._v(" with\n"),a("code",[e._v("strict=True")]),e._v(" are affected by modifications the hook makes to\n"),a("code",[e._v("missing_keys")]),e._v(" or "),a("code",[e._v("unexpected_keys")]),e._v(", as expected. Additions to either\nset of keys will result in an error being thrown when "),a("code",[e._v("strict=True")]),e._v(", and\nclearing out both missing and unexpected keys will avoid an error.")]),e._v(" "),a("p",[e._v("Returns:\n:class:"),a("code",[e._v("torch.utils.hooks.RemovableHandle")]),e._v(":\na handle that can be used to remove the added hook by calling\n"),a("code",[e._v("handle.remove()")])]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-11"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-11"}},[e._v("#")]),e._v(" register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook to be run before module's :meth:"),a("code",[e._v("~nn.Module.load_state_dict")]),e._v(" is called.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.")]),e._v(" "),a("h3",{attrs:{id:"register-module-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-module-6"}},[e._v("#")]),e._v(" register_module "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_module",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"module",annotation:"typing.Optional[ForwardRef('Module')]"}],return:null}}}),e._v(" "),a("p",[e._v("Alias for :func:"),a("code",[e._v("add_module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-parameter-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-parameter-6"}},[e._v("#")]),e._v(" register_parameter "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_parameter",sig:{params:[{name:"self"},{name:"name",annotation:"<class 'str'>"},{name:"param",annotation:"typing.Optional[torch.nn.parameter.Parameter]"}],return:null}}}),e._v(" "),a("p",[e._v("Add a parameter to the module.")]),e._v(" "),a("p",[e._v("The parameter can be accessed as an attribute using given name.")]),e._v(" "),a("p",[e._v("Args:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n"),a("code",[e._v("None")]),e._v(", then operations that run on parameters, such as :attr:"),a("code",[e._v("cuda")]),e._v(",\nare ignored. If "),a("code",[e._v("None")]),e._v(", the parameter is "),a("strong",[e._v("not")]),e._v(" included in the\nmodule's :attr:"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-post-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-post-hook-6"}},[e._v("#")]),e._v(" register_state_dict_post_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_post_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-pre-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-pre-hook-6"}},[e._v("#")]),e._v(" register_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"register_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a pre-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, prefix, keep_vars) -> None")]),e._v(" "),a("p",[e._v("The registered hooks can be used to perform pre-processing before the "),a("code",[e._v("state_dict")]),e._v("\ncall is made.")]),e._v(" "),a("h3",{attrs:{id:"requires-grad-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#requires-grad-6"}},[e._v("#")]),e._v(" requires_grad_ "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"requires_grad_",sig:{params:[{name:"self",annotation:"~T"},{name:"requires_grad",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Change if autograd should record operations on parameters in this module.")]),e._v(" "),a("p",[e._v("This method sets the parameters' :attr:"),a("code",[e._v("requires_grad")]),e._v(" attributes\nin-place.")]),e._v(" "),a("p",[e._v("This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).")]),e._v(" "),a("p",[e._v("See :ref:"),a("code",[e._v("locally-disable-grad-doc")]),e._v(" for a comparison between\n"),a("code",[e._v(".requires_grad_()")]),e._v(" and several similar mechanisms that may be confused with it.")]),e._v(" "),a("p",[e._v("Args:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"save-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-6"}},[e._v("#")]),e._v(" save "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"<class 'str'>"}],return:null}}}),e._v(" "),a("p",[e._v("Save model to a given location.")]),e._v(" "),a("p",[e._v(":param path:")]),e._v(" "),a("h3",{attrs:{id:"scale-action-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scale-action-6"}},[e._v("#")]),e._v(" scale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"scale_action",sig:{params:[{name:"self"},{name:"action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [low, high] to [-1, 1]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param action: Action to scale\n:return: Scaled action")]),e._v(" "),a("h3",{attrs:{id:"set-extra-state-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-extra-state-6"}},[e._v("#")]),e._v(" set_extra_state "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_extra_state",sig:{params:[{name:"self"},{name:"state",annotation:"typing.Any"}],return:null}}}),e._v(" "),a("p",[e._v("Set extra state contained in the loaded "),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("This function is called from :func:"),a("code",[e._v("load_state_dict")]),e._v(" to handle any extra state\nfound within the "),a("code",[e._v("state_dict")]),e._v(". Implement this function and a corresponding\n:func:"),a("code",[e._v("get_extra_state")]),e._v(" for your module if you need to store extra state within its\n"),a("code",[e._v("state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\nstate (dict): Extra state from the "),a("code",[e._v("state_dict")])]),e._v(" "),a("h3",{attrs:{id:"set-submodule-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-submodule-6"}},[e._v("#")]),e._v(" set_submodule "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_submodule",sig:{params:[{name:"self"},{name:"target",annotation:"<class 'str'>"},{name:"module",annotation:"Module"},{name:"strict",default:"False",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Set the submodule given by "),a("code",[e._v("target")]),e._v(" if it exists, otherwise throw an error.")]),e._v(" "),a("p",[e._v(".. note::\nIf "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(" (default), the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("strict")]),e._v(" is set to "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error if\nthe submodule does not exist.")]),e._v(" "),a("p",[e._v("For example, let's say you have an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(" that\nlooks like this:")]),e._v(" "),a("p",[e._v(".. code-block:: text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(3, 3, 3)\n        )\n        (linear): Linear(3, 3)\n    )\n)\n")])])]),a("p",[e._v("(The diagram shows an "),a("code",[e._v("nn.Module")]),e._v(" "),a("code",[e._v("A")]),e._v(". "),a("code",[e._v("A")]),e._v(" has a nested\nsubmodule "),a("code",[e._v("net_b")]),e._v(", which itself has two submodules "),a("code",[e._v("net_c")]),e._v("\nand "),a("code",[e._v("linear")]),e._v(". "),a("code",[e._v("net_c")]),e._v(" then has a submodule "),a("code",[e._v("conv")]),e._v(".)")]),e._v(" "),a("p",[e._v("To override the "),a("code",[e._v("Conv2d")]),e._v(" with a new submodule "),a("code",[e._v("Linear")]),e._v(", you\ncould call "),a("code",[e._v('set_submodule("net_b.net_c.conv", nn.Linear(1, 1))')]),e._v("\nwhere "),a("code",[e._v("strict")]),e._v(" could be "),a("code",[e._v("True")]),e._v(" or "),a("code",[e._v("False")])]),e._v(" "),a("p",[e._v("To add a new submodule "),a("code",[e._v("Conv2d")]),e._v(" to the existing "),a("code",[e._v("net_b")]),e._v(" module,\nyou would call "),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1))')]),e._v(".")]),e._v(" "),a("p",[e._v("In the above if you set "),a("code",[e._v("strict=True")]),e._v(" and call\n"),a("code",[e._v('set_submodule("net_b.conv", nn.Conv2d(1, 1, 1), strict=True)')]),e._v(", an AttributeError\nwill be raised because "),a("code",[e._v("net_b")]),e._v(" does not have a submodule named "),a("code",[e._v("conv")]),e._v(".")]),e._v(" "),a("p",[e._v("Args:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\nmodule: The module to set the submodule to.\nstrict: If "),a("code",[e._v("False")]),e._v(", the method will replace an existing submodule\nor create a new submodule if the parent module exists. If "),a("code",[e._v("True")]),e._v(",\nthe method will only attempt to replace an existing submodule and throw an error\nif the submodule doesn't already exist.")]),e._v(" "),a("p",[e._v("Raises:\nValueError: If the "),a("code",[e._v("target")]),e._v(" string is empty or if "),a("code",[e._v("module")]),e._v(" is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".\nAttributeError: If at any point along the path resulting from\nthe "),a("code",[e._v("target")]),e._v(" string the (sub)path resolves to a non-existent\nattribute name or an object that is not an instance of "),a("code",[e._v("nn.Module")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"set-training-mode-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-training-mode-6"}},[e._v("#")]),e._v(" set_training_mode "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"set_training_mode",sig:{params:[{name:"self"},{name:"mode",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Put the policy in either training or evaluation mode.")]),e._v(" "),a("p",[e._v("This affects certain modules, such as batch normalisation and dropout.")]),e._v(" "),a("p",[e._v(":param mode: if true, set to training mode, else set to evaluation mode")]),e._v(" "),a("h3",{attrs:{id:"share-memory-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#share-memory-6"}},[e._v("#")]),e._v(" share_memory "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"share_memory",sig:{params:[{name:"self",annotation:"~T"}],return:"~T"}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("torch.Tensor.share_memory_")]),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"state-dict-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-dict-6"}},[e._v("#")]),e._v(" state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"state_dict",sig:{params:[{name:"self"},{name:"*args"},{name:"destination",default:"None"},{name:"prefix",default:""},{name:"keep_vars",default:"False"}]}}}),e._v(" "),a("p",[e._v("Return a dictionary containing references to the whole state of the module.")]),e._v(" "),a("p",[e._v("Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to "),a("code",[e._v("None")]),e._v(" are not included.")]),e._v(" "),a("p",[e._v(".. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.")]),e._v(" "),a("p",[e._v(".. warning::\nCurrently "),a("code",[e._v("state_dict()")]),e._v(" also accepts positional arguments for\n"),a("code",[e._v("destination")]),e._v(", "),a("code",[e._v("prefix")]),e._v(" and "),a("code",[e._v("keep_vars")]),e._v(" in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.")]),e._v(" "),a("p",[e._v(".. warning::\nPlease avoid the use of argument "),a("code",[e._v("destination")]),e._v(" as it is not\ndesigned for end-users.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an "),a("code",[e._v("OrderedDict")]),e._v(" will be created and returned.\nDefault: "),a("code",[e._v("None")]),e._v(".\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: "),a("code",[e._v("''")]),e._v(".\nkeep_vars (bool, optional): by default the :class:"),a("code",[e._v("~torch.Tensor")]),e._v(" s\nreturned in the state dict are detached from autograd. If it's\nset to "),a("code",[e._v("True")]),e._v(", detaching will not be performed.\nDefault: "),a("code",[e._v("False")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\ndict:\na dictionary containing a whole state of the module")]),e._v(" "),a("p",[e._v("Example::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v(">>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n")])])]),a("h3",{attrs:{id:"to-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-6"}},[e._v("#")]),e._v(" to "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to",sig:{params:[{name:"self"},{name:"*args"},{name:"**kwargs"}]}}}),e._v(" "),a("p",[e._v("Move and/or cast the parameters and buffers.")]),e._v(" "),a("p",[e._v("This can be called as")]),e._v(" "),a("p",[e._v(".. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(dtype, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(tensor, non_blocking=False)\n:noindex:")]),e._v(" "),a("p",[e._v(".. function:: to(memory_format=torch.channels_last)\n:noindex:")]),e._v(" "),a("p",[e._v("Its signature is similar to :meth:"),a("code",[e._v("torch.Tensor.to")]),e._v(", but only accepts\nfloating point or complex :attr:"),a("code",[e._v("dtype")]),e._v("\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:"),a("code",[e._v("dtype")]),e._v("\n(if given). The integral parameters and buffers will be moved\n:attr:"),a("code",[e._v("device")]),e._v(", if that is given, but with dtypes unchanged. When\n:attr:"),a("code",[e._v("non_blocking")]),e._v(" is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.")]),e._v(" "),a("p",[e._v("See below for examples.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): the desired device of the parameters\nand buffers in this module\ndtype (:class:"),a("code",[e._v("torch.dtype")]),e._v("): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:"),a("code",[e._v("torch.memory_format")]),e._v("): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("p",[e._v("Examples::")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n')])])]),a("h3",{attrs:{id:"to-empty-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-empty-6"}},[e._v("#")]),e._v(" to_empty "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"to_empty",sig:{params:[{name:"self",annotation:"~T"},{name:"device",annotation:"typing.Union[int, str, torch.device, NoneType]"},{name:"recurse",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move the parameters and buffers to the specified device without copying storage.")]),e._v(" "),a("p",[e._v("Args:\ndevice (:class:"),a("code",[e._v("torch.device")]),e._v("): The desired device of the parameters\nand buffers in this module.\nrecurse (bool): Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"train-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train-6"}},[e._v("#")]),e._v(" train "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self",annotation:"~T"},{name:"mode",default:"True",annotation:"<class 'bool'>"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Set the module in training mode.")]),e._v(" "),a("p",[e._v("This has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:"),a("code",[e._v("Dropout")]),e._v(", :class:"),a("code",[e._v("BatchNorm")]),e._v(",\netc.")]),e._v(" "),a("p",[e._v("Args:\nmode (bool): whether to set training mode ("),a("code",[e._v("True")]),e._v(") or evaluation\nmode ("),a("code",[e._v("False")]),e._v("). Default: "),a("code",[e._v("True")]),e._v(".")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"type-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#type-6"}},[e._v("#")]),e._v(" type "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"type",sig:{params:[{name:"self",annotation:"~T"},{name:"dst_type",annotation:"typing.Union[torch.dtype, str]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Casts all parameters and buffers to :attr:"),a("code",[e._v("dst_type")]),e._v(".")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Args:\ndst_type (type or string): the desired type")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"unscale-action-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unscale-action-6"}},[e._v("#")]),e._v(" unscale_action "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"unscale_action",sig:{params:[{name:"self"},{name:"scaled_action",annotation:"<class 'numpy.ndarray'>"}],return:"<class 'numpy.ndarray'>"}}}),e._v(" "),a("p",[e._v("Rescale the action from [-1, 1] to [low, high]\n(no need for symmetric action space)")]),e._v(" "),a("p",[e._v(":param scaled_action: Action to un-scale")]),e._v(" "),a("h3",{attrs:{id:"xpu-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#xpu-6"}},[e._v("#")]),e._v(" xpu "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"xpu",sig:{params:[{name:"self",annotation:"~T"},{name:"device",default:"None",annotation:"typing.Union[int, torch.device, NoneType]"}],return:"~T"}}}),e._v(" "),a("p",[e._v("Move all model parameters and buffers to the XPU.")]),e._v(" "),a("p",[e._v("This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.")]),e._v(" "),a("p",[e._v(".. note::\nThis method modifies the module in-place.")]),e._v(" "),a("p",[e._v("Arguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device")]),e._v(" "),a("p",[e._v("Returns:\nModule: self")]),e._v(" "),a("h3",{attrs:{id:"zero-grad-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zero-grad-6"}},[e._v("#")]),e._v(" zero_grad "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"zero_grad",sig:{params:[{name:"self"},{name:"set_to_none",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),a("p",[e._v("Reset gradients of all model parameters.")]),e._v(" "),a("p",[e._v("See similar function under :class:"),a("code",[e._v("torch.optim.Optimizer")]),e._v(" for more context.")]),e._v(" "),a("p",[e._v("Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:"),a("code",[e._v("torch.optim.Optimizer.zero_grad")]),e._v(" for details.")]),e._v(" "),a("h3",{attrs:{id:"build-mlp-extractor-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-mlp-extractor-6"}},[e._v("#")]),e._v(" _build_mlp_extractor "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_build_mlp_extractor",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),a("p",[e._v("Create the policy and value networks.\nPart of the layers can be shared.")]),e._v(" "),a("h3",{attrs:{id:"dummy-schedule-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dummy-schedule-6"}},[e._v("#")]),e._v(" _dummy_schedule "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_dummy_schedule",sig:{params:[{name:"progress_remaining",annotation:"<class 'float'>"}],return:"<class 'float'>"}}}),e._v(" "),a("p",[e._v("(float) Useful for pickling policy.")]),e._v(" "),a("h3",{attrs:{id:"get-action-dist-from-latent-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-action-dist-from-latent-6"}},[e._v("#")]),e._v(" _get_action_dist_from_latent "),a("Badge",{attrs:{text:"MaskableActorCriticPolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_action_dist_from_latent",sig:{params:[{name:"self"},{name:"latent_pi",annotation:"<class 'torch.Tensor'>"}],return:"<class 'sb3_contrib.common.maskable.distributions.MaskableDistribution'>"}}}),e._v(" "),a("p",[e._v("Retrieve action distribution given the latent codes.")]),e._v(" "),a("p",[e._v(":param latent_pi: Latent code for the actor\n:return: Action distribution")]),e._v(" "),a("h3",{attrs:{id:"get-backward-hooks-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-backward-hooks-6"}},[e._v("#")]),e._v(" _get_backward_hooks "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_backward_hooks",sig:{params:[{name:"self"}]}}}),e._v(" "),a("p",[e._v("Return the backward hooks for use in the call function.")]),e._v(" "),a("p",[e._v("It returns two lists, one with the full backward hooks and one with the non-full\nbackward hooks.")]),e._v(" "),a("h3",{attrs:{id:"get-constructor-parameters-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-constructor-parameters-6"}},[e._v("#")]),e._v(" _get_constructor_parameters "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_get_constructor_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Get data that need to be saved in order to re-create the model when loading it from disk.")]),e._v(" "),a("p",[e._v(":return: The dictionary to pass to the as kwargs constructor when reconstruction this model.")]),e._v(" "),a("h3",{attrs:{id:"load-from-state-dict-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#load-from-state-dict-6"}},[e._v("#")]),e._v(" _load_from_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_load_from_state_dict",sig:{params:[{name:"self"},{name:"state_dict"},{name:"prefix"},{name:"local_metadata"},{name:"strict"},{name:"missing_keys"},{name:"unexpected_keys"},{name:"error_msgs"}]}}}),e._v(" "),a("p",[e._v("Copy parameters and buffers from :attr:"),a("code",[e._v("state_dict")]),e._v(" into only this module, but not its descendants.")]),e._v(" "),a("p",[e._v("This is called on every submodule\nin :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". Metadata saved for this\nmodule in input :attr:"),a("code",[e._v("state_dict")]),e._v(" is provided as :attr:"),a("code",[e._v("local_metadata")]),e._v(".\nFor state dicts without metadata, :attr:"),a("code",[e._v("local_metadata")]),e._v(" is empty.\nSubclasses can achieve class-specific backward compatible loading using\nthe version number at "),a("code",[e._v('local_metadata.get("version", None)')]),e._v(".\nAdditionally, :attr:"),a("code",[e._v("local_metadata")]),e._v(" can also contain the key\n"),a("code",[e._v("assign_to_params_buffers")]),e._v(" that indicates whether keys should be\nassigned their corresponding tensor in the state_dict.")]),e._v(" "),a("p",[e._v(".. note::\n:attr:"),a("code",[e._v("state_dict")]),e._v(" is not the same object as the input\n:attr:"),a("code",[e._v("state_dict")]),e._v(" to :meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")]),e._v(". So\nit can be modified.")]),e._v(" "),a("p",[e._v("Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nprefix (str): the prefix for parameters and buffers used in this\nmodule\nlocal_metadata (dict): a dict containing the metadata for this module.\nSee\nstrict (bool): whether to strictly enforce that the keys in\n:attr:"),a("code",[e._v("state_dict")]),e._v(" with :attr:"),a("code",[e._v("prefix")]),e._v(" match the names of\nparameters and buffers in this module\nmissing_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add missing keys to\nthis list\nunexpected_keys (list of str): if "),a("code",[e._v("strict=True")]),e._v(", add unexpected\nkeys to this list\nerror_msgs (list of str): error messages should be added to this\nlist, and will be reported together in\n:meth:"),a("code",[e._v("~torch.nn.Module.load_state_dict")])]),e._v(" "),a("h3",{attrs:{id:"named-members-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-members-6"}},[e._v("#")]),e._v(" _named_members "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_named_members",sig:{params:[{name:"self"},{name:"get_members_fn"},{name:"prefix",default:""},{name:"recurse",default:"True"},{name:"remove_duplicate",default:"True",annotation:"<class 'bool'>"}]}}}),e._v(" "),a("p",[e._v("Help yield various names + members of modules.")]),e._v(" "),a("h3",{attrs:{id:"predict-12"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#predict-12"}},[e._v("#")]),e._v(" _predict "),a("Badge",{attrs:{text:"BasePolicy",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[torch.Tensor, typing.Dict[str, torch.Tensor]]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"},{name:"action_masks",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:"<class 'torch.Tensor'>"}}}),e._v(" "),a("p",[e._v("Get the action according to the policy for a given observation.")]),e._v(" "),a("p",[e._v(":param observation:\n:param deterministic: Whether to use stochastic or deterministic actions\n:param action_masks: Action masks to apply to the action distribution\n:return: Taken action according to the policy")]),e._v(" "),a("h3",{attrs:{id:"register-load-state-dict-pre-hook-12"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-load-state-dict-pre-hook-12"}},[e._v("#")]),e._v(" _register_load_state_dict_pre_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_load_state_dict_pre_hook",sig:{params:[{name:"self"},{name:"hook"},{name:"with_module",default:"False"}]}}}),e._v(" "),a("p",[e._v("See :meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" for details.")]),e._v(" "),a("p",[e._v("A subtle difference is that if "),a("code",[e._v("with_module")]),e._v(" is set to "),a("code",[e._v("False")]),e._v(", then the\nhook will not take the "),a("code",[e._v("module")]),e._v(" as the first argument whereas\n:meth:"),a("code",[e._v("~torch.nn.Module.register_load_state_dict_pre_hook")]),e._v(" always takes the\n"),a("code",[e._v("module")]),e._v(" as the first argument.")]),e._v(" "),a("p",[e._v("Arguments:\nhook (Callable): Callable hook that will be invoked before\nloading the state dict.\nwith_module (bool, optional): Whether or not to pass the module\ninstance to the hook as the first parameter.")]),e._v(" "),a("h3",{attrs:{id:"register-state-dict-hook-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#register-state-dict-hook-6"}},[e._v("#")]),e._v(" _register_state_dict_hook "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_register_state_dict_hook",sig:{params:[{name:"self"},{name:"hook"}]}}}),e._v(" "),a("p",[e._v("Register a post-hook for the :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(" method.")]),e._v(" "),a("p",[e._v("It should have the following signature::\nhook(module, state_dict, prefix, local_metadata) -> None or state_dict")]),e._v(" "),a("p",[e._v("The registered hooks can modify the "),a("code",[e._v("state_dict")]),e._v(" inplace or return a new one.\nIf a new "),a("code",[e._v("state_dict")]),e._v(" is returned, it will only be respected if it is the root\nmodule that :meth:"),a("code",[e._v("~nn.Module.state_dict")]),e._v(" is called from.")]),e._v(" "),a("h3",{attrs:{id:"save-to-state-dict-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-to-state-dict-6"}},[e._v("#")]),e._v(" _save_to_state_dict "),a("Badge",{attrs:{text:"Module",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_save_to_state_dict",sig:{params:[{name:"self"},{name:"destination"},{name:"prefix"},{name:"keep_vars"}]}}}),e._v(" "),a("p",[e._v("Save module state to the "),a("code",[e._v("destination")]),e._v(" dictionary.")]),e._v(" "),a("p",[e._v("The "),a("code",[e._v("destination")]),e._v(" dictionary will contain the state\nof the module, but not its descendants. This is called on every\nsubmodule in :meth:"),a("code",[e._v("~torch.nn.Module.state_dict")]),e._v(".")]),e._v(" "),a("p",[e._v("In rare cases, subclasses can achieve class-specific behavior by\noverriding this method with custom logic.")]),e._v(" "),a("p",[e._v("Args:\ndestination (dict): a dict where state will be stored\nprefix (str): the prefix for parameters and buffers used in this\nmodule")]),e._v(" "),a("h3",{attrs:{id:"update-features-extractor-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#update-features-extractor-6"}},[e._v("#")]),e._v(" _update_features_extractor "),a("Badge",{attrs:{text:"BaseModel",type:"warn"}})],1),e._v(" "),a("skdecide-signature",{attrs:{name:"_update_features_extractor",sig:{params:[{name:"self"},{name:"net_kwargs",annotation:"typing.Dict[str, typing.Any]"},{name:"features_extractor",default:"None",annotation:"typing.Optional[stable_baselines3.common.torch_layers.BaseFeaturesExtractor]"}],return:"typing.Dict[str, typing.Any]"}}}),e._v(" "),a("p",[e._v("Update the network keyword arguments and create a new features extractor object if needed.\nIf a "),a("code",[e._v("features_extractor")]),e._v(" object is passed, then it will be shared.")]),e._v(" "),a("p",[e._v(":param net_kwargs: the base network keyword arguments, without the ones\nrelated to features extractor\n:param features_extractor: a features extractor object.\nIf None, a new object will be created.\n:return: The updated keyword arguments")])],1)}),[],!1,null,null,null);t.default=r.exports}}]);