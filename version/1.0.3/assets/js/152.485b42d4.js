(window.webpackJsonp=window.webpackJsonp||[]).push([[152],{664:function(e,a,t){"use strict";t.r(a);var n=t(38),r=Object(n.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"hub-solver-stable-baselines-gnn-common-off-policy-algorithm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hub-solver-stable-baselines-gnn-common-off-policy-algorithm"}},[e._v("#")]),e._v(" hub.solver.stable_baselines.gnn.common.off_policy_algorithm")]),e._v(" "),t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"custom-block-title"},[e._v("Domain specification")]),e._v(" "),t("skdecide-summary")],1),e._v(" "),t("h2",{attrs:{id:"graphoffpolicyalgorithm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#graphoffpolicyalgorithm"}},[e._v("#")]),e._v(" GraphOffPolicyAlgorithm")]),e._v(" "),t("p",[e._v("Base class for On-Policy algorithms (ex: SAC/TD3) with graph observations.")]),e._v(" "),t("h3",{attrs:{id:"constructor"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#constructor"}},[e._v("#")]),e._v(" Constructor "),t("Badge",{attrs:{text:"GraphOffPolicyAlgorithm",type:"tip"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"GraphOffPolicyAlgorithm",sig:{params:[{name:"policy",annotation:"typing.Union[str, type[stable_baselines3.common.policies.ActorCriticPolicy]]"},{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"replay_buffer_class",default:"None",annotation:"typing.Optional[type[stable_baselines3.common.buffers.ReplayBuffer]]"},{name:"**kwargs"}]}}}),e._v(" "),t("p",[e._v("Initialize self.  See help(type(self)) for accurate signature.")]),e._v(" "),t("h3",{attrs:{id:"collect-rollouts"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#collect-rollouts"}},[e._v("#")]),e._v(" collect_rollouts "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"collect_rollouts",sig:{params:[{name:"self"},{name:"env",annotation:"<class 'stable_baselines3.common.vec_env.base_vec_env.VecEnv'>"},{name:"callback",annotation:"<class 'stable_baselines3.common.callbacks.BaseCallback'>"},{name:"train_freq",annotation:"<class 'stable_baselines3.common.type_aliases.TrainFreq'>"},{name:"replay_buffer",annotation:"<class 'stable_baselines3.common.buffers.ReplayBuffer'>"},{name:"action_noise",default:"None",annotation:"typing.Optional[stable_baselines3.common.noise.ActionNoise]"},{name:"learning_starts",default:"0",annotation:"<class 'int'>"},{name:"log_interval",default:"None",annotation:"typing.Optional[int]"}],return:"<class 'stable_baselines3.common.type_aliases.RolloutReturn'>"}}}),e._v(" "),t("p",[e._v("Collect experiences and store them into a "),t("code",[e._v("ReplayBuffer")]),e._v(".")]),e._v(" "),t("p",[e._v(":param env: The training environment\n:param callback: Callback that will be called at each step\n(and at the beginning and end of the rollout)\n:param train_freq: How much experience to collect\nby doing rollouts of current policy.\nEither "),t("code",[e._v("TrainFreq(\\<n>, TrainFrequencyUnit.STEP)")]),e._v("\nor "),t("code",[e._v("TrainFreq(\\<n>, TrainFrequencyUnit.EPISODE)")]),e._v("\nwith "),t("code",[e._v("\\<n>")]),e._v(" being an integer greater than 0.\n:param action_noise: Action noise that will be used for exploration\nRequired for deterministic policy (e.g. TD3). This can also be used\nin addition to the stochastic policy for SAC.\n:param learning_starts: Number of steps before learning for the warm-up phase.\n:param replay_buffer:\n:param log_interval: Log data every "),t("code",[e._v("log_interval")]),e._v(" episodes\n:return:")]),e._v(" "),t("h3",{attrs:{id:"get-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-env"}},[e._v("#")]),e._v(" get_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_env",sig:{params:[{name:"self"}],return:"typing.Optional[stable_baselines3.common.vec_env.base_vec_env.VecEnv]"}}}),e._v(" "),t("p",[e._v("Returns the current environment (can be None if not defined).")]),e._v(" "),t("p",[e._v(":return: The current environment")]),e._v(" "),t("h3",{attrs:{id:"get-parameters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-parameters"}},[e._v("#")]),e._v(" get_parameters "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_parameters",sig:{params:[{name:"self"}],return:"typing.Dict[str, typing.Dict]"}}}),e._v(" "),t("p",[e._v("Return the parameters of the agent. This includes parameters from different networks, e.g.\ncritics (value functions) and policies (pi functions).")]),e._v(" "),t("p",[e._v(":return: Mapping of from names of the objects to PyTorch state-dicts.")]),e._v(" "),t("h3",{attrs:{id:"get-vec-normalize-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-vec-normalize-env"}},[e._v("#")]),e._v(" get_vec_normalize_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"get_vec_normalize_env",sig:{params:[{name:"self"}],return:"typing.Optional[stable_baselines3.common.vec_env.vec_normalize.VecNormalize]"}}}),e._v(" "),t("p",[e._v("Return the "),t("code",[e._v("VecNormalize")]),e._v(" wrapper of the training env\nif it exists.")]),e._v(" "),t("p",[e._v(":return: The "),t("code",[e._v("VecNormalize")]),e._v(" env.")]),e._v(" "),t("h3",{attrs:{id:"learn"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#learn"}},[e._v("#")]),e._v(" learn "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"learn",sig:{params:[{name:"self",annotation:"~SelfOffPolicyAlgorithm"},{name:"total_timesteps",annotation:"<class 'int'>"},{name:"callback",default:"None",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"log_interval",default:"4",annotation:"<class 'int'>"},{name:"tb_log_name",default:"run",annotation:"<class 'str'>"},{name:"reset_num_timesteps",default:"True",annotation:"<class 'bool'>"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"~SelfOffPolicyAlgorithm"}}}),e._v(" "),t("p",[e._v("Return a trained model.")]),e._v(" "),t("p",[e._v(":param total_timesteps: The total number of samples (env steps) to train on\n:param callback: callback(s) called at every step with state of the algorithm.\n:param log_interval: for on-policy algos (e.g., PPO, A2C, ...) this is the number of\ntraining iterations (i.e., log_interval * n_steps * n_envs timesteps) before logging;\nfor off-policy algos (e.g., TD3, SAC, ...) this is the number of episodes before\nlogging.\n:param tb_log_name: the name of the run for TensorBoard logging\n:param reset_num_timesteps: whether or not to reset the current timestep number (used in logging)\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: the trained model")]),e._v(" "),t("h3",{attrs:{id:"load"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load"}},[e._v("#")]),e._v(" load "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"load",sig:{params:[{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"env",default:"None",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv'), NoneType]"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"},{name:"custom_objects",default:"None",annotation:"typing.Optional[typing.Dict[str, typing.Any]]"},{name:"print_system_info",default:"False",annotation:"<class 'bool'>"},{name:"force_reset",default:"True",annotation:"<class 'bool'>"},{name:"**kwargs"}],return:"~SelfBaseAlgorithm"}}}),e._v(" "),t("p",[e._v("Load the model from a zip-file.\nWarning: "),t("code",[e._v("load")]),e._v(" re-creates the model from scratch, it does not update it in-place!\nFor an in-place load use "),t("code",[e._v("set_parameters")]),e._v(" instead.")]),e._v(" "),t("p",[e._v(":param path: path to the file (or a file-like) where to\nload the agent from\n:param env: the new environment to run the loaded model on\n(can be None if you only need prediction from a trained model) has priority over any saved environment\n:param device: Device on which the code should run.\n:param custom_objects: Dictionary of objects to replace\nupon loading. If a variable is present in this dictionary as a\nkey, it will not be deserialized and the corresponding item\nwill be used instead. Similar to custom_objects in\n"),t("code",[e._v("keras.models.load_model")]),e._v(". Useful when you have an object in\nfile that can not be deserialized.\n:param print_system_info: Whether to print system info from the saved model\nand the current system info (useful to debug loading issues)\n:param force_reset: Force call to "),t("code",[e._v("reset()")]),e._v(" before training\nto avoid unexpected behavior.\nSee https://github.com/DLR-RM/stable-baselines3/issues/597\n:param kwargs: extra arguments to change the model when loading\n:return: new model instance with loaded parameters")]),e._v(" "),t("h3",{attrs:{id:"load-replay-buffer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load-replay-buffer"}},[e._v("#")]),e._v(" load_replay_buffer "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"load_replay_buffer",sig:{params:[{name:"self"},{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"truncate_last_traj",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),t("p",[e._v("Load a replay buffer from a pickle file.")]),e._v(" "),t("p",[e._v(":param path: Path to the pickled replay buffer.\n:param truncate_last_traj: When using "),t("code",[e._v("HerReplayBuffer")]),e._v(" with online sampling:\nIf set to "),t("code",[e._v("True")]),e._v(", we assume that the last trajectory in the replay buffer was finished\n(and truncate it).\nIf set to "),t("code",[e._v("False")]),e._v(", we assume that we continue the same trajectory (same episode).")]),e._v(" "),t("h3",{attrs:{id:"predict"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#predict"}},[e._v("#")]),e._v(" predict "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"predict",sig:{params:[{name:"self"},{name:"observation",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"state",default:"None",annotation:"typing.Optional[typing.Tuple[numpy.ndarray, ...]]"},{name:"episode_start",default:"None",annotation:"typing.Optional[numpy.ndarray]"},{name:"deterministic",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[numpy.ndarray, typing.Optional[typing.Tuple[numpy.ndarray, ...]]]"}}}),e._v(" "),t("p",[e._v("Get the policy action from an observation (and optional hidden state).\nIncludes sugar-coating to handle different observations (e.g. normalizing images).")]),e._v(" "),t("p",[e._v(":param observation: the input observation\n:param state: The last hidden states (can be None, used in recurrent policies)\n:param episode_start: The last masks (can be None, used in recurrent policies)\nthis correspond to beginning of episodes,\nwhere the hidden states of the RNN must be reset.\n:param deterministic: Whether or not to return deterministic actions.\n:return: the model's action and the next hidden state\n(used in recurrent policies)")]),e._v(" "),t("h3",{attrs:{id:"save"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#save"}},[e._v("#")]),e._v(" save "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"save",sig:{params:[{name:"self"},{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"},{name:"exclude",default:"None",annotation:"typing.Optional[typing.Iterable[str]]"},{name:"include",default:"None",annotation:"typing.Optional[typing.Iterable[str]]"}],return:null}}}),e._v(" "),t("p",[e._v("Save all the attributes of the object and the model parameters in a zip-file.")]),e._v(" "),t("p",[e._v(":param path: path to the file where the rl agent should be saved\n:param exclude: name of parameters that should be excluded in addition to the default ones\n:param include: name of parameters that might be excluded but should be included anyway")]),e._v(" "),t("h3",{attrs:{id:"save-replay-buffer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#save-replay-buffer"}},[e._v("#")]),e._v(" save_replay_buffer "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"save_replay_buffer",sig:{params:[{name:"self"},{name:"path",annotation:"typing.Union[str, pathlib.Path, io.BufferedIOBase]"}],return:null}}}),e._v(" "),t("p",[e._v("Save the replay buffer as a pickle file.")]),e._v(" "),t("p",[e._v(":param path: Path to the file where the replay buffer should be saved.\nif path is a str or pathlib.Path, the path is automatically created if necessary.")]),e._v(" "),t("h3",{attrs:{id:"set-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-env"}},[e._v("#")]),e._v(" set_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_env",sig:{params:[{name:"self"},{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"force_reset",default:"True",annotation:"<class 'bool'>"}],return:null}}}),e._v(" "),t("p",[e._v("Checks the validity of the environment, and if it is coherent, set it as the current environment.\nFurthermore wrap any non vectorized env into a vectorized\nchecked parameters:")]),e._v(" "),t("ul",[t("li",[e._v("observation_space")]),e._v(" "),t("li",[e._v("action_space")])]),e._v(" "),t("p",[e._v(":param env: The environment for learning a policy\n:param force_reset: Force call to "),t("code",[e._v("reset()")]),e._v(" before training\nto avoid unexpected behavior.\nSee issue https://github.com/DLR-RM/stable-baselines3/issues/597")]),e._v(" "),t("h3",{attrs:{id:"set-logger"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-logger"}},[e._v("#")]),e._v(" set_logger "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_logger",sig:{params:[{name:"self"},{name:"logger",annotation:"<class 'stable_baselines3.common.logger.Logger'>"}],return:null}}}),e._v(" "),t("p",[e._v("Setter for for logger object.")]),e._v(" "),t("p",[e._v(".. warning::")]),e._v(" "),t("p",[e._v("When passing a custom logger object,\nthis will overwrite "),t("code",[e._v("tensorboard_log")]),e._v(" and "),t("code",[e._v("verbose")]),e._v(" settings\npassed to the constructor.")]),e._v(" "),t("h3",{attrs:{id:"set-parameters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-parameters"}},[e._v("#")]),e._v(" set_parameters "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_parameters",sig:{params:[{name:"self"},{name:"load_path_or_dict",annotation:"typing.Union[str, typing.Dict[str, torch.Tensor]]"},{name:"exact_match",default:"True",annotation:"<class 'bool'>"},{name:"device",default:"auto",annotation:"typing.Union[torch.device, str]"}],return:null}}}),e._v(" "),t("p",[e._v("Load parameters from a given zip-file or a nested dictionary containing parameters for\ndifferent modules (see "),t("code",[e._v("get_parameters")]),e._v(").")]),e._v(" "),t("p",[e._v(":param load_path_or_iter: Location of the saved data (path or file-like, see "),t("code",[e._v("save")]),e._v("), or a nested\ndictionary containing nn.Module parameters used by the policy. The dictionary maps\nobject names to a state-dictionary returned by "),t("code",[e._v("torch.nn.Module.state_dict()")]),e._v(".\n:param exact_match: If True, the given parameters should include parameters for each\nmodule and each of their parameters, otherwise raises an Exception. If set to False, this\ncan be used to update only specific parameters.\n:param device: Device on which the code should run.")]),e._v(" "),t("h3",{attrs:{id:"set-random-seed"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-random-seed"}},[e._v("#")]),e._v(" set_random_seed "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"set_random_seed",sig:{params:[{name:"self"},{name:"seed",default:"None",annotation:"typing.Optional[int]"}],return:null}}}),e._v(" "),t("p",[e._v("Set the seed of the pseudo-random generators\n(python, numpy, pytorch, gym, action_space)")]),e._v(" "),t("p",[e._v(":param seed:")]),e._v(" "),t("h3",{attrs:{id:"train"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#train"}},[e._v("#")]),e._v(" train "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"train",sig:{params:[{name:"self"},{name:"gradient_steps",annotation:"<class 'int'>"},{name:"batch_size",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),t("p",[e._v("Sample the replay buffer and do the updates\n(gradient descent and update target networks)")]),e._v(" "),t("h3",{attrs:{id:"convert-train-freq"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#convert-train-freq"}},[e._v("#")]),e._v(" _convert_train_freq "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_convert_train_freq",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Convert "),t("code",[e._v("train_freq")]),e._v(" parameter (int or tuple)\nto a TrainFreq object.")]),e._v(" "),t("h3",{attrs:{id:"dump-logs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dump-logs"}},[e._v("#")]),e._v(" _dump_logs "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_dump_logs",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Write log.")]),e._v(" "),t("h3",{attrs:{id:"excluded-save-params"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#excluded-save-params"}},[e._v("#")]),e._v(" _excluded_save_params "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_excluded_save_params",sig:{params:[{name:"self"}],return:"typing.List[str]"}}}),e._v(" "),t("p",[e._v("Returns the names of the parameters that should be excluded from being\nsaved by pickling. E.g. replay buffers are skipped by default\nas they take up a lot of space. PyTorch variables should be excluded\nwith this so they can be stored with "),t("code",[e._v("th.save")]),e._v(".")]),e._v(" "),t("p",[e._v(":return: List of parameters that should be excluded from being saved with pickle.")]),e._v(" "),t("h3",{attrs:{id:"get-policy-from-name"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-policy-from-name"}},[e._v("#")]),e._v(" _get_policy_from_name "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_get_policy_from_name",sig:{params:[{name:"self"},{name:"policy_name",annotation:"<class 'str'>"}],return:"typing.Type[stable_baselines3.common.policies.BasePolicy]"}}}),e._v(" "),t("p",[e._v("Get a policy class from its name representation.")]),e._v(" "),t("p",[e._v('The goal here is to standardize policy naming, e.g.\nall algorithms can call upon "MlpPolicy" or "CnnPolicy",\nand they receive respective policies that work for them.')]),e._v(" "),t("p",[e._v(":param policy_name: Alias of the policy\n:return: A policy class (type)")]),e._v(" "),t("h3",{attrs:{id:"get-torch-save-params"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#get-torch-save-params"}},[e._v("#")]),e._v(" _get_torch_save_params "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_get_torch_save_params",sig:{params:[{name:"self"}],return:"typing.Tuple[typing.List[str], typing.List[str]]"}}}),e._v(" "),t("p",[e._v("Get the name of the torch variables that will be saved with\nPyTorch "),t("code",[e._v("th.save")]),e._v(", "),t("code",[e._v("th.load")]),e._v(" and "),t("code",[e._v("state_dicts")]),e._v(" instead of the default\npickling strategy. This is to handle device placement correctly.")]),e._v(" "),t("p",[e._v('Names can point to specific variables under classes, e.g.\n"policy.optimizer" would point to '),t("code",[e._v("optimizer")]),e._v(" object of "),t("code",[e._v("self.policy")]),e._v("\nif this object.")]),e._v(" "),t("p",[e._v(":return:\nList of Torch variables whose state dicts to save (e.g. th.nn.Modules),\nand list of other Torch variables to store with "),t("code",[e._v("th.save")]),e._v(".")]),e._v(" "),t("h3",{attrs:{id:"init-callback"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init-callback"}},[e._v("#")]),e._v(" _init_callback "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_init_callback",sig:{params:[{name:"self"},{name:"callback",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"<class 'stable_baselines3.common.callbacks.BaseCallback'>"}}}),e._v(" "),t("p",[e._v(":param callback: Callback(s) called at every step with state of the algorithm.\n:param progress_bar: Display a progress bar using tqdm and rich.\n:return: A hybrid callback calling "),t("code",[e._v("callback")]),e._v(" and performing evaluation.")]),e._v(" "),t("h3",{attrs:{id:"on-step"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#on-step"}},[e._v("#")]),e._v(" _on_step "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_on_step",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Method called after each step in the environment.\nIt is meant to trigger DQN target network update\nbut can be used for other purposes")]),e._v(" "),t("h3",{attrs:{id:"sample-action"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sample-action"}},[e._v("#")]),e._v(" _sample_action "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_sample_action",sig:{params:[{name:"self"},{name:"learning_starts",annotation:"<class 'int'>"},{name:"action_noise",default:"None",annotation:"typing.Optional[stable_baselines3.common.noise.ActionNoise]"},{name:"n_envs",default:"1",annotation:"<class 'int'>"}],return:"typing.Tuple[numpy.ndarray, numpy.ndarray]"}}}),e._v(" "),t("p",[e._v("Sample an action according to the exploration policy.\nThis is either done by sampling the probability distribution of the policy,\nor sampling a random action (from a uniform distribution over the action space)\nor by adding noise to the deterministic output.")]),e._v(" "),t("p",[e._v(":param action_noise: Action noise that will be used for exploration\nRequired for deterministic policy (e.g. TD3). This can also be used\nin addition to the stochastic policy for SAC.\n:param learning_starts: Number of steps before learning for the warm-up phase.\n:param n_envs:\n:return: action to take in the environment\nand scaled action that will be stored in the replay buffer.\nThe two differs when the action space is not normalized (bounds are not [-1, 1]).")]),e._v(" "),t("h3",{attrs:{id:"setup-learn"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-learn"}},[e._v("#")]),e._v(" _setup_learn "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_learn",sig:{params:[{name:"self"},{name:"total_timesteps",annotation:"<class 'int'>"},{name:"callback",default:"None",annotation:"typing.Union[NoneType, typing.Callable, typing.List[ForwardRef('BaseCallback')], ForwardRef('BaseCallback')]"},{name:"reset_num_timesteps",default:"True",annotation:"<class 'bool'>"},{name:"tb_log_name",default:"run",annotation:"<class 'str'>"},{name:"progress_bar",default:"False",annotation:"<class 'bool'>"}],return:"typing.Tuple[int, stable_baselines3.common.callbacks.BaseCallback]"}}}),e._v(" "),t("p",[e._v("cf "),t("code",[e._v("BaseAlgorithm")]),e._v(".")]),e._v(" "),t("h3",{attrs:{id:"setup-lr-schedule"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-lr-schedule"}},[e._v("#")]),e._v(" _setup_lr_schedule "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_lr_schedule",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Transform to callable if needed.")]),e._v(" "),t("h3",{attrs:{id:"setup-model"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#setup-model"}},[e._v("#")]),e._v(" _setup_model "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_setup_model",sig:{params:[{name:"self"}],return:null}}}),e._v(" "),t("p",[e._v("Create networks, buffer and optimizers.")]),e._v(" "),t("h3",{attrs:{id:"store-transition"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#store-transition"}},[e._v("#")]),e._v(" _store_transition "),t("Badge",{attrs:{text:"OffPolicyAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_store_transition",sig:{params:[{name:"self"},{name:"replay_buffer",annotation:"<class 'stable_baselines3.common.buffers.ReplayBuffer'>"},{name:"buffer_action",annotation:"<class 'numpy.ndarray'>"},{name:"new_obs",annotation:"typing.Union[numpy.ndarray, typing.Dict[str, numpy.ndarray]]"},{name:"reward",annotation:"<class 'numpy.ndarray'>"},{name:"dones",annotation:"<class 'numpy.ndarray'>"},{name:"infos",annotation:"typing.List[typing.Dict[str, typing.Any]]"}],return:null}}}),e._v(" "),t("p",[e._v("Store transition in the replay buffer.\nWe store the normalized action and the unnormalized observation.\nIt also handles terminal observations (because VecEnv resets automatically).")]),e._v(" "),t("p",[e._v(":param replay_buffer: Replay buffer object where to store the transition.\n:param buffer_action: normalized action\n:param new_obs: next observation in the current episode\nor first observation of the episode (when dones is True)\n:param reward: reward for the current transition\n:param dones: Termination signal\n:param infos: List of additional information about the transition.\nIt may contain the terminal observations and information about timeout.")]),e._v(" "),t("h3",{attrs:{id:"update-current-progress-remaining"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-current-progress-remaining"}},[e._v("#")]),e._v(" _update_current_progress_remaining "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_current_progress_remaining",sig:{params:[{name:"self"},{name:"num_timesteps",annotation:"<class 'int'>"},{name:"total_timesteps",annotation:"<class 'int'>"}],return:null}}}),e._v(" "),t("p",[e._v("Compute current progress remaining (starts from 1 and ends to 0)")]),e._v(" "),t("p",[e._v(":param num_timesteps: current number of timesteps\n:param total_timesteps:")]),e._v(" "),t("h3",{attrs:{id:"update-info-buffer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-info-buffer"}},[e._v("#")]),e._v(" _update_info_buffer "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_info_buffer",sig:{params:[{name:"self"},{name:"infos",annotation:"typing.List[typing.Dict[str, typing.Any]]"},{name:"dones",default:"None",annotation:"typing.Optional[numpy.ndarray]"}],return:null}}}),e._v(" "),t("p",[e._v("Retrieve reward, episode length, episode success and update the buffer\nif using Monitor wrapper or a GoalEnv.")]),e._v(" "),t("p",[e._v(":param infos: List of additional information about the transition.\n:param dones: Termination signals")]),e._v(" "),t("h3",{attrs:{id:"update-learning-rate"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#update-learning-rate"}},[e._v("#")]),e._v(" _update_learning_rate "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_update_learning_rate",sig:{params:[{name:"self"},{name:"optimizers",annotation:"typing.Union[typing.List[torch.optim.optimizer.Optimizer], torch.optim.optimizer.Optimizer]"}],return:null}}}),e._v(" "),t("p",[e._v("Update the optimizers learning rate using the current learning rate schedule\nand the current progress remaining (from 1 to 0).")]),e._v(" "),t("p",[e._v(":param optimizers:\nAn optimizer or a list of optimizers.")]),e._v(" "),t("h3",{attrs:{id:"wrap-env"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wrap-env"}},[e._v("#")]),e._v(" _wrap_env "),t("Badge",{attrs:{text:"BaseAlgorithm",type:"warn"}})],1),e._v(" "),t("skdecide-signature",{attrs:{name:"_wrap_env",sig:{params:[{name:"env",annotation:"typing.Union[gymnasium.core.Env, ForwardRef('VecEnv')]"},{name:"verbose",default:"0",annotation:"<class 'int'>"},{name:"monitor_wrapper",default:"True",annotation:"<class 'bool'>"}],return:"<class 'stable_baselines3.common.vec_env.base_vec_env.VecEnv'>"}}}),e._v(" "),t("p",[e._v('"\nWrap environment with the appropriate wrappers if needed.\nFor instance, to have a vectorized environment\nor to re-order the image channels.')]),e._v(" "),t("p",[e._v(":param env:\n:param verbose: Verbosity level: 0 for no output, 1 for indicating wrappers used\n:param monitor_wrapper: Whether to wrap the env in a "),t("code",[e._v("Monitor")]),e._v(" when possible.\n:return: The wrapped environment.")])],1)}),[],!1,null,null,null);a.default=r.exports}}]);