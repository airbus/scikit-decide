(window.webpackJsonp=window.webpackJsonp||[]).push([[130],{644:function(t,e,a){"use strict";a.r(e);var r=a(38),s=Object(r.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"utils"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#utils"}},[t._v("#")]),t._v(" utils")]),t._v(" "),a("p",[t._v("This module contains utility functions.")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("Domain specification")]),t._v(" "),a("skdecide-summary")],1),t._v(" "),a("h2",{attrs:{id:"get-data-home"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-data-home"}},[t._v("#")]),t._v(" get_data_home")]),t._v(" "),a("skdecide-signature",{attrs:{name:"get_data_home",sig:{params:[{name:"data_home",default:"None",annotation:"Optional[str]"}],return:"str"}}}),t._v(" "),a("p",[t._v("Return the path of the scikit-decide data directory.")]),t._v(" "),a("p",[t._v("This folder is used by some large dataset loaders to avoid downloading the\ndata several times, as for instance the weather data used by the flight planning domain.\nBy default the data dir is set to a folder named 'skdecide_data' in the\nuser home folder.\nAlternatively, it can be set by the 'SKDECIDE_DATA' environment\nvariable or programmatically by giving an explicit folder path. The '~'\nsymbol is expanded to the user home folder.\nIf the folder does not already exist, it is automatically created.")]),t._v(" "),a("p",[t._v("Params:\ndata_home : The path to scikit-decide data directory. If "),a("code",[t._v("None")]),t._v(", the default path\nis "),a("code",[t._v("~/skdecide_data")]),t._v(".")]),t._v(" "),a("h2",{attrs:{id:"replayoutofactionmethod"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#replayoutofactionmethod"}},[t._v("#")]),t._v(" ReplayOutOfActionMethod")]),t._v(" "),a("p",[t._v("An enumeration.")]),t._v(" "),a("h3",{attrs:{id:"error"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#error"}},[t._v("#")]),t._v(" ERROR "),a("Badge",{attrs:{text:"ReplayOutOfActionMethod",type:"tip"}})],1),t._v(" "),a("h3",{attrs:{id:"last"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#last"}},[t._v("#")]),t._v(" LAST "),a("Badge",{attrs:{text:"ReplayOutOfActionMethod",type:"tip"}})],1),t._v(" "),a("h3",{attrs:{id:"loop"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#loop"}},[t._v("#")]),t._v(" LOOP "),a("Badge",{attrs:{text:"ReplayOutOfActionMethod",type:"tip"}})],1),t._v(" "),a("h2",{attrs:{id:"replaysolver"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#replaysolver"}},[t._v("#")]),t._v(" ReplaySolver")]),t._v(" "),a("p",[t._v("Wrapper around a list of actions mimicking a computed policy.")]),t._v(" "),a("p",[t._v("The goal is to be able to replay a rollout from a previous episode.")]),t._v(" "),a("h4",{attrs:{id:"attributes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#attributes"}},[t._v("#")]),t._v(" Attributes")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("actions")]),t._v(": list of actions to wrap")]),t._v(" "),a("li",[a("strong",[t._v("out_of_action_method")]),t._v(": method to use when we run out of actions\n"),a("ul",[a("li",[t._v("LOOP: we loop on actions, beginning back with the first action,")]),t._v(" "),a("li",[t._v("LAST: we keep returning the last action,")]),t._v(" "),a("li",[t._v("ERROR: we raise a RuntimeError.")])])])]),t._v(" "),a("h4",{attrs:{id:"example"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#example"}},[t._v("#")]),t._v(" Example")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# rollout with actual solver")]),t._v("\nepisodes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rollout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    domain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    solver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    return_episodes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# take the first episode")]),t._v("\nobservations"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" actions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" values "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" episodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# wrap the corresponding actions in a replay solver")]),t._v("\nreplay_solver "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ReplaySolver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("actions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# replay the rollout")]),t._v("\nreplayed_episodes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rollout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    domain"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("domain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    solver"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("replay_solver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    return_episodes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# same outputs (for deterministic domain)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" episodes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" replayed_episodes\n")])])]),a("h3",{attrs:{id:"constructor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constructor"}},[t._v("#")]),t._v(" Constructor "),a("Badge",{attrs:{text:"ReplaySolver",type:"tip"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"ReplaySolver",sig:{params:[{name:"actions",annotation:"List[D.T_agent[D.T_concurrency[D.T_event]]]"},{name:"out_of_action_method",default:"ReplayOutOfActionMethod.LAST",annotation:"ReplayOutOfActionMethod"}]}}}),t._v(" "),a("p",[t._v("Initialize self.  See help(type(self)) for accurate signature.")]),t._v(" "),a("h3",{attrs:{id:"get-next-action"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-next-action"}},[t._v("#")]),t._v(" get_next_action "),a("Badge",{attrs:{text:"DeterministicPolicies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"get_next_action",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"D.T_agent[D.T_concurrency[D.T_event]]"}}}),t._v(" "),a("p",[t._v("Get the next deterministic action (from the solver's current policy).")]),t._v(" "),a("h4",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation for which next action is requested.")])]),t._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("The next deterministic action.")]),t._v(" "),a("h3",{attrs:{id:"get-next-action-distribution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-next-action-distribution"}},[t._v("#")]),t._v(" get_next_action_distribution "),a("Badge",{attrs:{text:"UncertainPolicies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"get_next_action_distribution",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"Distribution[D.T_agent[D.T_concurrency[D.T_event]]]"}}}),t._v(" "),a("p",[t._v("Get the probabilistic distribution of next action for the given observation (from the solver's current\npolicy).")]),t._v(" "),a("h4",{attrs:{id:"parameters-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-2"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation to consider.")])]),t._v(" "),a("h4",{attrs:{id:"returns-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-2"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("The probabilistic distribution of next action.")]),t._v(" "),a("h3",{attrs:{id:"is-policy-defined-for"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-policy-defined-for"}},[t._v("#")]),t._v(" is_policy_defined_for "),a("Badge",{attrs:{text:"Policies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"is_policy_defined_for",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"bool"}}}),t._v(" "),a("p",[t._v("Check whether the solver's current policy is defined for the given observation.")]),t._v(" "),a("h4",{attrs:{id:"parameters-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-3"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation to consider.")])]),t._v(" "),a("h4",{attrs:{id:"returns-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-3"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("True if the policy is defined for the given observation memory (False otherwise).")]),t._v(" "),a("h3",{attrs:{id:"sample-action"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-action"}},[t._v("#")]),t._v(" sample_action "),a("Badge",{attrs:{text:"Policies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"sample_action",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"D.T_agent[D.T_concurrency[D.T_event]]"}}}),t._v(" "),a("p",[t._v("Sample an action for the given observation (from the solver's current policy).")]),t._v(" "),a("h4",{attrs:{id:"parameters-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-4"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation for which an action must be sampled.")])]),t._v(" "),a("h4",{attrs:{id:"returns-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-4"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("The sampled action.")]),t._v(" "),a("h3",{attrs:{id:"get-next-action-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-next-action-2"}},[t._v("#")]),t._v(" _get_next_action "),a("Badge",{attrs:{text:"DeterministicPolicies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"_get_next_action",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"D.T_agent[D.T_concurrency[D.T_event]]"}}}),t._v(" "),a("p",[t._v("Get the next deterministic action (from the solver's current policy).")]),t._v(" "),a("h4",{attrs:{id:"parameters-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-5"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation for which next action is requested.")])]),t._v(" "),a("h4",{attrs:{id:"returns-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-5"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("The next deterministic action.")]),t._v(" "),a("h3",{attrs:{id:"get-next-action-distribution-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-next-action-distribution-2"}},[t._v("#")]),t._v(" _get_next_action_distribution "),a("Badge",{attrs:{text:"UncertainPolicies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"_get_next_action_distribution",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"Distribution[D.T_agent[D.T_concurrency[D.T_event]]]"}}}),t._v(" "),a("p",[t._v("Get the probabilistic distribution of next action for the given observation (from the solver's current\npolicy).")]),t._v(" "),a("h4",{attrs:{id:"parameters-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-6"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation to consider.")])]),t._v(" "),a("h4",{attrs:{id:"returns-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-6"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("The probabilistic distribution of next action.")]),t._v(" "),a("h3",{attrs:{id:"is-policy-defined-for-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-policy-defined-for-2"}},[t._v("#")]),t._v(" _is_policy_defined_for "),a("Badge",{attrs:{text:"Policies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"_is_policy_defined_for",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"bool"}}}),t._v(" "),a("p",[t._v("Check whether the solver's current policy is defined for the given observation.")]),t._v(" "),a("h4",{attrs:{id:"parameters-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-7"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation to consider.")])]),t._v(" "),a("h4",{attrs:{id:"returns-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-7"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("True if the policy is defined for the given observation memory (False otherwise).")]),t._v(" "),a("h3",{attrs:{id:"sample-action-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-action-2"}},[t._v("#")]),t._v(" _sample_action "),a("Badge",{attrs:{text:"Policies",type:"warn"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"_sample_action",sig:{params:[{name:"self"},{name:"observation",annotation:"D.T_agent[D.T_observation]"}],return:"D.T_agent[D.T_concurrency[D.T_event]]"}}}),t._v(" "),a("p",[t._v("Sample an action for the given observation (from the solver's current policy).")]),t._v(" "),a("h4",{attrs:{id:"parameters-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-8"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("observation")]),t._v(": The observation for which an action must be sampled.")])]),t._v(" "),a("h4",{attrs:{id:"returns-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-8"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("The sampled action.")]),t._v(" "),a("h2",{attrs:{id:"rollout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#rollout"}},[t._v("#")]),t._v(" rollout")]),t._v(" "),a("skdecide-signature",{attrs:{name:"rollout",sig:{params:[{name:"domain",annotation:"Domain"},{name:"solver",default:"None",annotation:"Optional[Union[Solver, Policies]]"},{name:"from_memory",default:"None",annotation:"Optional[D.T_memory[D.T_state]]"},{name:"from_action",default:"None",annotation:"Optional[D.T_agent[D.T_concurrency[D.T_event]]]"},{name:"num_episodes",default:"1",annotation:"int"},{name:"max_steps",default:"None",annotation:"Optional[int]"},{name:"render",default:"True",annotation:"bool"},{name:"max_framerate",default:"None",annotation:"Optional[float]"},{name:"verbose",default:"True",annotation:"bool"},{name:"action_formatter",default:"<lambda function>",annotation:"Optional[Callable[[D.T_event], str]]"},{name:"outcome_formatter",default:"<lambda function>",annotation:"Optional[Callable[[EnvironmentOutcome], str]]"},{name:"return_episodes",default:"False",annotation:"bool"},{name:"goal_logging_level",default:"20",annotation:"int"},{name:"rollout_callback",default:"None",annotation:"Optional[RolloutCallback]"}],return:"Optional[List[Tuple[List[D.T_agent[D.T_observation]], List[D.T_agent[D.T_concurrency[D.T_event]]], List[D.T_agent[Value[D.T_value]]]]]]"}}}),t._v(" "),a("p",[t._v("This method will run one or more episodes in a domain according to the policy of a solver.")]),t._v(" "),a("h4",{attrs:{id:"parameters-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-9"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("domain")]),t._v(": The domain in which the episode(s) will be run.")]),t._v(" "),a("li",[a("strong",[t._v("solver")]),t._v(": The solver whose policy will select actions to take (if None, a random policy is used).")]),t._v(" "),a("li",[a("strong",[t._v("from_memory")]),t._v(": The memory or state to consider as rollout starting point (if None, the domain is reset first).")]),t._v(" "),a("li",[a("strong",[t._v("from_action")]),t._v(": The last applied action when from_memory is used (if necessary for initial observation computation).")]),t._v(" "),a("li",[a("strong",[t._v("num_episodes")]),t._v(": The number of episodes to run.")]),t._v(" "),a("li",[a("strong",[t._v("max_steps")]),t._v(": The maximum number of steps for each episode (if None, no limit is set).")]),t._v(" "),a("li",[a("strong",[t._v("render")]),t._v(": Whether to render the episode(s) during rollout if the domain is renderable.")]),t._v(" "),a("li",[a("strong",[t._v("max_framerate")]),t._v(": The maximum number of steps/renders per second (if None, steps/renders are never slowed down).")]),t._v(" "),a("li",[a("strong",[t._v("verbose")]),t._v(": Whether to print information to the console during rollout.")]),t._v(" "),a("li",[a("strong",[t._v("action_formatter")]),t._v(": The function transforming actions in the string to print (if None, no print).")]),t._v(" "),a("li",[a("strong",[t._v("outcome_formatter")]),t._v(": The function transforming EnvironmentOutcome objects in the string to print (if None, no print).")]),t._v(" "),a("li",[a("strong",[t._v("return_episodes")]),t._v(": if True, return the list of episodes, each episode as a tuple of observations, actions, and values.\nelse return nothing.")]),t._v(" "),a("li",[a("strong",[t._v("goal_logging_level")]),t._v(": logging level at which we want to display if goal has been reached or not")])]),t._v(" "),a("h2",{attrs:{id:"rolloutcallback"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#rolloutcallback"}},[t._v("#")]),t._v(" RolloutCallback")]),t._v(" "),a("p",[t._v("Callback used during rollout to add custom behaviour.")]),t._v(" "),a("p",[t._v("One should derives from this one in order to hook in different stages of the rollout.")]),t._v(" "),a("h3",{attrs:{id:"at-episode-end"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#at-episode-end"}},[t._v("#")]),t._v(" at_episode_end "),a("Badge",{attrs:{text:"RolloutCallback",type:"tip"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"at_episode_end",sig:{params:[{name:"self"}]}}}),t._v(" "),a("p",[t._v("Called after each episode.")]),t._v(" "),a("h3",{attrs:{id:"at-episode-start"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#at-episode-start"}},[t._v("#")]),t._v(" at_episode_start "),a("Badge",{attrs:{text:"RolloutCallback",type:"tip"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"at_episode_start",sig:{params:[{name:"self"}]}}}),t._v(" "),a("p",[t._v("Called before each episode.")]),t._v(" "),a("h3",{attrs:{id:"at-episode-step"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#at-episode-step"}},[t._v("#")]),t._v(" at_episode_step "),a("Badge",{attrs:{text:"RolloutCallback",type:"tip"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"at_episode_step",sig:{params:[{name:"self"},{name:"i_episode",annotation:"int"},{name:"step",annotation:"int"},{name:"domain",annotation:"Domain"},{name:"solver",annotation:"Union[Solver, Policies]"},{name:"action",annotation:"D.T_agent[D.T_concurrency[D.T_event]]"},{name:"outcome",annotation:"EnvironmentOutcome[D.T_agent[D.T_observation], D.T_agent[Value[D.T_value]], D.T_agent[D.T_predicate], D.T_agent[D.T_info]]"}],return:"bool"}}}),t._v(" "),a("h4",{attrs:{id:"parameters-10"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters-10"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("i_episode")]),t._v(": current episode number")]),t._v(" "),a("li",[a("strong",[t._v("step")]),t._v(": current step number within the episode")]),t._v(" "),a("li",[a("strong",[t._v("domain")]),t._v(": domain considered")]),t._v(" "),a("li",[a("strong",[t._v("solver")]),t._v(": solver considered (or randomwalk policy if solver was None in rollout)")]),t._v(" "),a("li",[a("strong",[t._v("action")]),t._v(": last action sampled")]),t._v(" "),a("li",[a("strong",[t._v("outcome")]),t._v(": outcome of the last action applied to the domain")])]),t._v(" "),a("h4",{attrs:{id:"returns-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-9"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),a("p",[t._v("stopping: if True, the rollout for the current episode stops and the next episode starts.")]),t._v(" "),a("h3",{attrs:{id:"at-rollout-end"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#at-rollout-end"}},[t._v("#")]),t._v(" at_rollout_end "),a("Badge",{attrs:{text:"RolloutCallback",type:"tip"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"at_rollout_end",sig:{params:[{name:"self"}]}}}),t._v(" "),a("p",[t._v("Called at rollout end.")]),t._v(" "),a("h3",{attrs:{id:"at-rollout-start"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#at-rollout-start"}},[t._v("#")]),t._v(" at_rollout_start "),a("Badge",{attrs:{text:"RolloutCallback",type:"tip"}})],1),t._v(" "),a("skdecide-signature",{attrs:{name:"at_rollout_start",sig:{params:[{name:"self"}]}}}),t._v(" "),a("p",[t._v("Called at rollout start.")])],1)}),[],!1,null,null,null);e.default=s.exports}}]);