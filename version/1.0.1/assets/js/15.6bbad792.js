(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{531:function(t,s,a){"use strict";a.r(s);var e=a(38),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("skdecide-spec",{scopedSlots:t._u([{key:"Domain",fn:function(){return[a("p",[t._v("This is the highest level domain class (inheriting top-level class for each mandatory domain characteristic).")]),t._v(" "),a("p",[t._v("This helper class can be used as the main base class for domains.")]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Domain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v('with "..." replaced when needed by a number of classes from following domain characteristics (the ones in\nparentheses are optional):')]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("agent")]),t._v(": MultiAgent -> SingleAgent")]),t._v(" "),a("li",[a("strong",[t._v("concurrency")]),t._v(": Parallel -> Sequential")]),t._v(" "),a("li",[a("strong",[t._v("(constraints)")]),t._v(": Constrained")]),t._v(" "),a("li",[a("strong",[t._v("dynamics")]),t._v(": Environment -> Simulation -> UncertainTransitions -> EnumerableTransitions\n-> DeterministicTransitions")]),t._v(" "),a("li",[a("strong",[t._v("events")]),t._v(": Events -> Actions -> UnrestrictedActions")]),t._v(" "),a("li",[a("strong",[t._v("(goals)")]),t._v(": Goals")]),t._v(" "),a("li",[a("strong",[t._v("(initialization)")]),t._v(": Initializable -> UncertainInitialized -> DeterministicInitialized")]),t._v(" "),a("li",[a("strong",[t._v("memory")]),t._v(": History -> FiniteHistory -> Markovian -> Memoryless")]),t._v(" "),a("li",[a("strong",[t._v("observability")]),t._v(": PartiallyObservable -> TransformedObservable -> FullyObservable")]),t._v(" "),a("li",[a("strong",[t._v("(renderability)")]),t._v(": Renderable")]),t._v(" "),a("li",[a("strong",[t._v("value")]),t._v(": Rewards -> PositiveCosts")])])]},proxy:!0},{key:"RLDomain",fn:function(){return[a("p",[t._v("This is a typical Reinforcement Learning domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("Environment")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("Initializable")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("TransformedObservable")]),t._v(" "),a("li",[t._v("Rewards")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"MultiAgentRLDomain",fn:function(){return[a("p",[t._v("This is a typical multi-agent Reinforcement Learning domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("MultiAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("Environment")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("Initializable")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("TransformedObservable")]),t._v(" "),a("li",[t._v("Rewards")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"StatelessSimulatorDomain",fn:function(){return[a("p",[t._v("This is a typical stateless simulator domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("Simulation")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("TransformedObservable")]),t._v(" "),a("li",[t._v("Rewards")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("StatelessSimulatorDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"MDPDomain",fn:function(){return[a("p",[t._v("This is a typical Markov Decision Process domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("EnumerableTransitions")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("DeterministicInitialized")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("FullyObservable")]),t._v(" "),a("li",[t._v("Rewards")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MDPDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"POMDPDomain",fn:function(){return[a("p",[t._v("This is a typical Partially Observable Markov Decision Process domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("EnumerableTransitions")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("UncertainInitialized")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("PartiallyObservable")]),t._v(" "),a("li",[t._v("Rewards")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("POMDPDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"GoalMDPDomain",fn:function(){return[a("p",[t._v("This is a typical Goal Markov Decision Process domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("EnumerableTransitions")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("Goals")]),t._v(" "),a("li",[t._v("DeterministicInitialized")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("FullyObservable")]),t._v(" "),a("li",[t._v("PositiveCosts")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GoalMDPDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"GoalPOMDPDomain",fn:function(){return[a("p",[t._v("This is a typical Goal Partially Observable Markov Decision Process domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("EnumerableTransitions")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("Goals")]),t._v(" "),a("li",[t._v("UncertainInitialized")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("PartiallyObservable")]),t._v(" "),a("li",[t._v("PositiveCosts")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GoalPOMDPDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"DeterministicPlanningDomain",fn:function(){return[a("p",[t._v("This is a typical deterministic planning domain class.")]),t._v(" "),a("p",[t._v("This helper class can be used as an alternate base class for domains, inheriting the following:")]),t._v(" "),a("ul",[a("li",[t._v("Domain")]),t._v(" "),a("li",[t._v("SingleAgent")]),t._v(" "),a("li",[t._v("Sequential")]),t._v(" "),a("li",[t._v("DeterministicTransitions")]),t._v(" "),a("li",[t._v("Actions")]),t._v(" "),a("li",[t._v("Goals")]),t._v(" "),a("li",[t._v("DeterministicInitialized")]),t._v(" "),a("li",[t._v("Markovian")]),t._v(" "),a("li",[t._v("FullyObservable")]),t._v(" "),a("li",[t._v("PositiveCosts")])]),t._v(" "),a("p",[t._v("Typical use:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("DeterministicPlanningDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("It is also possible to refine any alternate base class, like for instance:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("D")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RLDomain"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FullyObservable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])]},proxy:!0},{key:"MultiAgent",fn:function(){return[a("p",[t._v("A domain must inherit this class if it is multi-agent (i.e hosting multiple independent agents).")]),t._v(" "),a("p",[t._v("Agents are identified by (string) agent names.")])]},proxy:!0},{key:"SingleAgent",fn:function(){return[a("p",[t._v("A domain must inherit this class if it is single-agent (i.e hosting only one agent).")])]},proxy:!0},{key:"Parallel",fn:function(){return[a("p",[t._v("A domain must inherit this class if multiple events/actions can happen in parallel.")])]},proxy:!0},{key:"Sequential",fn:function(){return[a("p",[t._v("A domain must inherit this class if its events/actions are sequential (non-parallel).")])]},proxy:!0},{key:"Constrained",fn:function(){return[a("p",[t._v("A domain must inherit this class if it has constraints.")])]},proxy:!0},{key:"Environment",fn:function(){return[a("p",[t._v("A domain must inherit this class if agents interact with it like a black-box environment.")]),t._v(" "),a("p",[t._v("Black-box environment examples include: the real world, compiled ATARI games, etc.")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("Environment domains are typically stateful: they must keep the current state or history in their memory to\ncompute next steps (automatically done by default in the "),a("code",[t._v("_memory")]),t._v(" attribute).")])])]},proxy:!0},{key:"Simulation",fn:function(){return[a("p",[t._v("A domain must inherit this class if agents interact with it like a simulation.")]),t._v(" "),a("p",[t._v("Compared to pure environment domains, simulation ones have the additional ability to sample transitions from any\ngiven state.")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("Simulation domains are typically stateless: they do not need to store the current state or history in memory\nsince it is usually passed as parameter of their functions. By default, they only become stateful whenever they\nare used as environments (e.g. via "),a("code",[t._v("Initializable.reset()")]),t._v(" and "),a("code",[t._v("Environment.step()")]),t._v(" functions).")])])]},proxy:!0},{key:"UncertainTransitions",fn:function(){return[a("p",[t._v("A domain must inherit this class if its dynamics is uncertain and provided as a white-box model.")]),t._v(" "),a("p",[t._v("Compared to pure simulation domains, uncertain transition ones provide in addition the full probability distribution\nof next states given a memory and action.")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("Uncertain transition domains are typically stateless: they do not need to store the current state or history in\nmemory since it is usually passed as parameter of their functions. By default, they only become stateful\nwhenever they are used as environments (e.g. via "),a("code",[t._v("Initializable.reset()")]),t._v(" and "),a("code",[t._v("Environment.step()")]),t._v(" functions).")])])]},proxy:!0},{key:"EnumerableTransitions",fn:function(){return[a("p",[t._v("A domain must inherit this class if its dynamics is uncertain (with enumerable transitions) and provided as a\nwhite-box model.")]),t._v(" "),a("p",[t._v("Compared to pure uncertain transition domains, enumerable transition ones guarantee that all probability\ndistributions of next state are discrete.")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("Enumerable transition domains are typically stateless: they do not need to store the current state or history in\nmemory since it is usually passed as parameter of their functions. By default, they only become stateful\nwhenever they are used as environments (e.g. via "),a("code",[t._v("Initializable.reset()")]),t._v(" and "),a("code",[t._v("Environment.step()")]),t._v(" functions).")])])]},proxy:!0},{key:"DeterministicTransitions",fn:function(){return[a("p",[t._v("A domain must inherit this class if its dynamics is deterministic and provided as a white-box model.")]),t._v(" "),a("p",[t._v("Compared to pure enumerable transition domains, deterministic transition ones guarantee that there is only one next\nstate for a given source memory (state or history) and action.")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("Deterministic transition domains are typically stateless: they do not need to store the current state or history\nin memory since it is usually passed as parameter of their functions. By default, they only become stateful\nwhenever they are used as environments (e.g. via "),a("code",[t._v("Initializable.reset()")]),t._v(" and "),a("code",[t._v("Environment.step()")]),t._v(" functions).")])])]},proxy:!0},{key:"Events",fn:function(){return[a("p",[t._v("A domain must inherit this class if it handles events (controllable or not not by the agents).")])]},proxy:!0},{key:"Actions",fn:function(){return[a("p",[t._v("A domain must inherit this class if it handles only actions (i.e. controllable events).")])]},proxy:!0},{key:"UnrestrictedActions",fn:function(){return[a("p",[t._v("A domain must inherit this class if it handles only actions (i.e. controllable events), which are always all\napplicable.")])]},proxy:!0},{key:"Goals",fn:function(){return[a("p",[t._v("A domain must inherit this class if it has formalized goals.")])]},proxy:!0},{key:"Initializable",fn:function(){return[a("p",[t._v("A domain must inherit this class if it can be initialized.")])]},proxy:!0},{key:"UncertainInitialized",fn:function(){return[a("p",[t._v("A domain must inherit this class if its states are initialized according to a probability distribution known as\nwhite-box.")])]},proxy:!0},{key:"DeterministicInitialized",fn:function(){return[a("p",[t._v("A domain must inherit this class if it has a deterministic initial state known as white-box.")])]},proxy:!0},{key:"History",fn:function(){return[a("p",[t._v("A domain must inherit this class if its full state history must be stored to compute its dynamics (non-Markovian\ndomain).")])]},proxy:!0},{key:"FiniteHistory",fn:function(){return[a("p",[t._v("A domain must inherit this class if the last N states must be stored to compute its dynamics (Markovian\ndomain of order N).")]),t._v(" "),a("p",[t._v("N is specified by the return value of the "),a("code",[t._v("FiniteHistory._get_memory_maxlen()")]),t._v(" function.")])]},proxy:!0},{key:"Markovian",fn:function(){return[a("p",[t._v("A domain must inherit this class if only its last state must be stored to compute its dynamics (pure Markovian\ndomain).")])]},proxy:!0},{key:"Memoryless",fn:function(){return[a("p",[t._v("A domain must inherit this class if it does not require any previous state(s) to be stored to compute its\ndynamics.")]),t._v(" "),a("p",[t._v("A dice roll simulator is an example of memoryless domain (next states are independent of previous ones).")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("Whenever an existing domain (environment, simulator...) needs to be wrapped instead of implemented fully in\nscikit-decide (e.g. compiled ATARI games), Memoryless can be used because the domain memory (if any) would\nbe handled externally.")])])]},proxy:!0},{key:"PartiallyObservable",fn:function(){return[a("p",[t._v("A domain must inherit this class if it is partially observable.")]),t._v(" "),a("p",[t._v('"Partially observable" means that the observation provided to the agent is computed from (but generally not equal\nto) the internal state of the domain. Additionally, according to literature, a partially observable domain must\nprovide the probability distribution of the observation given a state and action.')])]},proxy:!0},{key:"TransformedObservable",fn:function(){return[a("p",[t._v("A domain must inherit this class if it is transformed observable.")]),t._v(" "),a("p",[t._v('"Transformed observable" means that the observation provided to the agent is deterministically computed from (but\ngenerally not equal to) the internal state of the domain.')])]},proxy:!0},{key:"FullyObservable",fn:function(){return[a("p",[t._v("A domain must inherit this class if it is fully observable.")]),t._v(" "),a("p",[t._v('"Fully observable" means that the observation provided to the agent is equal to the internal state of the domain.')]),t._v(" "),a("div",{staticClass:"custom-block warning"},[a("p",{staticClass:"custom-block-title"},[t._v("WARNING")]),t._v(" "),a("p",[t._v("In the case of fully observable domains, make sure that the observation type D.T_observation is equal to the\nstate type D.T_state.")])])]},proxy:!0},{key:"Renderable",fn:function(){return[a("p",[t._v("A domain must inherit this class if it can be rendered with any kind of visualization.")])]},proxy:!0},{key:"Rewards",fn:function(){return[a("p",[t._v("A domain must inherit this class if it sends rewards (positive and/or negative).")])]},proxy:!0},{key:"PositiveCosts",fn:function(){return[a("p",[t._v("A domain must inherit this class if it sends only positive costs (i.e. negative rewards).")]),t._v(" "),a("p",[t._v("Having only positive costs is a required assumption for certain solvers to work, such as classical planners.")])]},proxy:!0}])})],1)}),[],!1,null,null,null);s.default=n.exports}}]);